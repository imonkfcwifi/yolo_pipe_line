# -*- coding: utf-8 -*-
import numpy as np
import cv2
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort
from datetime import datetime
import pandas as pd
from collections import defaultdict, deque
import os
import math
import time
import tkinter as tk
from tkinter import filedialog, messagebox, simpledialog
import threading
import subprocess
import sys
import webbrowser
from PIL import Image, ImageDraw, ImageFont

# ìƒìˆ˜ ì •ì˜ (ë§¤ì§ ë„˜ë²„ ì œê±°)
class Constants:
    # ê±°ë¦¬ ë° ì†ë„ ê´€ë ¨
    DISTANCE_THRESHOLD_PIXELS = 10  # ì ê³¼ ì„  ì‚¬ì´ì˜ ê±°ë¦¬ ì„ê³„ê°’
    DRAG_DETECTION_RADIUS = 15     # ë“œë˜ê·¸ ê°ì§€ ë°˜ê²½
    ACCEL_LIMIT_MPS2 = 50          # ë¹„í˜„ì‹¤ì ì¸ ê°€ì†ë„ ì œí•œê°’
    
    # ì„±ëŠ¥ ê´€ë ¨
    DISTANCE_CALC_INTERVAL = 5      # Ní”„ë ˆì„ë§ˆë‹¤ ê±°ë¦¬ ê³„ì‚°
    SPEED_CACHE_SIZE = 100          # ì†ë„ ë°ì´í„° ìºì‹œ í¬ê¸°
    
    # YOLO ë° ì¶”ì  ê´€ë ¨
    DEFAULT_IMGSZ = 1280           # ê¸°ë³¸ ì´ë¯¸ì§€ í¬ê¸°
    CONFIDENCE_THRESHOLD = 0.2     # YOLO ì‹ ë¢°ë„ ì„ê³„ê°’
    FILTER_THRESHOLD = 0.15        # ê²€ì§€ í•„í„°ë§ ì„ê³„ê°’
    IOU_THRESHOLD = 0.4            # IOU ì„ê³„ê°’
    
    # ì¶”ì ê¸° ì„¤ì •
    TRACKER_MAX_AGE = 10
    TRACKER_MIN_HITS = 3
    TRACKER_IOU_THRESHOLD = 0.3
    
    # ê°€ë ¤ì§ ê²€ì§€ ê´€ë ¨ ìƒìˆ˜
    EDGE_MARGIN_RATIO = 0.03      # í”„ë ˆì„ ê°€ì¥ìë¦¬ ë§ˆì§„ ë¹„ìœ¨
    EDGE_MARGIN_PX = 16           # í”„ë ˆì„ ê°€ì¥ìë¦¬ ë§ˆì§„ í”½ì…€
    CORE_SHRINK_RATIO = 0.06      # ROI í…Œë‘ë¦¬ ì œê±° ë¹„ìœ¨
    EDGE_CUT_DELTA = 0.20         # ì—£ì§€ì»· íŒë‹¨ ì„ê³„ê°’
    EMA_ALPHA = 0.5               # EMA í‰í™œí™” ê³„ìˆ˜
    ALARM_OCC_THRESHOLD = 0.50    # ê²½ë³´ ê°€ë ¤ì§ ì„ê³„ê°’
    ALARM_MIN_FRAMES = 3          # ì—°ì† ê²½ë³´ ìµœì†Œ í”„ë ˆì„

# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
estimated_vertical_m = None
estimated_horizontal_m = None
lane_polygons = []
lane_thresholds = {}
custom_regions = []
current_custom_region = []
custom_region_mode = False
tracking_lines = []
dragging_point = None
dragging_index = None

# í•­ê³µë·° ëª¨ë“œ ê´€ë ¨ ì „ì—­ ë³€ìˆ˜
aerial_view_mode = False
aerial_meters_per_pixel = None

# ì£¼í–‰ëª¨ë“œ ê´€ë ¨ ì „ì—­ ë³€ìˆ˜
driving_mode = False
detected_signs = {}  # ê²€ì§€ëœ í‘œì§€íŒ ì €ì¥
sign_screenshots_saved = {}  # ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì—¬ë¶€ ì¶”ì 

# ê°€ë ¤ì§ ê²€ì§€ ê´€ë ¨ ì „ì—­ ë³€ìˆ˜
occlusion_tracker = {}  # íŠ¸ë™ IDë³„ ê°€ë ¤ì§ EMA ë° ê²½ë³´ ìƒíƒœ
occlusion_events = []   # ê°€ë ¤ì§ ì´ë²¤íŠ¸ ë¡œê·¸

# Tail ì¶”ì  ê¸°ëŠ¥ ê´€ë ¨ ì „ì—­ ë³€ìˆ˜
show_trails = False  # 't' í‚¤ë¡œ í† ê¸€
show_accumulated_trails = False  # 'i' í‚¤ë¡œ í† ê¸€ (ëª¨ë“  ê°ì²´ì˜ ëˆ„ì  ê²½ë¡œ)
accumulated_trail_paths = {}  # ëª¨ë“  ê°ì²´ì˜ ì „ì²´ ì´ë™ ê²½ë¡œ ì €ì¥

# === ê°€ë ¤ì§ ê²€ì§€ ê´€ë ¨ í•¨ìˆ˜ë“¤ ===

def compute_occ_ratio_roi(roi_bgr):
    """
    ROIì—ì„œ ê°€ë ¤ì§ ë¹„ìœ¨ì„ ê³„ì‚°
    Args:
        roi_bgr: BGR ì´ë¯¸ì§€ ROI
    Returns:
        (occ_ratio, method): ê°€ë ¤ì§ ë¹„ìœ¨(0~1)ê³¼ ë¶„ì„ ë°©ë²•
    """
    try:
        h, w = roi_bgr.shape[:2]
        if h < 20 or w < 20:
            return 0.5, "fallback"  # ë„ˆë¬´ ì‘ì€ ROI
        
        # Grayscale ë³€í™˜
        gray = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2GRAY)
        
        # ë„ˆë¬´ ì–´ë‘ìš´ ì´ë¯¸ì§€ ì²´í¬
        mean_brightness = np.mean(gray)
        if mean_brightness < 30:
            return 0.5, "fallback"  # ë„ˆë¬´ ì–´ë‘ì›€
        
        # Gaussian Blur
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        
        # Canny Edge Detection
        edges = cv2.Canny(blurred, 50, 150)
        
        # ì»¨íˆ¬ì–´ ì°¾ê¸°
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return 0.5, "fallback"  # ì»¨íˆ¬ì–´ ì—†ìŒ
        
        # ê°€ì¥ í° ì»¨íˆ¬ì–´ ì„ íƒ
        largest_contour = max(contours, key=cv2.contourArea)
        
        # ì»¨íˆ¬ì–´ ë©´ì ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ fallback
        contour_area = cv2.contourArea(largest_contour)
        if contour_area < (w * h * 0.1):  # ROI ë©´ì ì˜ 10% ë¯¸ë§Œ
            return 0.5, "fallback"
        
        # ì›í˜• vs ë‹¤ê°í˜• íŒë‹¨ (ë‘˜ë ˆ ê¸¸ì´ ê¸°ì¤€)
        perimeter = cv2.arcLength(largest_contour, True)
        circularity = 4 * np.pi * contour_area / (perimeter ** 2) if perimeter > 0 else 0
        
        if circularity > 0.6:  # ì›í˜•ì— ê°€ê¹Œì›€
            return _compute_circle_visibility(edges, largest_contour, w, h)
        else:  # ë‹¤ê°í˜•
            return _compute_polygon_visibility(edges, largest_contour, w, h)
            
    except Exception as e:
        print(f"ê°€ë ¤ì§ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return 0.5, "fallback"

def _compute_circle_visibility(edges, contour, w, h):
    """ì›í˜• ê°ì²´ì˜ ê°€ì‹œì„± ê³„ì‚°"""
    try:
        # ìµœì†Œì™¸ì ‘ì›
        (cx, cy), radius = cv2.minEnclosingCircle(contour)
        if radius < 10:
            return 0.5, "fallback"
        
        # ì›ë‘˜ë ˆì—ì„œ ìƒ˜í”Œë§
        num_samples = max(int(2 * np.pi * radius / 3), 20)  # 3í”½ì…€ë§ˆë‹¤ í•˜ë‚˜ì”©
        angles = np.linspace(0, 2 * np.pi, num_samples, endpoint=False)
        
        visible_count = 0
        for angle in angles:
            x = int(cx + radius * np.cos(angle))
            y = int(cy + radius * np.sin(angle))
            
            # ë²”ìœ„ ì²´í¬
            if 0 <= x < w and 0 <= y < h:
                # ì£¼ë³€ 3x3 ì˜ì—­ì—ì„œ ì—£ì§€ í™•ì¸
                edge_found = False
                for dx in [-1, 0, 1]:
                    for dy in [-1, 0, 1]:
                        nx, ny = x + dx, y + dy
                        if 0 <= nx < w and 0 <= ny < h and edges[ny, nx] > 0:
                            edge_found = True
                            break
                    if edge_found:
                        break
                
                if edge_found:
                    visible_count += 1
        
        visible_ratio = visible_count / len(angles) if len(angles) > 0 else 0
        occ_ratio = 1 - visible_ratio
        return occ_ratio, "circle"
        
    except Exception:
        return 0.5, "fallback"

def _compute_polygon_visibility(edges, contour, w, h):
    """ë‹¤ê°í˜• ê°ì²´ì˜ ê°€ì‹œì„± ê³„ì‚°"""
    try:
        # ê¸°ëŒ€ ê²½ê³„ ë§ˆìŠ¤í¬ ìƒì„±
        mask = np.zeros((h, w), dtype=np.uint8)
        cv2.polylines(mask, [contour], True, 255, 2)
        
        # ê²½ê³„ì™€ ì—£ì§€ì˜ êµì§‘í•©
        boundary_pixels = np.where(mask > 0)
        total_boundary = len(boundary_pixels[0])
        
        if total_boundary == 0:
            return 0.5, "fallback"
        
        # ê²½ê³„ ìœ„ì¹˜ì—ì„œ ì—£ì§€ í™•ì¸
        visible_count = 0
        for i in range(total_boundary):
            y, x = boundary_pixels[0][i], boundary_pixels[1][i]
            if edges[y, x] > 0:
                visible_count += 1
        
        visible_ratio = visible_count / total_boundary
        occ_ratio = 1 - visible_ratio
        return occ_ratio, "poly"
        
    except Exception:
        return 0.5, "fallback"

def is_near_edge(box, W, H, margin_ratio=None, margin_px=None):
    """
    ë°•ìŠ¤ê°€ í”„ë ˆì„ ê°€ì¥ìë¦¬ ê·¼ì²˜ì— ìˆëŠ”ì§€ íŒë‹¨
    Args:
        box: (x1, y1, x2, y2)
        W, H: í”„ë ˆì„ í¬ê¸°
        margin_ratio: ê°€ì¥ìë¦¬ ë§ˆì§„ ë¹„ìœ¨
        margin_px: ê°€ì¥ìë¦¬ ë§ˆì§„ í”½ì…€
    Returns:
        (near_edge, distance): ê°€ì¥ìë¦¬ ê·¼ì ‘ ì—¬ë¶€ì™€ ìµœì†Œ ê±°ë¦¬
    """
    if margin_ratio is None:
        margin_ratio = Constants.EDGE_MARGIN_RATIO
    if margin_px is None:
        margin_px = Constants.EDGE_MARGIN_PX
    
    x1, y1, x2, y2 = box
    
    # ë§ˆì§„ ê³„ì‚° (ë¹„ìœ¨ê³¼ í”½ì…€ ì¤‘ í° ê°’)
    margin_w = max(W * margin_ratio, margin_px)
    margin_h = max(H * margin_ratio, margin_px)
    
    # ê° ê°€ì¥ìë¦¬ê¹Œì§€ì˜ ê±°ë¦¬
    dist_left = x1
    dist_right = W - x2
    dist_top = y1
    dist_bottom = H - y2
    
    min_distance = min(dist_left, dist_right, dist_top, dist_bottom)
    
    # ê°€ì¥ìë¦¬ ê·¼ì²˜ íŒë‹¨
    near_edge = (dist_left < margin_w or dist_right < margin_w or 
                dist_top < margin_h or dist_bottom < margin_h)
    
    return near_edge, min_distance

def compute_occ_core(roi_bgr, shrink_ratio=None):
    """
    ROI í…Œë‘ë¦¬ë¥¼ ì œê±°í•œ í›„ ê°€ë ¤ì§ ë¹„ìœ¨ ê³„ì‚°
    Args:
        roi_bgr: BGR ì´ë¯¸ì§€ ROI
        shrink_ratio: í…Œë‘ë¦¬ ì œê±° ë¹„ìœ¨
    Returns:
        occ_core: ì½”ì–´ ì˜ì—­ì˜ ê°€ë ¤ì§ ë¹„ìœ¨ (None if ì‹¤íŒ¨)
    """
    if shrink_ratio is None:
        shrink_ratio = Constants.CORE_SHRINK_RATIO
    
    try:
        h, w = roi_bgr.shape[:2]
        
        # ì¶•ì†Œí•  í”½ì…€ ìˆ˜ ê³„ì‚°
        shrink_w = int(w * shrink_ratio)
        shrink_h = int(h * shrink_ratio)
        
        # ì¶•ì†Œëœ ì˜ì—­ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ None ë°˜í™˜
        if w - 2*shrink_w < 10 or h - 2*shrink_h < 10:
            return None
        
        # ì½”ì–´ ì˜ì—­ ì¶”ì¶œ
        core_roi = roi_bgr[shrink_h:h-shrink_h, shrink_w:w-shrink_w]
        
        # ì½”ì–´ ì˜ì—­ì˜ ê°€ë ¤ì§ ë¹„ìœ¨ ê³„ì‚°
        occ_core, _ = compute_occ_ratio_roi(core_roi)
        return occ_core
        
    except Exception:
        return None

def severity_from_occ(occ):
    """ê°€ë ¤ì§ ì •ë„ì— ë”°ë¥¸ ë“±ê¸‰ ë°˜í™˜"""
    if occ < 0.15:
        return "ì •ìƒ"
    elif occ < 0.40:
        return "ë¶€ë¶„"
    else:
        return "ì‹¬í•¨"

def ema_update(prev_value, new_value, alpha=None):
    """EMA (Exponential Moving Average) ì—…ë°ì´íŠ¸"""
    if alpha is None:
        alpha = Constants.EMA_ALPHA
    return alpha * new_value + (1 - alpha) * prev_value

def get_severity_color(severity):
    """ë“±ê¸‰ë³„ ìƒ‰ìƒ ë°˜í™˜ (BGR)"""
    colors = {
        "ì •ìƒ": (0, 255, 0),      # ì´ˆë¡
        "ë¶€ë¶„": (0, 255, 255),    # ë…¸ë‘
        "ì‹¬í•¨": (0, 0, 255)       # ë¹¨ê°•
    }
    return colors.get(severity, (255, 255, 255))  # ê¸°ë³¸ê°’: í°ìƒ‰

def run_image_test_mode(test_images, model_path, output_folder_path, current_datetime):
    """ì´ë¯¸ì§€ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰"""
    try:
        print(f"ğŸ§ª ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹œì‘")
        print(f"   ëª¨ë¸: {model_path}")
        print(f"   ì´ë¯¸ì§€ ìˆ˜: {len(test_images)}")
        
        # YOLO ëª¨ë¸ ë¡œë“œ
        yolo_model = YOLO(model_path)
        class_names = yolo_model.names if hasattr(yolo_model, 'names') else {}
        
        test_results = []
        
        # ëª¨ë“œë³„ í…ŒìŠ¤íŠ¸ ë¶„í• 
        modes_to_test = ["ì°¨ëŸ‰ ê²€ì§€ (í•­ê³µë·°/í˜¸ëª¨ê·¸ë˜í”¼ìš©)", "í‘œì§€íŒ ê²€ì§€ (ì£¼í–‰ëª¨ë“œìš©)", "ê°€ë ¤ì§ ë¶„ì„"]
        
        for img_idx, image_path in enumerate(test_images):
            print(f"\nğŸ“· ì´ë¯¸ì§€ {img_idx + 1}/{len(test_images)}: {os.path.basename(image_path)}")
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            frame = cv2.imread(image_path)
            if frame is None:
                print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
                continue
            
            H, W = frame.shape[:2]
            
            # YOLO ê²€ì§€
            try:
                results = yolo_model.predict(source=frame, 
                                           imgsz=Constants.DEFAULT_IMGSZ, 
                                           conf=Constants.CONFIDENCE_THRESHOLD, 
                                           iou=Constants.IOU_THRESHOLD)
                
                detections = []
                for result_item in results:
                    boxes = result_item.boxes
                    for box in boxes:
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        conf = float(box.conf[0])
                        cls_idx = int(box.cls[0])
                        class_name = class_names.get(cls_idx, f"Unknown_{cls_idx}")
                        detections.append([x1, y1, x2, y2, conf, cls_idx, class_name])
                
                print(f"   ê²€ì§€ëœ ê°ì²´: {len(detections)}ê°œ")
                
                # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
                result_frame = frame.copy()
                
                # ëª¨ë“œë³„ ë¶„ì„
                for detection in detections:
                    x1, y1, x2, y2, conf, cls_idx, class_name = detection
                    
                    # 1. ì°¨ëŸ‰ ê²€ì§€ í…ŒìŠ¤íŠ¸ (í•­ê³µë·°/í˜¸ëª¨ê·¸ë˜í”¼ ëª¨ë“œìš©)
                    vehicle_classes = ['car', 'truck', 'bus', 'motorbike', 'bicycle', 'van']
                    is_vehicle = class_name.lower() in vehicle_classes
                    
                    # 2. í‘œì§€íŒ ê²€ì§€ í…ŒìŠ¤íŠ¸ (ì£¼í–‰ëª¨ë“œìš©)
                    sign_classes = ['traffic_sign', 'stop_sign', 'yield_sign', 'speed_limit', 'no_entry', 'traffic_light', 'warning_sign']
                    is_sign = any(sign_class in class_name.lower() for sign_class in sign_classes)
                    
                    # 3. ê°€ë ¤ì§ ë¶„ì„ (í‘œì§€íŒì— ëŒ€í•´ì„œë§Œ)
                    occ_ratio, occ_method = 0.0, "none"
                    severity = "ì •ìƒ"
                    if is_sign:
                        try:
                            roi = frame[int(y1):int(y2), int(x1):int(x2)]
                            if roi.size > 0:
                                occ_ratio, occ_method = compute_occ_ratio_roi(roi)
                                severity = severity_from_occ(occ_ratio)
                        except Exception as e:
                            print(f"   ê°€ë ¤ì§ ë¶„ì„ ì˜¤ë¥˜: {e}")
                    
                    # ìƒ‰ìƒ ê²°ì •
                    if is_sign:
                        color = get_severity_color(severity)  # ê°€ë ¤ì§ ì •ë„ë³„ ìƒ‰ìƒ
                    elif is_vehicle:
                        color = (255, 0, 0)  # íŒŒë‘ (ì°¨ëŸ‰)
                    else:
                        color = (128, 128, 128)  # íšŒìƒ‰ (ê¸°íƒ€)
                    
                    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                    cv2.rectangle(result_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
                    
                    # ë¼ë²¨ ìƒì„± (í•œê¸€)
                    labels = [f"{class_name}: {conf:.2f}"]
                    
                    if is_vehicle:
                        labels.append("ì°¨ëŸ‰")
                    elif is_sign:
                        labels.append(f"í‘œì§€íŒ | ê°€ë¦¼:{occ_ratio:.2f} ({severity}) [{occ_method}]")
                    else:
                        labels.append("ê¸°íƒ€")
                    
                    # ë¼ë²¨ í‘œì‹œ (PIL ì‚¬ìš©ìœ¼ë¡œ í•œê¸€ ì§€ì›)
                    y_offset = int(y1) - 10
                    for i, label in enumerate(labels):
                        label_y = y_offset - i * 25
                        if label_y < 25:
                            label_y = int(y2) + 25 + i * 25
                        
                        result_frame = put_text_pil(result_frame, label, (int(x1), label_y), 
                                                   font_size=16, color=color)
                    
                    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡
                    test_results.append({
                        'image_name': os.path.basename(image_path),
                        'class_name': class_name,
                        'confidence': conf,
                        'is_vehicle': is_vehicle,
                        'is_sign': is_sign,
                        'occ_ratio': occ_ratio,
                        'occ_severity': severity,
                        'occ_method': occ_method,
                        'bbox': (x1, y1, x2, y2)
                    })
                
                # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
                result_filename = f"test_result_{img_idx+1:03d}_{os.path.basename(image_path)}"
                result_path = os.path.join(output_folder_path, result_filename)
                cv2.imwrite(result_path, result_frame)
                
                print(f"   âœ… ê²°ê³¼ ì €ì¥: {result_filename}")
                
                # ì‹¤ì‹œê°„ ì´ë¯¸ì§€ í‘œì‹œ
                display_frame = result_frame.copy()
                
                # í™”ë©´ í¬ê¸°ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ
                screen_height = 800  # ìµœëŒ€ ë†’ì´
                if display_frame.shape[0] > screen_height:
                    scale = screen_height / display_frame.shape[0]
                    new_width = int(display_frame.shape[1] * scale)
                    display_frame = cv2.resize(display_frame, (new_width, screen_height))
                
                # ì •ë³´ ì˜¤ë²„ë ˆì´ ì¶”ê°€
                overlay = display_frame.copy()
                cv2.rectangle(overlay, (10, 10), (400, 120), (0, 0, 0), -1)
                cv2.addWeighted(overlay, 0.7, display_frame, 0.3, 0, display_frame)
                
                # í…ìŠ¤íŠ¸ ì •ë³´ í‘œì‹œ (PIL ì‚¬ìš©ìœ¼ë¡œ í•œê¸€ ì§€ì›)
                info_texts = [
                    f"ì´ë¯¸ì§€ {img_idx + 1}/{len(test_images)}: {os.path.basename(image_path)}",
                    f"ì „ì²´ ê²€ì§€: {len(detections)}ê°œ",
                    f"ì°¨ëŸ‰: {sum(1 for d in detections if d[6].lower() in ['car', 'truck', 'bus', 'motorbike', 'bicycle', 'van'])}ê°œ",
f"í‘œì§€íŒ: {sum(1 for d in detections if any(s in d[6].lower() for s in ['traffic_sign', 'stop_sign', 'yield_sign', 'speed_limit', 'no_entry', 'traffic_light', 'warning_sign']))}ê°œ"
                ]
                
                # PILì„ ì‚¬ìš©í•´ì„œ í•œê¸€ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
                for i, text in enumerate(info_texts):
                    display_frame = put_text_pil(display_frame, text, (15, 30 + i * 25), font_size=20, color=(255, 255, 255))
                
                # OpenCV ì°½ì— í‘œì‹œ
                cv2.namedWindow('Test Results', cv2.WINDOW_NORMAL)
                cv2.imshow('Test Results', display_frame)
                
                # í‚¤ ì…ë ¥ ëŒ€ê¸°
                print(f"   ğŸ‘€ ê²°ê³¼ í™•ì¸ ì¤‘... (ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ë©´ ë‹¤ìŒ ì´ë¯¸ì§€ë¡œ, 'q'ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ)")
                key = cv2.waitKey(0) & 0xFF
                
                if key == ord('q') or key == 27:  # 'q' ë˜ëŠ” ESC í‚¤
                    print(f"   ğŸ›‘ ì‚¬ìš©ìê°€ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                    cv2.destroyAllWindows()
                    break
                
                cv2.destroyWindow('Test Results')
                
            except Exception as e:
                print(f"   âŒ YOLO ê²€ì§€ ì‹¤íŒ¨: {e}")
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ CSV ì €ì¥
        if test_results:
            df_test = pd.DataFrame(test_results)
            
            # í†µê³„ ê³„ì‚°
            total_detections = len(df_test)
            vehicle_detections = len(df_test[df_test['is_vehicle'] == True])
            sign_detections = len(df_test[df_test['is_sign'] == True])
            occluded_signs = len(df_test[(df_test['is_sign'] == True) & (df_test['occ_ratio'] > 0.15)])
            
            test_csv_path = os.path.join(output_folder_path, 
                f"{current_datetime}_image_test_results.csv")
            df_test.to_csv(test_csv_path, index=False, encoding='utf-8-sig')
            
            # í…ŒìŠ¤íŠ¸ ìš”ì•½ ìƒì„±
            summary_text = []
            summary_text.append("=== ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ===")
            summary_text.append(f"í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(test_images)}ì¥")
            summary_text.append(f"ì´ ê²€ì§€ ê°ì²´: {total_detections}ê°œ")
            summary_text.append(f"")
            summary_text.append("== ëª¨ë“œë³„ ê²€ì§€ ì„±ëŠ¥ ==")
            summary_text.append(f"ì°¨ëŸ‰ ê²€ì§€ (í•­ê³µë·°/í˜¸ëª¨ê·¸ë˜í”¼): {vehicle_detections}ê°œ")
            summary_text.append(f"í‘œì§€íŒ ê²€ì§€ (ì£¼í–‰ëª¨ë“œ): {sign_detections}ê°œ")
            summary_text.append(f"")
            summary_text.append("== ê°€ë ¤ì§ ë¶„ì„ ê²°ê³¼ ==")
            summary_text.append(f"ë¶„ì„ëœ í‘œì§€íŒ: {sign_detections}ê°œ")
            summary_text.append(f"ê°€ë ¤ì§„ í‘œì§€íŒ: {occluded_signs}ê°œ")
            
            if sign_detections > 0:
                occlusion_rate = (occluded_signs / sign_detections) * 100
                summary_text.append(f"ê°€ë ¤ì§ ë¹„ìœ¨: {occlusion_rate:.1f}%")
            
            summary_text.append(f"")
            summary_text.append(f"ì‚¬ìš©ëœ ëª¨ë¸: {os.path.basename(model_path)}")
            
            summary_path = os.path.join(output_folder_path, 
                f"{current_datetime}_image_test_summary.txt")
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(summary_text))
            
            print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            print(f"   ê²°ê³¼ CSV: {test_csv_path}")
            print(f"   ìš”ì•½ íŒŒì¼: {summary_path}")
            
            # ëª¨ë“  OpenCV ì°½ ë‹«ê¸°
            cv2.destroyAllWindows()
            
            # ê²°ê³¼ í´ë” ìë™ìœ¼ë¡œ ì—´ê¸°
            try:
                if os.name == 'nt':  # Windows
                    os.startfile(output_folder_path)
                elif os.name == 'posix':  # macOS/Linux
                    subprocess.run(['open', output_folder_path], check=True)
                print(f"ğŸ“‚ ê²°ê³¼ í´ë”ë¥¼ ì—´ì—ˆìŠµë‹ˆë‹¤: {output_folder_path}")
            except Exception as e:
                print(f"âš ï¸ í´ë” ì—´ê¸° ì‹¤íŒ¨: {e}")
            
            messagebox.showinfo("í…ŒìŠ¤íŠ¸ ì™„ë£Œ", 
                               f"ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n\n"
                               f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_folder_path}\n"
                               f"â€¢ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(test_images)}ì¥\n"
                               f"â€¢ ì´ ê²€ì§€ ê°ì²´: {total_detections}ê°œ\n"
                               f"â€¢ ì°¨ëŸ‰ ê²€ì§€: {vehicle_detections}ê°œ\n"
                               f"â€¢ í‘œì§€íŒ ê²€ì§€: {sign_detections}ê°œ\n"
                               f"â€¢ ê°€ë ¤ì§„ í‘œì§€íŒ: {occluded_signs}ê°œ\n\n"
                               f"ğŸ“‚ ê²°ê³¼ í´ë”ê°€ ìë™ìœ¼ë¡œ ì—´ë ¸ìŠµë‹ˆë‹¤!")
        else:
            cv2.destroyAllWindows()
            
            # ê²€ì§€ëœ ê°ì²´ê°€ ì—†ì–´ë„ í´ë” ì—´ê¸°
            try:
                if os.name == 'nt':  # Windows
                    os.startfile(output_folder_path)
                elif os.name == 'posix':  # macOS/Linux
                    subprocess.run(['open', output_folder_path], check=True)
                print(f"ğŸ“‚ ê²°ê³¼ í´ë”ë¥¼ ì—´ì—ˆìŠµë‹ˆë‹¤: {output_folder_path}")
            except Exception as e:
                print(f"âš ï¸ í´ë” ì—´ê¸° ì‹¤íŒ¨: {e}")
            
            messagebox.showinfo("í…ŒìŠ¤íŠ¸ ì™„ë£Œ", 
                               f"ê²€ì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
                               f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_folder_path}\n"
                               f"ğŸ“‚ ê²°ê³¼ í´ë”ê°€ ìë™ìœ¼ë¡œ ì—´ë ¸ìŠµë‹ˆë‹¤!")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì˜¤ë¥˜: {e}")
        cv2.destroyAllWindows()  # ì˜¤ë¥˜ ì‹œì—ë„ ì°½ ë‹«ê¸°
        messagebox.showerror("ì˜¤ë¥˜", f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}")

def simple_iou_matching(detections, prev_boxes, iou_threshold=0.5):
    """
    ê°„ë‹¨í•œ IoU ê¸°ë°˜ ë§¤ì¹­ (íŠ¸ë˜í‚¹ì´ ì—†ì„ ë•Œ ì‚¬ìš©)
    Args:
        detections: [(x1,y1,x2,y2,conf,cls_id), ...]
        prev_boxes: {track_id: (x1,y1,x2,y2), ...}
        iou_threshold: IoU ì„ê³„ê°’
    Returns:
        matches: {detection_idx: track_id, ...}
    """
    def calculate_iou(box1, box2):
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        if x2 <= x1 or y2 <= y1:
            return 0.0
        
        intersection = (x2 - x1) * (y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    matches = {}
    used_track_ids = set()
    
    for det_idx, detection in enumerate(detections):
        det_box = detection[:4]  # (x1,y1,x2,y2)
        best_iou = 0
        best_track_id = None
        
        for track_id, prev_box in prev_boxes.items():
            if track_id in used_track_ids:
                continue
            
            iou = calculate_iou(det_box, prev_box)
            if iou > best_iou and iou > iou_threshold:
                best_iou = iou
                best_track_id = track_id
        
        if best_track_id is not None:
            matches[det_idx] = best_track_id
            used_track_ids.add(best_track_id)
    
    return matches

def put_text_pil(frame, text, position, font_size=20, color=(255,255,255)):
    """ë‹¤ì–‘í•œ OSì—ì„œ í˜¸í™˜ë˜ëŠ” í°íŠ¸ ì‚¬ìš©"""
    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    
    # OSë³„ í°íŠ¸ ê²½ë¡œ ì„¤ì •
    font_paths = []
    if sys.platform == "win32":
        font_paths = [
            "C:/Windows/Fonts/malgun.ttf",      # í•œê¸€
            "C:/Windows/Fonts/arial.ttf",       # ì˜ì–´
            "C:/Windows/Fonts/NanumGothic.ttf", # ë‚˜ëˆ”ê³ ë”•
        ]
    elif sys.platform == "darwin":  # macOS
        font_paths = [
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",
            "/System/Library/Fonts/Arial.ttf",
        ]
    else:  # Linux
        font_paths = [
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
        ]
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ ì°¾ê¸°
    font = None
    for font_path in font_paths:
        try:
            if os.path.exists(font_path):
                font = ImageFont.truetype(font_path, font_size)
                break
        except (IOError, OSError):
            continue
    
    # ëª¨ë“  í°íŠ¸ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
    if font is None:
        font = ImageFont.load_default()
    
    draw.text(position, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

def draw_multiline_text(frame, text, position, font_size=20, color=(255,255,255)):
    lines = text.split('\n')
    y = position[1]
    for line in lines:
        frame = put_text_pil(frame, line, (position[0], y), font_size, color)
        y += font_size + 5
    return frame

class ViewTransformer:
    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:
        source = source.astype(np.float32)
        target = target.astype(np.float32)
        self.m = cv2.getPerspectiveTransform(source, target)

    def transform_points(self, points: np.ndarray) -> np.ndarray:
        if points.size == 0:
            return points
        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)
        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)
        return transformed_points.reshape(-1, 2)

def dist(a, b):
    return np.sqrt((a[0]-b[0])**2 + (a[1]-b[1])**2)

def calculate_aerial_distance(point1, point2):
    """í•­ê³µë·° ëª¨ë“œì—ì„œ ë‘ ì  ì‚¬ì´ì˜ ì‹¤ì œ ê±°ë¦¬ ê³„ì‚° (ë¯¸í„°)"""
    global aerial_meters_per_pixel
    if aerial_meters_per_pixel is None:
        return None
    
    pixel_distance = math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2)
    return pixel_distance * aerial_meters_per_pixel

def sanitize_filename(filename, max_length=100):
    """Windows í˜¸í™˜ íŒŒì¼ëª… ìƒì„± (ìµœëŒ€ 150ì ì œí•œ)"""
    # Windows ê¸ˆì§€ ë¬¸ì ì œê±°: <>:"/\|?*[]
    forbidden_chars = '<>:"/\\|?*[]'
    for char in forbidden_chars:
        filename = filename.replace(char, '')
    
    # íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
    import re
    filename = re.sub(r'[^\w\s.-]', '_', filename)
    
    # ì—°ì†ëœ ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
    filename = re.sub(r'\s+', '_', filename)
    
    # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ í•˜ë‚˜ë¡œ ì •ë¦¬
    while '__' in filename:
        filename = filename.replace('__', '_')
    
    # ì•ë’¤ ê³µë°±ê³¼ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    filename = filename.strip(' _')
    
    # ê¸¸ì´ ì œí•œ (í™•ì¥ì ê³ ë ¤)
    if len(filename) > max_length:
        # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ë¶„ë¦¬
        if '.' in filename:
            name_part, ext_part = filename.rsplit('.', 1)
            available_length = max_length - len(ext_part) - 1  # .í™•ì¥ì ê¸¸ì´ ì œì™¸
            if available_length > 0:
                filename = name_part[:available_length] + '.' + ext_part
            else:
                filename = filename[:max_length]
        else:
            filename = filename[:max_length]
    
    # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ì ìœ¼ë¡œë§Œ êµ¬ì„±ëœ ê²½ìš° ì²˜ë¦¬
    if not filename or filename.replace('.', '').strip() == '':
        filename = 'output'
    
    return filename

def create_test_output_folder(model_path):
    """í…ŒìŠ¤íŠ¸ ëª¨ë“œìš© ì¶œë ¥ í´ë” ìƒì„±"""
    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    model_name = os.path.basename(model_path)
    
    # íŒŒì¼ëª… ì •ì œ
    safe_model_name = sanitize_filename(model_name, 50)
    
    output_folder_name = f"TEST_{safe_model_name}_imonkfcwifi_{current_datetime}"
    output_folder_name = sanitize_filename(output_folder_name, 200)
    
    print(f"í…ŒìŠ¤íŠ¸ ëª¨ë“œ í´ë”ëª…: {output_folder_name}")
    
    # í˜„ì¬ ë””ë ‰í† ë¦¬ì— í´ë” ìƒì„±
    current_dir = os.getcwd()
    output_folder_path = os.path.join(current_dir, output_folder_name)
    output_folder_path = os.path.normpath(output_folder_path)
    
    print(f"ìƒì„±í•˜ë ¤ëŠ” í…ŒìŠ¤íŠ¸ í´ë” ê²½ë¡œ: {output_folder_path}")
    
    # í´ë” ìƒì„± ì‹œë„
    try:
        if not os.path.exists(output_folder_path):
            os.makedirs(output_folder_path, exist_ok=True)
            print(f"âœ… í…ŒìŠ¤íŠ¸ í´ë” ìƒì„± ì„±ê³µ: {output_folder_path}")
        else:
            print(f"âš ï¸ í´ë”ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {output_folder_path}")
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ í´ë” ìƒì„± ì‹¤íŒ¨: {e}")
        # ë°ìŠ¤í¬í†± í´ë”ë¡œ ëŒ€ì²´
        desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
        if os.path.exists(desktop_path):
            output_folder_path = os.path.join(desktop_path, output_folder_name)
            try:
                os.makedirs(output_folder_path, exist_ok=True)
                print(f"âœ… ë°ìŠ¤í¬í†±ì— í…ŒìŠ¤íŠ¸ í´ë” ìƒì„±: {output_folder_path}")
            except:
                print(f"âŒ ë°ìŠ¤í¬í†±ì—ë„ í´ë” ìƒì„± ì‹¤íŒ¨. í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©.")
                output_folder_path = current_dir
        else:
            output_folder_path = current_dir
    
    return output_folder_path, current_datetime

def create_output_folder(input_video_path, model_path):
    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    video_name = os.path.basename(input_video_path)
    model_name = os.path.basename(model_path)
    video_name_without_ext = os.path.splitext(video_name)[0]
    
    # íŒŒì¼ëª… ì •ì œ
    safe_video_name = sanitize_filename(video_name_without_ext, 100)
    safe_model_name = sanitize_filename(model_name, 50)
    
    output_folder_name = f"{safe_video_name}_{safe_model_name}_imonkfcwifi_{current_datetime}"
    output_folder_name = sanitize_filename(output_folder_name, 200)
    
    print(f"ì›ë³¸ ë¹„ë””ì˜¤ëª…: {video_name_without_ext}")
    print(f"ì •ì œëœ ë¹„ë””ì˜¤ëª…: {safe_video_name}")
    print(f"ìµœì¢… í´ë”ëª…: {output_folder_name}")
    
    # Windows ê²½ë¡œ ì²˜ë¦¬ ê°œì„ 
    video_dir = os.path.dirname(os.path.abspath(input_video_path))
    if not video_dir:  # ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš°
        video_dir = os.getcwd()
    
    output_folder_path = os.path.join(video_dir, output_folder_name)
    # ê²½ë¡œ ì •ê·œí™”
    output_folder_path = os.path.normpath(output_folder_path)
    
    print(f"ìƒì„±í•˜ë ¤ëŠ” í´ë” ê²½ë¡œ: {output_folder_path}")
    
    # í´ë” ìƒì„± ì‹œë„
    try:
        os.makedirs(output_folder_path, exist_ok=True)
        print(f"í´ë” ìƒì„± ì„±ê³µ: {output_folder_path}")
    except Exception as e:
        print(f"í´ë” ìƒì„± ì˜¤ë¥˜: {e}")
        # í´ë” ìƒì„± ì‹¤íŒ¨ ì‹œ ë” ê°„ë‹¨í•œ ì´ë¦„ìœ¼ë¡œ fallback
        fallback_name = f"output_{current_datetime}"
        fallback_path = os.path.join(os.getcwd(), fallback_name)
        try:
            os.makedirs(fallback_path, exist_ok=True)
            output_folder_path = fallback_path
            print(f"Fallback í´ë” ìƒì„± ì„±ê³µ: {output_folder_path}")
        except Exception as e2:
            print(f"Fallback í´ë” ìƒì„±ë„ ì‹¤íŒ¨: {e2}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©
            output_folder_path = os.getcwd()
            print(f"í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {output_folder_path}")
    
    return output_folder_path, current_datetime, video_name, model_name, video_name_without_ext

def open_folder(folder_path):
    if sys.platform == "win32":
        os.startfile(folder_path)
    elif sys.platform == "darwin":
        subprocess.Popen(["open", folder_path])
    else:
        subprocess.Popen(["xdg-open", folder_path])

def yolo_mouse_callback(event, x, y, flags, param):
    global custom_region_mode, current_custom_region, dragging_point, dragging_index, tracking_lines
    global aerial_view_mode  # í•­ê³µë·° ëª¨ë“œ í™•ì¸ìš©
    
    # OpenCV ì´ë²¤íŠ¸ ìƒìˆ˜ê°’ í™•ì¸
    cv2_constants = {
        'EVENT_LBUTTONDOWN': cv2.EVENT_LBUTTONDOWN,
        'EVENT_RBUTTONDOWN': cv2.EVENT_RBUTTONDOWN,
        'EVENT_MBUTTONDOWN': cv2.EVENT_MBUTTONDOWN,
        'EVENT_LBUTTONUP': cv2.EVENT_LBUTTONUP,
        'EVENT_RBUTTONUP': cv2.EVENT_RBUTTONUP,
        'EVENT_MOUSEMOVE': cv2.EVENT_MOUSEMOVE
    }
    
    # ìƒì„¸í•œ ë§ˆìš°ìŠ¤ ì´ë²¤íŠ¸ ë””ë²„ê¹…
    event_names = {
        cv2.EVENT_RBUTTONDOWN: "ìš°í´ë¦­",
        cv2.EVENT_MBUTTONDOWN: "íœ í´ë¦­", 
        cv2.EVENT_LBUTTONDOWN: "ì¢Œí´ë¦­",
        cv2.EVENT_LBUTTONUP: "ì¢Œí´ë¦­ì—…",
        cv2.EVENT_RBUTTONUP: "ìš°í´ë¦­ì—…",
        cv2.EVENT_MOUSEMOVE: "ë§ˆìš°ìŠ¤ì´ë™"
    }
    
    # ëª¨ë“  í´ë¦­ ì´ë²¤íŠ¸ì— ëŒ€í•œ ìƒì„¸ ë¡œê¹…
    if event in [cv2.EVENT_RBUTTONDOWN, cv2.EVENT_MBUTTONDOWN, cv2.EVENT_LBUTTONDOWN]:
        event_name = event_names.get(event, f'ì•Œ ìˆ˜ ì—†ëŠ” ì´ë²¤íŠ¸{event}')
        print(f"[ë§ˆìš°ìŠ¤ ì´ë²¤íŠ¸] {event_name} at ({x}, {y})")
        print(f"[ë””ë²„ê·¸] ì´ë²¤íŠ¸ ê°’: {event}")
        print(f"[OpenCV ìƒìˆ˜ê°’] ì¢Œí´ë¦­:{cv2.EVENT_LBUTTONDOWN}, ìš°í´ë¦­:{cv2.EVENT_RBUTTONDOWN}, íœ :{cv2.EVENT_MBUTTONDOWN}")
        print(f"[ë””ë²„ê·¸] ëª¨ë“œ: {'í•­ê³µë·°' if aerial_view_mode else 'í˜¸ëª¨ê·¸ë˜í”¼'}, ì»¤ìŠ¤í…€ ì˜ì—­ ëª¨ë“œ: {custom_region_mode}")
        print(f"[ë””ë²„ê·¸] í”Œë˜ê·¸: {flags}")
        
        # ì´ë²¤íŠ¸ ê°’ ì •í™•ì„± ê²€ì¦
        if event == cv2.EVENT_LBUTTONDOWN:
            print("[í™•ì¸] ì •ìƒì ì¸ ì¢Œí´ë¦­ ì´ë²¤íŠ¸")
        elif event == cv2.EVENT_RBUTTONDOWN:
            print("[í™•ì¸] ì •ìƒì ì¸ ìš°í´ë¦­ ì´ë²¤íŠ¸")
        elif event == cv2.EVENT_MBUTTONDOWN:
            print("[í™•ì¸] ì •ìƒì ì¸ íœ í´ë¦­ ì´ë²¤íŠ¸")
        else:
            print(f"[ê²½ê³ ] ì˜ˆìƒì¹˜ ëª»í•œ ì´ë²¤íŠ¸ ê°’: {event}")
    
    # ì»¤ìŠ¤í…€ ì˜ì—­ ëª¨ë“œ ì²˜ë¦¬
    if custom_region_mode:
        if event == cv2.EVENT_LBUTTONDOWN:
            current_custom_region.append((x, y))
            print(f"[ì»¤ìŠ¤í…€ ì˜ì—­] ì  ì¶”ê°€: ({x}, {y}), ì´ {len(current_custom_region)}ê°œ ì ")
        elif event == cv2.EVENT_RBUTTONDOWN:
            if len(current_custom_region) >= 3:
                custom_regions.append(np.array(current_custom_region, dtype=np.int32))
                print(f"[ì»¤ìŠ¤í…€ ì˜ì—­] ì˜ì—­ ì™„ì„±: {len(current_custom_region)}ê°œ ì ìœ¼ë¡œ êµ¬ì„±")
            current_custom_region.clear()
            custom_region_mode = False
            print("[ì»¤ìŠ¤í…€ ì˜ì—­] ëª¨ë“œ ì¢…ë£Œ")
    else:
        # ì¼ë°˜ ëª¨ë“œì—ì„œ ë§ˆìš°ìŠ¤ ì´ë²¤íŠ¸ ì²˜ë¦¬ - ìš°í´ë¦­ ë˜ëŠ” íœ í´ë¦­ìœ¼ë¡œ ì¶”ì ì„  ìƒì„±
        if event == cv2.EVENT_RBUTTONDOWN or event == cv2.EVENT_MBUTTONDOWN:
            click_type = "ìš°í´ë¦­" if event == cv2.EVENT_RBUTTONDOWN else "íœ í´ë¦­"
            print(f"[{click_type} ì²˜ë¦¬] ì¢Œí‘œ: ({x}, {y}), í•­ê³µë·° ëª¨ë“œ: {aerial_view_mode}")
            
            # ìƒˆ ì¶”ì ì„  ì‹œì‘ ë˜ëŠ” ê¸°ì¡´ ì¶”ì ì„  ì™„ì„±
            if not tracking_lines or len(tracking_lines[-1]["points"]) == 2:
                line_name = f"Tracking Line {len(tracking_lines) + 1}"
                new_line = {
                    "name": line_name, 
                    "points": [(x, y)], 
                    "count_cross": 0, 
                    "crossed_ids": set(), 
                    "type": "tracking", 
                    "persistent_crossed_ids": set()
                }
                tracking_lines.append(new_line)
                print(f"[ì¶”ì ì„  ìƒì„±] {click_type}ìœ¼ë¡œ ìƒˆ ì¶”ì ì„  ì‹œì‘: {line_name} at ({x}, {y})")
            else:
                tracking_lines[-1]["points"].append((x, y))
                line_info = tracking_lines[-1]
                print(f"[ì¶”ì ì„  ì™„ì„±] {click_type}ìœ¼ë¡œ {line_info['name']} - ì‹œì‘{line_info['points'][0]} ë({x}, {y})")
                
        elif event == cv2.EVENT_LBUTTONDOWN:
            print(f"[ì¢Œí´ë¦­ ì²˜ë¦¬] ë“œë˜ê·¸ í¬ì¸íŠ¸ ê²€ìƒ‰ at ({x}, {y})")
            
            # ê¸°ì¡´ ì¶”ì ì„ ì˜ ì ë“¤ ì¤‘ì—ì„œ ë“œë˜ê·¸ ê°€ëŠ¥í•œ ì  ì°¾ê¸°
            for i, line in enumerate(tracking_lines):
                for j, point in enumerate(line["points"]):
                    distance = dist(point, (x, y))
                    if distance < Constants.DISTANCE_THRESHOLD_PIXELS:
                        dragging_point = point
                        dragging_index = (i, j)
                        print(f"[ë“œë˜ê·¸ ì‹œì‘] {line['name']}ì˜ ì  {j} at {point} (ê±°ë¦¬: {distance:.1f})")
                        return
            print("[ì¢Œí´ë¦­] ë“œë˜ê·¸ ê°€ëŠ¥í•œ ì ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            
        elif event == cv2.EVENT_MOUSEMOVE:
            if dragging_index is not None:
                line_idx, point_idx = dragging_index
                tracking_lines[line_idx]["points"][point_idx] = (x, y)
                
        elif event == cv2.EVENT_LBUTTONUP:
            if dragging_point is not None:
                print(f"[ë“œë˜ê·¸ ì™„ë£Œ] ìƒˆ ìœ„ì¹˜: ({x}, {y})")
            dragging_point = None
            dragging_index = None
    
    # ì¶”ì ì„  ìƒíƒœ ì¶œë ¥ (ìš°í´ë¦­/íœ í´ë¦­ í›„ì—ë§Œ)
    if event in [cv2.EVENT_RBUTTONDOWN, cv2.EVENT_MBUTTONDOWN]:
        print(f"[ìƒíƒœ í™•ì¸] tracking_lines ê°œìˆ˜: {len(tracking_lines)}")
        for i, line in enumerate(tracking_lines):
            points_info = f"{len(line['points'])}ê°œ ì "
            if len(line['points']) == 2:
                points_info += f" {line['points'][0]} -> {line['points'][1]}"
            elif len(line['points']) == 1:
                points_info += f" ì‹œì‘ì : {line['points'][0]}"
            print(f"  {i+1}. {line['name']} ({line['type']}) - {points_info}")

def update_smoothing_settings(speed_smoothing, fps):
    """
    ëª¨ë“œ ë³€ê²½ ì‹œ ìŠ¤ë¬´ë”© ì„¤ì •ì„ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜
    """
    global aerial_view_mode
    
    # í˜„ì¬ ëª¨ë“œì— ë”°ë¼ ìƒˆë¡œìš´ maxlen ê³„ì‚°
    if aerial_view_mode:
        new_maxlen = int(fps * 0.3) if fps > 0 else 2  # í•­ê³µë·°: ë” ì§§ì€ ìŠ¤ë¬´ë”©
    else:
        new_maxlen = int(fps * 1) if fps > 0 else 5    # í˜¸ëª¨ê·¸ë˜í”¼: ê¸°ì¡´ ìŠ¤ë¬´ë”©
    
    # ëª¨ë“  ê¸°ì¡´ dequeì˜ maxlenì„ ìƒˆë¡œ ì„¤ì •
    for track_id in list(speed_smoothing.keys()):
        old_data = list(speed_smoothing[track_id])
        speed_smoothing[track_id] = deque(old_data[-new_maxlen:], maxlen=new_maxlen)
    
    # ìƒˆë¡œìš´ defaultdict factory í•¨ìˆ˜ë„ ì—…ë°ì´íŠ¸
    speed_smoothing.default_factory = lambda: deque(maxlen=new_maxlen)
    
    return new_maxlen

def analysis_process(input_video_path, model_path, output_folder_path, current_datetime, video_name, model_name, video_name_without_ext, help_text, rect_points, width_m, height_m):
    region_records = []
    region_unique_ids = defaultdict(set)
    global custom_region_mode, current_custom_region, custom_regions, tracking_lines
    global estimated_vertical_m, estimated_horizontal_m
    global aerial_view_mode, aerial_meters_per_pixel
    global show_trails, show_accumulated_trails, accumulated_trail_paths
    global driving_mode, detected_signs, sign_screenshots_saved
    global occlusion_tracker, occlusion_events
    display_info = True
    lane_events = []

    yolo_model = YOLO(model_path)
    # YOLO ëª¨ë¸ í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸° (ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¼ê³  ê°€ì •í•˜ê³  .get() ì‚¬ìš©)
    if hasattr(yolo_model, 'names'): # v8
        class_names = yolo_model.names
    elif hasattr(yolo_model, 'model') and hasattr(yolo_model.model, 'names'): # ì´ì „ ë²„ì „ í˜¸í™˜ì„±
        class_names = yolo_model.model.names
    else:
        print("ê²½ê³ : ëª¨ë¸ì—ì„œ í´ë˜ìŠ¤ ì´ë¦„ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„ì‹œ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # ëª¨ë¸ íŒŒì¼ì— ë”°ë¼ ì´ ë¶€ë¶„ì€ ì§ì ‘ í™•ì¸í•˜ê³  ìˆ˜ì •í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì˜ˆì‹œ: class_names = {0: 'person', 1: 'bicycle', 2: 'car', ...}
        # ìš°ì„ ì€ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ë‘ê³ , Unknownìœ¼ë¡œ ì²˜ë¦¬ë˜ë„ë¡ í•©ë‹ˆë‹¤.
        class_names = {}

    # ëª¨ë“œë³„ ê´€ì‹¬ í´ë˜ìŠ¤ ì„¤ì •
    if driving_mode:
        # ì£¼í–‰ëª¨ë“œ: í‘œì§€íŒ í´ë˜ìŠ¤ (ì‚¬ìš©ìê°€ íŒŒì¸íŠœë‹í•œ ëª¨ë¸ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ)
        desired_classes = ['traffic_sign']
        sign_detection_data = []  # í‘œì§€íŒ ê²€ì§€ ë°ì´í„° ì €ì¥
    else:
        # í•­ê³µë·°ìš©/í˜¸ëª¨ê·¸ë˜í”¼ìš© í´ë˜ìŠ¤ (VisDrone ê¸°ì¤€) - ë“œë¡  ê²€ì¶œ ì¶”ê°€
        desired_classes = ['car', 'truck', 'bus', 'motorbike', 'bicycle', 'van', 'people', 'awning-tricycle', 'tricycle', 'drone', 'uav', 'aircraft', 'helicopter']
    
    # í´ë˜ìŠ¤ ì´ë¦„ ë³€ê²½ ë§¤í•‘ (VAN -> CAR-SUV ë“±)
    class_name_mapping = {
        'van': 'CAR-SUV',
        'car': 'CAR',
        'truck': 'TRUCK',
        'bus': 'BUS',
        'motorbike': 'MOTORBIKE',
        'bicycle': 'BICYCLE',
        'people': 'PERSON'
    }
    
    def map_class_name(original_class):
        """ì›ë³¸ í´ë˜ìŠ¤ëª…ì„ ì‚¬ìš©ì ì •ì˜ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘"""
        return class_name_mapping.get(original_class.lower(), original_class.upper())
    
    def save_sign_screenshot(frame, bbox, class_name, confidence, timestamp, frame_number):
        """í‘œì§€íŒ ê²€ì§€ ì‹œ ìŠ¤í¬ë¦°ìƒ· ì €ì¥"""
        try:
            x1, y1, x2, y2 = bbox
            # ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ í™•ì¥ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
            h, w = frame.shape[:2]
            margin = 50
            x1_exp = max(0, int(x1) - margin)
            y1_exp = max(0, int(y1) - margin)
            x2_exp = min(w, int(x2) + margin)
            y2_exp = min(h, int(y2) + margin)
            
            # ìŠ¤í¬ë¦°ìƒ· ì˜ì—­ ì¶”ì¶œ
            screenshot = frame[y1_exp:y2_exp, x1_exp:x2_exp]
            
            # íŒŒì¼ëª… ìƒì„±
            timestamp_str = timestamp.strftime("%H%M%S_%f")[:-3]  # ë°€ë¦¬ì´ˆê¹Œì§€
            filename = f"sign_{class_name}_{timestamp_str}_frame{frame_number:06d}.png"
            filepath = os.path.join(output_folder_path, filename)
            
            # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
            cv2.imwrite(filepath, screenshot)
            print(f"ğŸ“¸ í‘œì§€íŒ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {filename}")
            
            return filepath
        except Exception as e:
            print(f"ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
    
    def process_sign_detection(frame, detections, timestamp, frame_number):
        """í‘œì§€íŒ ê²€ì§€ ì²˜ë¦¬ ë° ë°ì´í„° ê¸°ë¡ (ê°€ë ¤ì§ ë¶„ì„ í¬í•¨)"""
        H, W = frame.shape[:2]
        
        # ì´ì „ í”„ë ˆì„ ë°•ìŠ¤ ì •ë³´ (ê°„ë‹¨í•œ IoU ë§¤ì¹­ìš©)
        if not hasattr(process_sign_detection, 'prev_boxes'):
            process_sign_detection.prev_boxes = {}
        
        current_boxes = {}
        next_track_id = max(process_sign_detection.prev_boxes.keys(), default=0) + 1
        
        for det_idx, detection in enumerate(detections):
            x1, y1, x2, y2, conf, cls_id = detection
            
            if cls_id in class_names:
                class_name = class_names[cls_id]
                
                # === ê°€ë ¤ì§ ë¶„ì„ ì‹œì‘ ===
                try:
                    # ROI ì¶”ì¶œ
                    roi = frame[int(y1):int(y2), int(x1):int(x2)]
                    
                    if roi.size > 0:
                        # 1. ê°€ë ¤ì§ ë¹„ìœ¨ ê³„ì‚°
                        occ_full, method = compute_occ_ratio_roi(roi)
                        
                        # 2. í”„ë ˆì„ ê°€ì¥ìë¦¬ ê·¼ì²˜ì¸ì§€ í™•ì¸
                        near_edge, d_edge = is_near_edge((x1, y1, x2, y2), W, H)
                        
                        # 3. ì½”ì–´ ì˜ì—­ ê°€ë ¤ì§ ê³„ì‚°
                        occ_core = compute_occ_core(roi)
                        
                        # 4. ì—£ì§€ì»· íŒë‹¨
                        is_edge_cut = (near_edge and occ_core is not None and 
                                      (occ_full - occ_core >= Constants.EDGE_CUT_DELTA))
                        
                        # 5. íŠ¸ë™ ID í• ë‹¹ (ê°„ë‹¨í•œ IoU ë§¤ì¹­)
                        matches = simple_iou_matching([detection], process_sign_detection.prev_boxes)
                        
                        if det_idx in matches:
                            track_id = matches[det_idx]
                        else:
                            track_id = next_track_id
                            next_track_id += 1
                        
                        current_boxes[track_id] = (x1, y1, x2, y2)
                        
                        # 6. EMA ì—…ë°ì´íŠ¸
                        if track_id not in occlusion_tracker:
                            occlusion_tracker[track_id] = {
                                'occ_ema': occ_full,
                                'alarm_count': 0,
                                'last_alarm_frame': -1
                            }
                        else:
                            if not is_edge_cut:  # ì—£ì§€ì»·ì´ ì•„ë‹ ë•Œë§Œ EMA ì—…ë°ì´íŠ¸
                                occlusion_tracker[track_id]['occ_ema'] = ema_update(
                                    occlusion_tracker[track_id]['occ_ema'], occ_full)
                        
                        # 7. ê²½ë³´ ì²˜ë¦¬
                        occ_ema = occlusion_tracker[track_id]['occ_ema']
                        severity = severity_from_occ(occ_ema)
                        
                        # ì—°ì† ê²½ë³´ ì¡°ê±´ í™•ì¸
                        if (not is_edge_cut and occ_ema >= Constants.ALARM_OCC_THRESHOLD):
                            occlusion_tracker[track_id]['alarm_count'] += 1
                            
                            # ì—°ì† Ní”„ë ˆì„ ì´ìƒ ê²½ë³´ ì¡°ê±´ ì¶©ì¡±
                            if (occlusion_tracker[track_id]['alarm_count'] >= Constants.ALARM_MIN_FRAMES and
                                frame_number - occlusion_tracker[track_id]['last_alarm_frame'] > 30):  # 1ì´ˆë§ˆë‹¤
                                
                                # ê°€ë ¤ì§ ì´ë²¤íŠ¸ ë¡œê¹…
                                occlusion_event = {
                                    'timestamp': timestamp,
                                    'frame_number': frame_number,
                                    'track_id': track_id,
                                    'class_name': class_name,
                                    'occ_ratio': occ_ema,
                                    'severity': severity,
                                    'method': method,
                                    'is_edge_cut': is_edge_cut,
                                    'bbox': (x1, y1, x2, y2)
                                }
                                occlusion_events.append(occlusion_event)
                                occlusion_tracker[track_id]['last_alarm_frame'] = frame_number
                                
                                print(f"âš ï¸ ê°€ë ¤ì§ ê²½ë³´: {class_name} (Track {track_id}) - occ:{occ_ema:.2f} ({severity})")
                        else:
                            occlusion_tracker[track_id]['alarm_count'] = 0
                        
                        # 8. ì˜¤ë²„ë ˆì´ í‘œì‹œ
                        color = get_severity_color(severity)
                        
                        # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)
                        
                        # ë¼ë²¨ ìƒì„± (í•œê¸€)
                        edge_suffix = " [ê°€ì¥ìë¦¬]" if is_edge_cut else f" [{method}]"
                        occ_label = f"ê°€ë¦¼:{occ_ema:.2f} ({severity}){edge_suffix}"
                        sign_label = f"{class_name}: {conf:.2f}"
                        
                        # ë¼ë²¨ í‘œì‹œ (PIL ì‚¬ìš©ìœ¼ë¡œ í•œê¸€ ì§€ì›)
                        y_offset = int(y1) - 35
                        frame = put_text_pil(frame, sign_label, (int(x1), y_offset), 
                                           font_size=18, color=color)
                        frame = put_text_pil(frame, occ_label, (int(x1), y_offset + 25), 
                                           font_size=16, color=color)
                        
                except Exception as e:
                    print(f"ê°€ë ¤ì§ ë¶„ì„ ì˜¤ë¥˜: {e}")
                    # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ í‘œì‹œ
                    color = (0, 255, 255)  # ë…¸ë€ìƒ‰
                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)
                    label = f"{class_name}: {conf:.2f}"
                    frame = put_text_pil(frame, label, (int(x1), int(y1) - 5), 
                                       font_size=18, color=color)
                # === ê°€ë ¤ì§ ë¶„ì„ ë ===
                
                # ê¸°ì¡´ ìŠ¤í¬ë¦°ìƒ· ë° ë°ì´í„° ê¸°ë¡ ë¡œì§
                sign_key = f"{class_name}_{frame_number//30}"  # 30í”„ë ˆì„(1ì´ˆ) ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
                
                if sign_key not in sign_screenshots_saved:
                    # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
                    screenshot_path = save_sign_screenshot(frame, [x1, y1, x2, y2], class_name, conf, timestamp, frame_number)
                    
                    # ê²€ì§€ ë°ì´í„° ê¸°ë¡
                    sign_data = {
                        'timestamp': timestamp,
                        'frame_number': frame_number,
                        'class_name': class_name,
                        'confidence': conf,
                        'bbox_x1': x1,
                        'bbox_y1': y1,
                        'bbox_x2': x2,
                        'bbox_y2': y2,
                        'screenshot_path': screenshot_path
                    }
                    
                    sign_detection_data.append(sign_data)
                    sign_screenshots_saved[sign_key] = True
                    
                    print(f"ğŸš¦ ìƒˆë¡œìš´ í‘œì§€íŒ ê²€ì§€: {class_name} (ì‹ ë¢°ë„: {conf:.2f})")
        
        # ì´ì „ í”„ë ˆì„ ë°•ìŠ¤ ì •ë³´ ì—…ë°ì´íŠ¸
        process_sign_detection.prev_boxes = current_boxes
    
    print(f"YOLO ëª¨ë¸ì˜ í´ë˜ìŠ¤ ëª©ë¡ (ë˜ëŠ” ê°ì§€ëœ ID): {class_names}")
    print(f"í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘: VAN -> CAR-SUV, CAR -> CAR ë“±")


    tracker = DeepSort(max_age=Constants.TRACKER_MAX_AGE, 
                       n_init=Constants.TRACKER_MIN_HITS,
                       max_iou_distance=Constants.TRACKER_IOU_THRESHOLD,
                       max_cosine_distance=0.4,
                       nn_budget=None,
                       embedder="mobilenet")
    # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±ì„ ìœ„í•´ íŠ¹ìˆ˜ë¬¸ì ì œê±°
    def clean_filename(filename):
        # Windowsì—ì„œ í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ìë“¤ ì œê±°
        import re
        # íŠ¹ìˆ˜ë¬¸ìë¥¼ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ëŒ€ì²´
        filename = re.sub(r'[<>:"/\\|?*\[\]]', '_', filename)
        # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        filename = re.sub(r'_+', '_', filename)
        # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ (í™•ì¥ì ì œì™¸í•˜ê³  100ì)
        if len(filename) > 100:
            filename = filename[:100]
        return filename.strip('_')
    
    clean_video_name = clean_filename(video_name)
    clean_model_name = clean_filename(model_name)
    output_video_path = os.path.join(output_folder_path, f"{current_datetime}_{clean_model_name}_{clean_video_name}_video.avi")
    
    print(f"ğŸ¬ ë¹„ë””ì˜¤ ì¶œë ¥ ê²½ë¡œ: {output_video_path}")
    print(f"   ì›ë³¸ ë¹„ë””ì˜¤ëª…: {video_name}")
    print(f"   ì •ë¦¬ëœ ë¹„ë””ì˜¤ëª…: {clean_video_name}")
    cap = cv2.VideoCapture(input_video_path)
    if not cap.isOpened():
        messagebox.showerror("ì˜¤ë¥˜", f"ì˜ìƒ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_video_path}")
        return

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = float(cap.get(cv2.CAP_PROP_FPS))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if fps == 0:
        print("ê²½ê³ : ë¹„ë””ì˜¤ FPSê°€ 0ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ 30ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        fps = 30.0


    # ë‹¤ì–‘í•œ ì½”ë± ì‹œë„ (Windows í˜¸í™˜ì„± ìš°ì„ ìˆœìœ„)
    codecs_to_try = [
        ('MJPG', 'avi'),  # Motion JPEG - ê°€ì¥ í˜¸í™˜ì„± ë†’ìŒ
        ('DIVX', 'avi'),  # DivX ì½”ë±
        ('XVID', 'avi'),  # XviD ì½”ë±
        ('mp4v', 'mp4'),  # MPEG-4
        ('I420', 'avi'),  # Raw YUV 4:2:0
        ('IYUV', 'avi')   # Raw YUV (fallback)
    ]
    
    out = None
    for codec, ext in codecs_to_try:
        try:
            fourcc = cv2.VideoWriter_fourcc(*codec)
            test_path = output_video_path.replace('.avi', f'.{ext}')
            print(f"ì½”ë± {codec} ì‹œë„ ì¤‘... ê²½ë¡œ: {test_path}")
            print(f"ë¹„ë””ì˜¤ ì„¤ì •: {width}x{height} @ {fps} FPS")
            out = cv2.VideoWriter(test_path, fourcc, fps, (width, height))
            if out.isOpened():
                output_video_path = test_path
                print(f"âœ… ë¹„ë””ì˜¤ ì¶œë ¥ ì„¤ì • ì„±ê³µ: {codec} ì½”ë±")
                break
            else:
                print(f"âŒ {codec} ì½”ë± ì‹¤íŒ¨")
                out.release()
                out = None
        except Exception as e:
            print(f"âŒ {codec} ì½”ë± ì˜¤ë¥˜: {e}")
            if out:
                out.release()
            out = None
            continue
    
    if not out or not out.isOpened():
        print("ê²½ê³ : ë¹„ë””ì˜¤ ì¶œë ¥ì„ ì„¤ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€ ì‹œë„ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
        print(f"ì‹œë„í•œ ê²½ë¡œ: {output_video_path}")
        print(f"ë¹„ë””ì˜¤ í¬ê¸°: {width}x{height}, FPS: {fps}")
        
        # ìµœí›„ì˜ ì‹œë„ - ê²½ë¡œ ê¸¸ì´ ì œí•œìœ¼ë¡œ íŒŒì¼ëª… ë‹¨ì¶•
        try:
            video_dir = os.path.dirname(output_video_path)
            # ì›ë³¸ íŒŒì¼ëª…ì—ì„œ í•µì‹¬ ë¶€ë¶„ë§Œ ì¶”ì¶œ (30ìë¡œ ì œí•œ)
            original_name = os.path.splitext(os.path.basename(input_video_path))[0]
            # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê¸¸ì´ ì œí•œ
            safe_name = ''.join(c for c in original_name if c.isalnum() or c in '-_')[:30]
            simple_name = f"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_{model_name}_{safe_name}_video.mp4"
            simple_path = os.path.join(video_dir, simple_name)
            
            print(f"ê°„ë‹¨í•œ ê²½ë¡œë¡œ ì‹œë„: {simple_path}")
            fourcc = cv2.VideoWriter_fourcc('m','p','4','v')  # MP4 í˜¸í™˜ ì½”ë±
            out = cv2.VideoWriter(simple_path, fourcc, fps, (width, height))
            
            if out.isOpened():
                output_video_path = simple_path
                print(f"âœ… ê°„ë‹¨í•œ ê²½ë¡œë¡œ ì„±ê³µ!")
            else:
                out.release()
                print("âŒ ìµœì¢… ì‹œë„ ì‹¤íŒ¨ - ë¹„ë””ì˜¤ ì €ì¥ ë¹„í™œì„±í™”")
                out = None
                
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ì¶œë ¥ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            out = None

    crossing_data = []
    object_positions_maxlen = max(5, int(fps * 0.5)) if fps > 0 else 5
    object_positions = defaultdict(lambda: deque(maxlen=object_positions_maxlen))
    
    # ê°ì²´ ì†ë„ ë²¡í„° ì €ì¥ì†Œ ì¶”ê°€ (í•­ê³µë·° ëª¨ë“œ ì˜ˆì¸¡ ì¶”ì ìš©)
    object_velocity_vectors = defaultdict(lambda: deque(maxlen=3))

    object_speeds = {}
    speed_records = []
    track_class = {}
    presence_data = []
    distance_deceleration_records = []
    class_counts_O = defaultdict(int)
    unique_ids_per_class = defaultdict(set)
    id_crossed_initial_line = defaultdict(bool)
    id_stepped_tracking_line = defaultdict(bool)

    # ì„±ëŠ¥ ìµœì í™”: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
    num_frames_smoothing = int(fps * 1) if fps > 0 else 5
    max_speed_history = int(fps * 2) if fps > 0 else 10  # ìµœëŒ€ 2ì´ˆ ê¸°ë¡
    speed_smoothing = defaultdict(lambda: deque(maxlen=num_frames_smoothing))
    speed_history = defaultdict(lambda: deque(maxlen=max_speed_history))

    region_tracking_counts = {}

    def assign_class_to_track(tracks, detections_list):
        for tr in tracks:
            tx1, ty1, tx2, ty2, tid = map(int, tr)
            t_center = ((tx1 + tx2) / 2, (ty1 + ty2) / 2)
            min_dist_val = float('inf')
            chosen_class = None
            for det_item in detections_list:
                dx1, dy1, dx2, dy2, conf, cls_idx = det_item
                d_center = ((dx1 + dx2) / 2, (dy1 + dy2) / 2)
                d_val = dist(t_center, d_center)
                if d_val < min_dist_val:
                    min_dist_val = d_val
                    # .get()ì„ ì‚¬ìš©í•˜ì—¬ class_namesê°€ ë”•ì…”ë„ˆë¦¬ì¼ ë•Œ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                    original_class = class_names.get(int(cls_idx), "Unknown")
                    chosen_class = map_class_name(original_class) if original_class != "Unknown" else "Unknown"

            if chosen_class is not None and original_class in desired_classes and tid not in track_class:
                track_class[tid] = chosen_class
                unique_ids_per_class[chosen_class].add(tid)


    def is_point_near_line(point, start, end, threshold=10):
        x0, y0 = point
        x1, y1 = start
        x2, y2 = end

        line_seg_length_sq = (x2 - x1)**2 + (y2 - y1)**2
        if line_seg_length_sq == 0:
            return dist(point, start) < threshold

        numerator = abs((y2 - y1)*x0 - (x2 - x1)*y0 + x2*y1 - y2*x1)
        denominator = math.sqrt(line_seg_length_sq)
        distance_to_line = numerator / denominator

        if distance_to_line >= threshold : return False

        dot_product = (x0 - x1)*(x2 - x1) + (y0 - y1)*(y2 - y1)
        if dot_product < 0: return dist(point, start) < threshold

        dot_product2 = (x0 - x2)*(x1 - x2) + (y0 - y2)*(y1 - y2)
        if dot_product2 < 0: return dist(point, end) < threshold

        return True


    cv2.namedWindow('YOLO Detection', cv2.WINDOW_NORMAL)
    # cv2.resizeWindow('YOLO Detection', 1920, 1080) # í•„ìš”ì— ë”°ë¼ ì£¼ì„ í•´ì œ
    cv2.setMouseCallback('YOLO Detection', yolo_mouse_callback)

    poly_pts_src = np.array(rect_points, dtype=np.float32)

    TARGET_WIDTH = width_m
    TARGET_HEIGHT = height_m
    TARGET = np.array(
        [[0, 0], [TARGET_WIDTH, 0], [TARGET_WIDTH, TARGET_HEIGHT], [0, TARGET_HEIGHT]],
        dtype=np.float32
    )
    view_transformer = ViewTransformer(source=poly_pts_src, target=TARGET)

    estimated_vertical_m = height_m
    estimated_horizontal_m = width_m
    print(f"Estimated vertical dimension: {estimated_vertical_m:.2f} m")
    print(f"Estimated horizontal dimension: {estimated_horizontal_m:.2f} m")
    
    # í•­ê³µë·° ëª¨ë“œ ì„¤ì • ì•ˆë‚´
    print("\n" + "="*50)
    print("í•­ê³µë·° ëª¨ë“œ ì‚¬ìš©ë²•:")
    print("- 'a' í‚¤: í•­ê³µë·° ëª¨ë“œ ON/OFF")
    print("- í•­ê³µë·° ëª¨ë“œì—ì„œ ë§ˆìš°ìŠ¤ë¡œ ê¸°ì¤€ì„  ê·¸ë¦¬ê¸°")
    print("- ê¸°ì¤€ì„ ì˜ ì‹¤ì œ ê±°ë¦¬ ì…ë ¥í•˜ì—¬ ìŠ¤ì¼€ì¼ ì„¤ì •")
    print("- 'r' í‚¤: í•­ê³µë·° ìŠ¤ì¼€ì¼ ì´ˆê¸°í™”")
    print("="*50)

    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
    current_frame = 0
    
    # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ ë³€ìˆ˜
    last_progress_time = time.time()
    
    while cap.isOpened() and current_frame < total_frames:
        ret, frame = cap.read()
        if not ret:
            break

        # í˜„ì¬ ì‹œê°„ê³¼ ì´ ì‹œê°„ì„ HH:MM:SS í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” ë¶€ë¶„ ì¶”ê°€
        current_time_sec = current_frame / fps if fps > 0 else 0
        total_time_sec = total_frames / fps if fps > 0 else 0
        
        # ì‹œê°„ í¬ë§· ë³€í™˜ í•¨ìˆ˜
        def format_time(seconds):
            hours, remainder = divmod(seconds, 3600)
            minutes, seconds = divmod(remainder, 60)
            return f"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}"
        
        current_time_formatted = format_time(current_time_sec)
        total_time_formatted = format_time(total_time_sec)
        timestamp_text = f"Video Time: {current_time_formatted}/{total_time_formatted}"
        
        # ê²€ì •ìƒ‰ ë°°ê²½ì˜ ë°˜íˆ¬ëª… ìƒìì— í°ìƒ‰ í…ìŠ¤íŠ¸ë¡œ ì‹œê°„ í‘œì‹œ
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (400, 50), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        cv2.putText(frame, timestamp_text, (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 
                    0.8, (255, 255, 255), 2)

        # YOLO ì˜ˆì¸¡ - ê°•í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            results = yolo_model.predict(source=frame, 
                                       imgsz=Constants.DEFAULT_IMGSZ, 
                                       conf=Constants.CONFIDENCE_THRESHOLD, 
                                       iou=Constants.IOU_THRESHOLD)
        except (RuntimeError, MemoryError) as e:
            print(f"YOLO ì˜ˆì¸¡ ì˜¤ë¥˜ (í”„ë ˆì„ {current_frame}): {e}")
            results = []
        except Exception as e:
            print(f"YOLO ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (í”„ë ˆì„ {current_frame}): {e}")
            results = []
        detections = []
        for result_item in results:
            boxes = result_item.boxes
            for box in boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                conf = float(box.conf[0])
                cls_idx = int(box.cls[0])
                original_cls_name = class_names.get(cls_idx, "Unknown") # .get() ì‚¬ìš©
                # í•­ê³µë·°ìš© ë” ê´€ëŒ€í•œ í•„í„°ë§
                if conf < Constants.FILTER_THRESHOLD or original_cls_name not in desired_classes:
                    continue
                detections.append([x1, y1, x2, y2, conf, cls_idx])
        
        # ì£¼í–‰ëª¨ë“œì—ì„œì˜ í‘œì§€íŒ ê²€ì§€ ì²˜ë¦¬
        if driving_mode and len(detections) > 0:
            current_timestamp = datetime.now()
            process_sign_detection(frame, detections, current_timestamp, current_frame)

        # ì£¼í–‰ëª¨ë“œì—ì„œëŠ” ì¶”ì ê¸° ìƒëµ, ë‹¤ë¥¸ ëª¨ë“œì—ì„œëŠ” ì¶”ì ê¸° ì‚¬ìš©
        tracked_objects = np.empty((0, 5))  # ê¸°ë³¸ê°’: [x1, y1, x2, y2, track_id] í˜•ì‹
        
        if not driving_mode:
            # ê°•í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬ ë° DeepSORT í˜¸í™˜ êµ¬í˜„
            try:
                if len(detections) > 0:
                    # DeepSORT requires detections as list of tuples: ([left, top, w, h], confidence, class)
                    deepsort_detections = []
                    for d in detections:
                        x1, y1, x2, y2, conf, cls_idx = d
                        # Convert [x1, y1, x2, y2] to [left, top, w, h]
                        left, top, w, h = x1, y1, x2 - x1, y2 - y1
                        original_cls_name = class_names.get(int(cls_idx), "unknown") if class_names else "unknown"
                        deepsort_detections.append(([left, top, w, h], conf, original_cls_name))
                    
                    tracks = tracker.update_tracks(deepsort_detections, frame=frame)
                else:
                    # ë¹ˆ ê²€ì¶œì—ì„œë„ ì¶”ì ê¸° ì—…ë°ì´íŠ¸ (ê¸°ì¡´ íŠ¸ë™ ìœ ì§€ë¥¼ ìœ„í•´)
                    tracks = tracker.update_tracks([], frame=frame)
            
                # Convert DeepSORT tracks to SORT-compatible format [x1, y1, x2, y2, track_id]
                tracked_objects_list = []
                
                # ë””ë²„ê¹… ì •ë³´
                if current_frame % 100 == 0:  # 100í”„ë ˆì„ë§ˆë‹¤ ë¡œê·¸
                    print(f"í”„ë ˆì„ {current_frame}: {len(tracks)}ê°œ íŠ¸ë™ ì²˜ë¦¬ ì¤‘")
                
                for track in tracks:
                    # íŠ¸ë™ í™•ì¸ ìƒíƒœ ê²€ì¦ (DeepSORTì—ì„œëŠ” ì´ˆê¸°ì— tentative ìƒíƒœì¼ ìˆ˜ ìˆìŒ)
                    if not hasattr(track, 'is_confirmed'):
                        continue
                    
                    # í™•ì¸ëœ íŠ¸ë™ë§Œ ì‚¬ìš© (n_init ë²ˆì˜ ì—°ì† ê°ì§€ í›„)
                    if not track.is_confirmed():
                        continue
                        
                    # íŠ¸ë™ ID ê²€ì¦
                    if not hasattr(track, 'track_id'):
                        continue
                        
                    # bbox ì¶”ì¶œ ë° ê²€ì¦
                    try:
                        # DeepSORTì˜ to_ltrb() ë©”ì†Œë“œ ì‚¬ìš© (left, top, right, bottom)
                        bbox = track.to_ltrb()
                        
                        # bbox ìœ íš¨ì„± ê²€ì¦
                        if bbox is None or len(bbox) != 4:
                            continue
                            
                        # ìœ íš¨í•œ ì¢Œí‘œê°’ ê²€ì¦
                        x1, y1, x2, y2 = bbox
                        if not all(isinstance(coord, (int, float, np.number)) for coord in bbox):
                            continue
                            
                        # bbox í¬ê¸° ê²€ì¦
                        if x2 <= x1 or y2 <= y1:
                            continue
                            
                        tracked_objects_list.append([
                            float(x1), float(y1), float(x2), float(y2), int(track.track_id)
                        ])
                        
                    except (AttributeError, TypeError, ValueError) as e:
                        print(f"íŠ¸ë™ {getattr(track, 'track_id', 'Unknown')} bbox ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                        continue
                
                # ê²°ê³¼ ë°°ì—´ ìƒì„±
                if tracked_objects_list:
                    tracked_objects = np.array(tracked_objects_list, dtype=np.float32)
                else:
                    tracked_objects = np.empty((0, 5), dtype=np.float32)
                    
            except (ValueError, RuntimeError, MemoryError) as e:
                print(f"ì¶”ì  ì˜¤ë¥˜ (í”„ë ˆì„ {current_frame}): {e}")
                tracked_objects = np.empty((0, 5), dtype=np.float32)
            except Exception as e:
                print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì¶”ì  ì˜¤ë¥˜ (í”„ë ˆì„ {current_frame}): {e}")
                print(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                tracked_objects = np.empty((0, 5), dtype=np.float32)

        # ì£¼í–‰ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ í´ë˜ìŠ¤ í• ë‹¹ ìˆ˜í–‰
        if (not driving_mode and
            isinstance(tracked_objects, np.ndarray) and 
            tracked_objects.size > 0 and 
            len(tracked_objects.shape) == 2 and 
            tracked_objects.shape[1] == 5 and 
            len(detections) > 0):
            assign_class_to_track(tracked_objects, detections)

        current_time = current_frame / fps if fps > 0 else 0

        if display_info: # 'p' í‚¤ë¡œ í† ê¸€ë˜ëŠ” ì •ë³´
            if not driving_mode:  # ì£¼í–‰ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ ê±°ë¦¬ ì •ë³´ í‘œì‹œ
                cv2.putText(frame, f"Est. Vert: {estimated_vertical_m:.2f}m Horiz: {estimated_horizontal_m:.2f}m", (50, 100),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)
            # ì£¼í–‰ëª¨ë“œì™€ í•­ê³µë·° ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ ROI ë°•ìŠ¤ í‘œì‹œ
            if not aerial_view_mode and not driving_mode and len(rect_points) == 4:
                pts_roi = np.array(rect_points, dtype=np.int32)
                cv2.polylines(frame, [pts_roi], True, (0,255,0), 2)


        current_tracked_ids = set()
        
        # tracked_objects ìœ íš¨ì„± ê²€ì¦ í›„ ë°˜ë³µ ì²˜ë¦¬
        if (isinstance(tracked_objects, np.ndarray) and 
            tracked_objects.size > 0 and 
            len(tracked_objects.shape) == 2 and 
            tracked_objects.shape[1] == 5):
            
            for trk_item in tracked_objects:
                try:
                    # íŠ¸ë™ ë°ì´í„° ì–¸íŒ¨í‚¹ ë° ê²€ì¦
                    if not isinstance(trk_item, (np.ndarray, list)) or len(trk_item) != 5:
                        continue
                    x1_trk, y1_trk, x2_trk, y2_trk, track_id = map(float, trk_item)
                    track_id = int(track_id)
                    
                    # ìœ íš¨í•œ bboxì¸ì§€ ê²€ì¦
                    if x2_trk <= x1_trk or y2_trk <= y1_trk:
                        continue
                        
                    label = f"ID {track_id}"

                    center_x = (x1_trk + x2_trk) / 2
                    center_y = (y1_trk + y2_trk) / 2
                    bbox_y2 = y2_trk

                    object_positions[track_id].append((center_x, center_y, bbox_y2, current_time))
                    current_tracked_ids.add(track_id)
                    
                    # ëˆ„ì  trail ê²½ë¡œì— í˜„ì¬ ìœ„ì¹˜ ì €ì¥ (í•­ìƒ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìˆ˜ì§‘)
                    if track_id not in accumulated_trail_paths:
                        accumulated_trail_paths[track_id] = []
                    accumulated_trail_paths[track_id].append((center_x, center_y))

                    xs_list = [p[0] for p in object_positions[track_id]]
                    ys_center_list = [p[1] for p in object_positions[track_id]]

                    # í•­ê³µë·° ëª¨ë“œì—ì„œ ë” ê°•ë ¥í•œ ì‹¤ì‹œê°„ ì¶”ì  ì ìš©
                    if aerial_view_mode and len(object_positions[track_id]) >= 2:
                        # ìµœê·¼ 2-3ê°œ ìœ„ì¹˜ë§Œ ì‚¬ìš©í•˜ì—¬ ì§€ì—° ìµœì†Œí™”
                        recent_positions = list(object_positions[track_id])[-3:]
                        recent_xs = [p[0] for p in recent_positions]
                        recent_ys = [p[1] for p in recent_positions]
                        
                        # ìµœì‹  ìœ„ì¹˜ì— ë§¤ìš° ë†’ì€ ê°€ì¤‘ì¹˜ ì ìš© (0.7~1.0)
                        weights = np.linspace(0.7, 1.0, len(recent_xs))
                        smooth_x = np.average(recent_xs, weights=weights)
                        smooth_y_center = np.average(recent_ys, weights=weights)
                        
                        # ë” ê°•ë ¥í•œ ì˜ˆì¸¡: ìµœê·¼ 2í”„ë ˆì„ ì†ë„ë¡œ ë” ë©€ë¦¬ ì˜ˆì¸¡
                        if len(recent_positions) >= 2:
                            current_pos = recent_positions[-1]
                            prev_pos = recent_positions[-2]
                            dt = max(0.016, current_pos[3] - prev_pos[3])  # ìµœì†Œ 16ms (60fps)
                            
                            vx = (current_pos[0] - prev_pos[0]) / dt
                            vy = (current_pos[1] - prev_pos[1]) / dt
                            object_velocity_vectors[track_id].append((vx, vy))
                            
                            # ë” ê¸´ ì˜ˆì¸¡ ì‹œê°„ìœ¼ë¡œ ì§€ì—° ë³´ìƒ ê°•í™”
                            prediction_time = 0.1  # 100ms ì• ì˜ˆì¸¡
                            smooth_x += vx * prediction_time
                            smooth_y_center += vy * prediction_time
                    else:
                        # í˜¸ëª¨ê·¸ë˜í”¼ ëª¨ë“œ: ê¸°ì¡´ ë‹¨ìˆœ í‰ê·  ì‚¬ìš©
                        smooth_x = np.mean(xs_list) if xs_list else center_x
                        smooth_y_center = np.mean(ys_center_list) if ys_center_list else center_y

                    speed_kph = None
                    acceleration_mps2 = None
                    deceleration_mps2 = None
                    acceleration_percent = None
                    deceleration_percent = None

                    if len(object_positions[track_id]) >= 2 and len(object_positions[track_id]) >= object_positions_maxlen // 2:
                        curr_pos_data = object_positions[track_id][-1]
                        prev_pos_data = object_positions[track_id][0]

                        dt_interval = curr_pos_data[3] - prev_pos_data[3]

                        if dt_interval > 0:
                            # í•­ê³µë·° ëª¨ë“œ ë˜ëŠ” ê¸°ì¡´ í˜¸ëª¨ê·¸ë˜í”¼ ëª¨ë“œì— ë”°ë¼ ê±°ë¦¬ ê³„ì‚° ë°©ì‹ ì„ íƒ
                            if aerial_view_mode and aerial_meters_per_pixel is not None:
                                # í•­ê³µë·° ëª¨ë“œ: ì„ í˜• ìŠ¤ì¼€ì¼ ê¸°ë°˜ ê±°ë¦¬ ê³„ì‚°
                                current_pixel_pos = (curr_pos_data[0], curr_pos_data[2])
                                prev_pixel_pos = (prev_pos_data[0], prev_pos_data[2])
                                dist_m_val = calculate_aerial_distance(prev_pixel_pos, current_pixel_pos)
                            else:
                                # ê¸°ì¡´ í˜¸ëª¨ê·¸ë˜í”¼ ëª¨ë“œ
                                real_pos_current = view_transformer.transform_points(np.array([[curr_pos_data[0], curr_pos_data[2]]]))[0]
                                real_pos_prev = view_transformer.transform_points(np.array([[prev_pos_data[0], prev_pos_data[2]]]))[0]
                                dist_m_val = dist(real_pos_current, real_pos_prev)
                            
                            if dist_m_val is not None and dist_m_val > 0:
                                speed_mps = dist_m_val / dt_interval
                            else:
                                speed_mps = 0

                            speed_smoothing[track_id].append(speed_mps)
                            avg_speed_mps = np.mean(speed_smoothing[track_id]) if speed_smoothing[track_id] else speed_mps

                            object_speeds[track_id] = avg_speed_mps
                            speed_kph = avg_speed_mps * 3.6

                            speed_history[track_id].append((avg_speed_mps, current_time))
                            while speed_history[track_id] and (current_time - speed_history[track_id][0][1]) > 0.5:
                                speed_history[track_id].popleft()

                            if len(speed_history[track_id]) >= 2:
                                current_speed_data = speed_history[track_id][-1]
                                first_speed_data_in_window = speed_history[track_id][0]

                                delta_v = current_speed_data[0] - first_speed_data_in_window[0]
                                delta_t_accel = current_speed_data[1] - first_speed_data_in_window[1]

                                if delta_t_accel > 0:
                                    calculated_accel_mps2 = delta_v / delta_t_accel
                                    if abs(calculated_accel_mps2) > Constants.ACCEL_LIMIT_MPS2:
                                        calculated_accel_mps2 = None

                                    if calculated_accel_mps2 is not None:
                                        base_speed_for_percent = first_speed_data_in_window[0] + 1e-5
                                        if calculated_accel_mps2 < 0:
                                            deceleration_mps2 = abs(calculated_accel_mps2)
                                            deceleration_percent = (deceleration_mps2 / base_speed_for_percent) * 100 if base_speed_for_percent != 0 else 0
                                        else:
                                            acceleration_mps2 = calculated_accel_mps2
                                            acceleration_percent = (acceleration_mps2 / base_speed_for_percent) * 100 if base_speed_for_percent != 0 else 0

                    cls_name_trk = track_class.get(track_id, "Unknown")
                    # í´ë˜ìŠ¤ ì´ë¦„ì´ ì´ë¯¸ ë§¤í•‘ë˜ì–´ ì €ì¥ë¨ (track_classì— ì €ì¥í•  ë•Œ map_class_name ì ìš©ë¨)

                    # ì°¨ëŸ‰ íƒ€ì…ë³„ ìƒ‰ìƒ ì²˜ë¦¬
                    if cls_name_trk == 'CAR-SUV':
                        circle_color = (0, 255, 255)  # ë…¸ë€ìƒ‰ ì› (VAN -> CAR-SUV)
                        text_color = (0, 255, 255)    # ë…¸ë€ìƒ‰ í…ìŠ¤íŠ¸
                    elif cls_name_trk == 'CAR':
                        circle_color = (0, 255, 0)    # ì´ˆë¡ìƒ‰ ì›
                        text_color = (0, 255, 0)      # ì´ˆë¡ìƒ‰ í…ìŠ¤íŠ¸
                    elif cls_name_trk == 'TRUCK':
                        circle_color = (255, 0, 0)    # íŒŒë€ìƒ‰ ì›
                        text_color = (255, 0, 0)      # íŒŒë€ìƒ‰ í…ìŠ¤íŠ¸
                    elif cls_name_trk.upper() in ['DRONE', 'UAV', 'AIRCRAFT', 'HELICOPTER']:
                        circle_color = (255, 0, 255)  # ë³´ë¼ìƒ‰ ì›
                        text_color = (255, 0, 255)    # ë³´ë¼ìƒ‰ í…ìŠ¤íŠ¸
                    else:
                        circle_color = (0, 0, 255)    # ê¸°ë³¸ ë¹¨ê°„ìƒ‰ ì›
                        text_color = (0, 255, 0)      # ê¸°ë³¸ ì´ˆë¡ìƒ‰ í…ìŠ¤íŠ¸

                    # ê°ì²´ ì´ë™ ê²½ë¡œ í‘œì‹œ (trail ê¸°ëŠ¥)
                    if show_trails and len(object_positions[track_id]) > 1:
                        trail_positions = list(object_positions[track_id])
                        
                        # trail ìƒ‰ìƒ ì„¤ì • (IDë³„ë¡œ ë‹¤ë¥¸ ìƒ‰ìƒ)
                        trail_colors = [
                            (255, 100, 100),  # ì—°í•œ ë¹¨ê°•
                            (100, 255, 100),  # ì—°í•œ ì´ˆë¡
                            (100, 100, 255),  # ì—°í•œ íŒŒë‘
                            (255, 255, 100),  # ì—°í•œ ë…¸ë‘
                            (255, 100, 255),  # ì—°í•œ ìí™
                            (100, 255, 255),  # ì—°í•œ ì‹œì•ˆ
                        ]
                        trail_color = trail_colors[track_id % len(trail_colors)]
                        
                        # ì´ë™ ê²½ë¡œë¥¼ ì„ ìœ¼ë¡œ ì—°ê²°
                        for i in range(1, len(trail_positions)):
                            pt1 = (int(trail_positions[i-1][0]), int(trail_positions[i-1][1]))
                            pt2 = (int(trail_positions[i][0]), int(trail_positions[i][1]))
                            cv2.line(frame, pt1, pt2, trail_color, 2)
                    
                    # ì¶”ì ì  í‘œì‹œ
                    if aerial_view_mode:
                        # í•­ê³µë·° ëª¨ë“œ: ë” í° ì¶”ì ì 
                        cv2.circle(frame, (int(smooth_x), int(smooth_y_center)), 7, circle_color, -1)
                        cv2.circle(frame, (int(smooth_x), int(smooth_y_center)), 10, circle_color, 2)  # ë°”ê¹¥ìª½ í…Œë‘ë¦¬
                    else:
                        # í˜¸ëª¨ê·¸ë˜í”¼ ëª¨ë“œ: ê¸°ì¡´ í‘œì‹œ ë°©ì‹
                        cv2.circle(frame, (int(smooth_x), int(smooth_y_center)), 5, circle_color, -1)

                    info_label = f"{label} {cls_name_trk}"
                    cv2.putText(frame, info_label, (int(x1_trk), int(y1_trk) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2)

                    if speed_kph is not None:
                        cv2.putText(frame, f"{speed_kph:.1f}km/h", (int(x1_trk), int(y1_trk) - 30),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)

                        speed_records.append({
                            "Frame": current_frame, "Time": current_time, "ID": track_id, "Class": cls_name_trk,
                            "IsInitialLine": id_crossed_initial_line[track_id],
                            "IsSteppedTrackingLine": id_stepped_tracking_line[track_id],
                            "Speed(km/h)": speed_kph,
                            "Acceleration(m/sÂ²)": acceleration_mps2, "Deceleration(m/sÂ²)": deceleration_mps2,
                            "Acceleration (%)": acceleration_percent, "Deceleration (%)": deceleration_percent
                        })

                    presence_data.append({
                        "Frame": current_frame, "Time": current_time, "ID": track_id, "Class": cls_name_trk,
                        "IsInitialLine": id_crossed_initial_line[track_id],
                        "IsSteppedTrackingLine": id_stepped_tracking_line[track_id]
                    })

                    for line_data in tracking_lines:
                        if len(line_data["points"]) == 2:
                            start_pt, end_pt = line_data["points"]
                            point_for_line_test = (smooth_x, smooth_y_center)

                            if is_point_near_line(point_for_line_test, start_pt, end_pt) and \
                               track_id not in line_data.get("persistent_crossed_ids", set()):

                                line_data.setdefault("crossed_ids", set()).add(track_id)
                                line_data.setdefault("persistent_crossed_ids", set()).add(track_id)
                                line_data["count_cross"] = len(line_data["persistent_crossed_ids"])

                                if line_data["type"] == "initial":
                                    id_crossed_initial_line[track_id] = True
                                    if cls_name_trk != "Unknown":
                                        class_counts_O[cls_name_trk] += 1
                                    crossing_data.append({
                                        "ID": track_id, "Class": cls_name_trk, "Time": current_time,
                                        "Line": line_data["name"], "IsInitialLine": True
                                    })
                                else:
                                    id_stepped_tracking_line[track_id] = True
                                    crossing_data.append({
                                        "ID": track_id, "Class": cls_name_trk, "Time": current_time,
                                        "Line": line_data["name"], "IsInitialLine": id_crossed_initial_line[track_id]
                                    })

                                line_data.setdefault("display_crossed_ids", set()).add(track_id)

                    center_for_region_test = (smooth_x, smooth_y_center)
                    for idx, region_poly in enumerate(custom_regions):
                        if cv2.pointPolygonTest(region_poly, center_for_region_test, False) >= 0:
                            stepped_any_line = id_stepped_tracking_line[track_id]
                            lines_stepped_names = [ln["name"] for ln in tracking_lines
                                                   if track_id in ln.get("crossed_ids", set()) and ln["type"] == "tracking"]

                            region_records.append({
                                "Region": idx + 1, "Frame": current_frame, "Time": current_time,
                                "ID": track_id, "Class": cls_name_trk,
                                "SteppedTrackingLine": stepped_any_line,
                                "Speed(km/h)": speed_kph if speed_kph is not None else 0,
                                "Acceleration(m/sÂ²)": acceleration_mps2 if acceleration_mps2 is not None else None,
                                "Deceleration(m/sÂ²)": deceleration_mps2 if deceleration_mps2 is not None else None,
                                "LinesStepped": ";".join(lines_stepped_names)
                            })
                            region_unique_ids[idx].add(track_id)

                except (ValueError, TypeError, IndexError) as e:
                    print(f"íŠ¸ë™ {track_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

        # tracked_objects ìœ íš¨ì„± ì¬ê²€ì¦ (ì•ˆì „í•œ ì ‘ê·¼)
        try:
            if (isinstance(tracked_objects, np.ndarray) and 
                tracked_objects.size > 0 and 
                len(tracked_objects.shape) == 2 and 
                tracked_objects.shape[1] == 5):
                tracked_ids_in_current_frame = set([int(trk[4]) for trk in tracked_objects])
            else:
                tracked_ids_in_current_frame = set()
        except (ValueError, TypeError, IndexError):
            tracked_ids_in_current_frame = set()
        all_known_ids = set(object_positions.keys())
        disappeared_ids = all_known_ids - tracked_ids_in_current_frame

        for d_id in disappeared_ids:
            if d_id in object_positions: del object_positions[d_id]
            if d_id in object_speeds: del object_speeds[d_id]
            if d_id in track_class: del track_class[d_id]
            if d_id in speed_smoothing: del speed_smoothing[d_id]
            if d_id in speed_history: del speed_history[d_id]
            if d_id in object_velocity_vectors: del object_velocity_vectors[d_id]  # ì†ë„ ë²¡í„°ë„ ì •ë¦¬
            for line_data_item in tracking_lines:
                line_data_item.get("crossed_ids", set()).discard(d_id)
                line_data_item.get("display_crossed_ids", set()).discard(d_id)

        # ì„±ëŠ¥ ìµœì í™”: ì°¨ê°„ ê±°ë¦¬ ê³„ì‚° ê°œì„ 
        def calculate_vehicle_distances():
            """ì°¨ê°„ ê±°ë¦¬ ê³„ì‚°ì„ ìµœì í™”ëœ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬"""
            # ìµœê·¼ ì†ë„ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ìºì‹± (O(1) ì ‘ê·¼)
            latest_speed_data = {}
            for rec in reversed(speed_records[-Constants.SPEED_CACHE_SIZE:]):  # ìµœê·¼ Nê°œë§Œ í™•ì¸
                vehicle_id = rec["ID"]
                if vehicle_id not in latest_speed_data:
                    latest_speed_data[vehicle_id] = {
                        "Acceleration(m/sÂ²)": rec["Acceleration(m/sÂ²)"],
                        "Deceleration(m/sÂ²)": rec["Deceleration(m/sÂ²)"],
                        "Acceleration (%)": rec["Acceleration (%)"],
                        "Deceleration (%)": rec["Deceleration (%)"]
                    }
            
            for line_data_dist in tracking_lines:
                if line_data_dist["type"] != "tracking": 
                    continue

                active_ids_on_line = list(line_data_dist.get("persistent_crossed_ids", set()).intersection(tracked_ids_in_current_frame))
                
                if len(active_ids_on_line) < 2: 
                    continue

                # ìœ„ì¹˜ ë°ì´í„° í•œ ë²ˆì— ìˆ˜ì§‘
                valid_positions = []
                for vehicle_id in active_ids_on_line:
                    if object_positions[vehicle_id]:
                        pos_data = object_positions[vehicle_id][-1]
                        valid_positions.append((vehicle_id, pos_data))
                
                # ëª¨ë“  ìŒ ì²˜ë¦¬ (ê°œì„ ëœ ë°©ì‹)
                from itertools import combinations
                for (id1, pos1), (id2, pos2) in combinations(valid_positions, 2):
                    p1_pixel = (pos1[0], pos1[1])
                    p2_pixel = (pos2[0], pos2[1])

                    # ë³€í™˜ í¬ì¸íŠ¸ ê³„ì‚°
                    transform_points = np.array([[pos1[0], pos1[2]], [pos2[0], pos2[2]]])
                    real_positions = view_transformer.transform_points(transform_points)
                    distance_m_val2 = dist(real_positions[0], real_positions[1])

                    # ì†ë„ ì •ë³´
                    speed1_kph = object_speeds.get(id1, 0) * 3.6
                    speed2_kph = object_speeds.get(id2, 0) * 3.6

                    # ê°€ì†ë„ ì •ë³´ (ìºì‹œëœ ë°ì´í„° ì‚¬ìš©)
                    data1 = latest_speed_data.get(id1, {})
                    data2 = latest_speed_data.get(id2, {})

                    distance_deceleration_records.append({
                        "Frame": current_frame, "Time": current_time,
                        "ID1": id1, "Class1": track_class.get(id1, "Unknown"), "Speed1(km/h)": speed1_kph,
                        "Acceleration1(m/sÂ²)": data1.get("Acceleration(m/sÂ²)"), 
                        "Deceleration1(m/sÂ²)": data1.get("Deceleration(m/sÂ²)"),
                        "Acceleration1 (%)": data1.get("Acceleration (%)"), 
                        "Deceleration1 (%)": data1.get("Deceleration (%)"),
                        "ID2": id2, "Class2": track_class.get(id2, "Unknown"), "Speed2(km/h)": speed2_kph,
                        "Acceleration2(m/sÂ²)": data2.get("Acceleration(m/sÂ²)"), 
                        "Deceleration2(m/sÂ²)": data2.get("Deceleration(m/sÂ²)"),
                        "Acceleration2 (%)": data2.get("Acceleration (%)"), 
                        "Deceleration2 (%)": data2.get("Deceleration (%)"),
                        "Distance(m)": distance_m_val2, "Line": line_data_dist["name"],
                        "IsInitialLine_ID1": id_crossed_initial_line[id1],
                        "IsInitialLine_ID2": id_crossed_initial_line[id2]
                    })
                    
                    # í™”ë©´ì— ê±°ë¦¬ í‘œì‹œ
                    mid_point_pixel = ((p1_pixel[0] + p2_pixel[0]) / 2, (p1_pixel[1] + p2_pixel[1]) / 2)
                    cv2.putText(frame, f"{distance_m_val2:.1f}m", (int(mid_point_pixel[0]), int(mid_point_pixel[1])),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,255), 2)
                    cv2.line(frame, (int(p1_pixel[0]), int(p1_pixel[1])), (int(p2_pixel[0]), int(p2_pixel[1])), (255,0,255), 1)

        # ë§¤ í”„ë ˆì„ë§ˆë‹¤ ì‹¤í–‰í•˜ì§€ ë§ê³  Ní”„ë ˆì„ë§ˆë‹¤ ì‹¤í–‰ (ì„±ëŠ¥ í–¥ìƒ)
        if current_frame % Constants.DISTANCE_CALC_INTERVAL == 0:
            calculate_vehicle_distances()


        # ëˆ„ì  trail í‘œì‹œ (ë¶ˆíˆ¬ëª… ìƒ‰ìƒ ìŠ¤íƒìœ¼ë¡œ ì°¨ë¡œ ì‚¬ìš©ëŸ‰ ë¶„ì„)
        if show_accumulated_trails:
            # ë¶ˆíˆ¬ëª… overlay ìƒì„±
            overlay = frame.copy()
            
            for obj_id, path in accumulated_trail_paths.items():
                if len(path) > 1:
                    # ë¶ˆíˆ¬ëª… ìƒ‰ìƒë“¤ (ê²¹ì¹ ìˆ˜ë¡ ì§„í•´ì§)
                    trail_colors = [
                        (50, 150, 255),   # ì—°í•œ ì£¼í™©
                        (50, 255, 150),   # ì—°í•œ ì´ˆë¡
                        (255, 150, 50),   # ì—°í•œ íŒŒë‘
                        (150, 255, 255),  # ì—°í•œ ë…¸ë‘
                        (255, 50, 255),   # ì—°í•œ ìí™
                        (255, 255, 50),   # ì—°í•œ ì‹œì•ˆ
                        (180, 50, 180),   # ì—°í•œ ë³´ë¼
                        (255, 200, 50),   # ì—°í•œ ì§„í•œ ì£¼í™©
                    ]
                    
                    # ê°ì²´ IDì— ë”°ë¥¸ ìƒ‰ìƒ ì„ íƒ
                    color = trail_colors[obj_id % len(trail_colors)]
                    
                    # overlayì— ê²½ë¡œ ê·¸ë¦¬ê¸° (ê²¹ì¹ ìˆ˜ë¡ ì§„í•´ì§)
                    for i in range(1, len(path)):
                        pt1 = (int(path[i-1][0]), int(path[i-1][1]))
                        pt2 = (int(path[i][0]), int(path[i][1]))
                        cv2.line(overlay, pt1, pt2, color, 5)  # ë” ë‘êº¼ìš´ ì„ 
            
            # ë¶ˆíˆ¬ëª…ë„ ì ìš©í•˜ì—¬ ì›ë³¸ í”„ë ˆì„ê³¼ ë¸”ë Œë”© (ê²¹ì¹ ìˆ˜ë¡ ì§„í•´ì§€ëŠ” íš¨ê³¼)
            alpha = 0.3  # ë¶ˆíˆ¬ëª…ë„ (0.0 = ì™„ì „ íˆ¬ëª…, 1.0 = ì™„ì „ ë¶ˆíˆ¬ëª…)
            cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)

        # tracking line ê·¸ë¦¬ê¸° ê°œì„  (ë””ë²„ê¹… ì •ë³´ í¬í•¨)
        # ì£¼í–‰ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ tracking_lines ê·¸ë¦¬ê¸°
        if not driving_mode:
            if len(tracking_lines) > 0:
                print(f"[í™”ë©´ ê·¸ë¦¬ê¸°] tracking_lines ê°œìˆ˜: {len(tracking_lines)}")
            
            for line_data_draw in tracking_lines:
                line_points = line_data_draw["points"]
                line_type = line_data_draw["type"]
                line_name = line_data_draw["name"]
                
                # ì„  ìƒ‰ìƒ: ì´ˆê¸°ì„ ì€ ë…¹ìƒ‰, ì¶”ì ì„ ì€ ë¹¨ê°„ìƒ‰
                color = (0, 255, 0) if line_type == "initial" else (0, 0, 255)
                
                if len(line_points) == 2:
                    # ì™„ì„±ëœ ì„  ê·¸ë¦¬ê¸°
                    start_draw, end_draw = line_points
                    cv2.line(frame, start_draw, end_draw, color, 3)  # ì„  ë‘ê»˜ ì¦ê°€
                    actual_count = len(line_data_draw.get("persistent_crossed_ids", set()))
                    
                    # í…ìŠ¤íŠ¸ ë°°ê²½ ì¶”ê°€ë¡œ ê°€ë…ì„± í–¥ìƒ
                    label = f"{line_name} Cnt: {actual_count}"
                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
                    cv2.rectangle(frame, 
                                 (int(start_draw[0]), int(start_draw[1])-25),
                                 (int(start_draw[0]) + label_size[0], int(start_draw[1])-5),
                                 (0, 0, 0), -1)
                    cv2.putText(frame, label, (int(start_draw[0]), int(start_draw[1])-10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                               
                elif len(line_points) == 1:
                    # ë¯¸ì™„ì„± ì„  (ì²« ë²ˆì§¸ ì ë§Œ ìˆëŠ” ê²½ìš°) - ì‹œì‘ì  í‘œì‹œ
                    start_point = line_points[0]
                    cv2.circle(frame, start_point, 8, color, -1)
                    cv2.putText(frame, f"{line_name} (ë¯¸ì™„ì„±)", 
                               (int(start_point[0]) + 10, int(start_point[1]) - 10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                
                # ëª¨ë“  ì  í‘œì‹œ (ë“œë˜ê·¸ ê°€ëŠ¥í•œ ì ë“¤)
                for i, p_vis in enumerate(line_points):
                    cv2.circle(frame, p_vis, 5, (255, 0, 0), -1)  # íŒŒë€ìƒ‰ ì›
                    cv2.circle(frame, p_vis, 6, (255, 255, 255), 1)  # í°ìƒ‰ í…Œë‘ë¦¬

        # ëª¨ë“œë³„ í™”ë©´ í‘œì‹œ
        if driving_mode:
            # ì£¼í–‰ëª¨ë“œ ìƒíƒœ í‘œì‹œ
            mode_text = "Driving Mode: Sign Detection"
            cv2.putText(frame, mode_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
            
            # í˜„ì¬ ê²€ì§€ëœ í‘œì§€íŒ ìˆ˜ í‘œì‹œ
            total_detections = len(sign_detection_data) if 'sign_detection_data' in locals() else 0
            detection_text = f"Signs Detected: {total_detections}"
            cv2.putText(frame, detection_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
        elif aerial_view_mode:
            # í•­ê³µë·° ëª¨ë“œ ìƒíƒœ í‘œì‹œ (ê¸°ì¤€ì„ ì€ ì´ë¯¸ ì„¤ì • ì™„ë£Œë˜ì–´ í‘œì‹œí•˜ì§€ ì•ŠìŒ)
            mode_text = "Aerial View Mode: ON"
            if aerial_meters_per_pixel is not None:
                mode_text += f" (Scale: {aerial_meters_per_pixel:.4f} m/px)"
            else:
                mode_text += " - Scale not set"
                
            cv2.putText(frame, mode_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
            
            # í•­ê³µë·° ëª¨ë“œì—ì„œ ì¶”ì  ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
            if aerial_view_mode:
                # ì¶”ì  ì¤‘ì¸ ê°ì²´ ìˆ˜ì™€ ìŠ¤ë¬´ë”© ì„¤ì • ì •ë³´
                tracking_info = f"Tracking: {len(current_tracked_ids)} objects | Smoothing: Reduced for accuracy"
                cv2.putText(frame, tracking_info, (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                
                # ì˜ˆì¸¡ ê¸°ë°˜ ì¶”ì  í™œì„±í™” í‘œì‹œ
                cv2.putText(frame, "Predictive tracking: ENABLED", (10, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        else:
            cv2.putText(frame, "Aerial View Mode: OFF (Press 'a' to enable)", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)
        
        # ë§ˆìš°ìŠ¤ ì‚¬ìš©ë²• ë„ì›€ë§ í‘œì‹œ
        if not custom_region_mode:
            help_lines = [
                "Mouse Controls:",
                "Right Click: Add/Complete Tracking Line (Red)",
                "Wheel Click: Add/Complete Initial Line (Green)", 
                "Left Click: Drag line endpoints",
                "Press 'c': Custom region mode"
            ]
            for i, help_line in enumerate(help_lines):
                y_pos = frame.shape[0] - 120 + (i * 20)  # í™”ë©´ í•˜ë‹¨ì—ì„œ ìœ„ë¡œ
                color = (255, 255, 0) if i == 0 else (255, 255, 255)  # ì œëª©ì€ ë…¸ë€ìƒ‰
                thickness = 2 if i == 0 else 1
                cv2.putText(frame, help_line, (10, y_pos), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness)

        # 'p' í‚¤ í† ê¸€ ì •ë³´ëŠ” ìœ„ë¡œ ì´ë™

        text_x_custom = width - 400 if width > 400 else 20 # í™”ë©´ ë„ˆë¹„ì— ë”°ë¼ ì¡°ì •
        y_offset_custom = 20
        for idx_cr, region_cr in enumerate(custom_regions):
            cv2.polylines(frame, [region_cr], True, (0, 255, 255), 2)

            region_speeds_list = []
            region_decel_list = []

            ids_in_region_now = []
            for trk_id_loop in tracked_ids_in_current_frame:
                if object_positions[trk_id_loop]:
                    last_pos_data = object_positions[trk_id_loop][-1]
                    center_for_custom_region_test = (last_pos_data[0], last_pos_data[1])
                    if cv2.pointPolygonTest(region_cr, center_for_custom_region_test, False) >= 0:
                        ids_in_region_now.append(trk_id_loop)
                        if trk_id_loop in object_speeds:
                            region_speeds_list.append(object_speeds[trk_id_loop] * 3.6)
                        for rec_sr in reversed(speed_records):
                            if rec_sr["ID"] == trk_id_loop and rec_sr["Deceleration(m/sÂ²)"] is not None:
                                region_decel_list.append(rec_sr["Deceleration(m/sÂ²)"])
                                break

            avg_speed_custom = np.mean(region_speeds_list) if region_speeds_list else 0
            avg_decel_custom = np.mean(region_decel_list) if region_decel_list else 0

            if idx_cr not in region_tracking_counts: region_tracking_counts[idx_cr] = {}
            for line_data_cr in tracking_lines:
                if len(line_data_cr["points"]) == 2:
                    p1_line, p2_line = line_data_cr["points"]
                    mid_line = ((p1_line[0] + p2_line[0]) / 2, (p1_line[1] + p2_line[1]) / 2)
                    if cv2.pointPolygonTest(region_cr, mid_line, False) >= 0:
                        line_name_cr = line_data_cr["name"]
                        current_global_count = len(line_data_cr.get("persistent_crossed_ids",set()))
                        region_tracking_counts[idx_cr][line_name_cr] = current_global_count


            region_text = (
                f"ì˜ì—­ {idx_cr+1}:\n"
                f"í‰ê· ì†ë„: {avg_speed_custom:.1f}km/h\n"
                f"í‰ê· ê°ì†ë„: {avg_decel_custom:.1f}m/s2\n"
                f"ì°¨ì„ ë³€ê²½ ê²€ì§€ì„ :"
            )
            for line_name_disp, count_disp in region_tracking_counts[idx_cr].items():
                region_text += f"\n  {line_name_disp}: {count_disp}"

            frame = draw_multiline_text(frame, region_text, (text_x_custom, y_offset_custom), font_size=18, color=(255,255,255))
            lines_count_rt = region_text.split('\n')
            line_height_rt = 18 + 5
            y_offset_custom += (line_height_rt * len(lines_count_rt)) + 15


        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'): break
        elif key == ord('n'):
            custom_region_mode = True
            current_custom_region.clear()
        elif key == ord('p'):
            display_info = not display_info
        elif key == ord('t'):
            # Trail í‘œì‹œ í† ê¸€
            show_trails = not show_trails
            print(f"ê°ì²´ ì´ë™ ê²½ë¡œ í‘œì‹œ: {'ON' if show_trails else 'OFF'}")
        elif key == ord('i'):
            # ëˆ„ì  trail í‘œì‹œ í† ê¸€ (ì°¨ë¡œ ì‚¬ìš©ëŸ‰ ë¶„ì„ìš©)
            show_accumulated_trails = not show_accumulated_trails
            if show_accumulated_trails:
                print("ëˆ„ì  Trail í‘œì‹œ ON - ëª¨ë“  ê°ì²´ì˜ ì „ì²´ ì´ë™ ê²½ë¡œ í‘œì‹œ (ì°¨ë¡œ ì‚¬ìš©ëŸ‰ ë¶„ì„)")
                print(f"í˜„ì¬ê¹Œì§€ {len(accumulated_trail_paths)}ê°œ ê°ì²´ì˜ ê²½ë¡œ ë°ì´í„° ë³´ìœ ")
            else:
                print("ëˆ„ì  Trail í‘œì‹œ OFF")
        elif key == ord('a'):
            # í•­ê³µë·° ëª¨ë“œëŠ” ì‹œì‘ ì‹œ ì„¤ì •ë˜ë¯€ë¡œ ë¶„ì„ ì¤‘ í† ê¸€ ë¶ˆê°€
            if aerial_view_mode:
                print("í•­ê³µë·° ëª¨ë“œëŠ” ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©°, ë¶„ì„ ì¤‘ì—ëŠ” ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print("í•­ê³µë·° ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì²˜ìŒë¶€í„° í•­ê³µë·° ëª¨ë“œë¡œ ë¶„ì„ì„ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.")
        elif key == ord('r'):
            # í•­ê³µë·° ìŠ¤ì¼€ì¼ì€ ì‹œì‘ ì‹œ ì„¤ì •ë˜ë¯€ë¡œ ì¬ì„¤ì • ë¶ˆê°€
            if aerial_view_mode:
                print("í•­ê³µë·° ìŠ¤ì¼€ì¼ì€ ë¶„ì„ ì‹œì‘ ì‹œ ì„¤ì •ë˜ì—ˆìœ¼ë©°, ë¶„ì„ ì¤‘ì—ëŠ” ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print("í˜¸ëª¨ê·¸ë˜í”¼ ëª¨ë“œì—ì„œëŠ” ìŠ¤ì¼€ì¼ ì¬ì„¤ì • ê¸°ëŠ¥ì´ ì—†ìŠµë‹ˆë‹¤.")

        # ë¹„ë””ì˜¤ í”„ë ˆì„ ì“°ê¸° ì‹œë„
        if out and out.isOpened():
            success = out.write(frame)
            if not success and current_frame % 100 == 0:  # 100í”„ë ˆì„ë§ˆë‹¤ í•œë²ˆì”© ì²´í¬
                print(f"ê²½ê³ : í”„ë ˆì„ {current_frame} ì“°ê¸° ì‹¤íŒ¨")
        else:
            if current_frame % 100 == 0:  # 100í”„ë ˆì„ë§ˆë‹¤ í•œë²ˆì”© ê²½ê³ 
                print(f"ê²½ê³ : VideoWriterê°€ ì—´ë ¤ìˆì§€ ì•ŠìŒ (í”„ë ˆì„ {current_frame})")
        
        cv2.imshow('YOLO Detection', frame)
        current_frame += 1
        
        # ì§„í–‰ë¥  í‘œì‹œ (5ì´ˆë§ˆë‹¤)
        current_time_now = time.time()
        if current_time_now - last_progress_time > 5:
            progress_pct = (current_frame / total_frames) * 100 if total_frames > 0 else 0
            print(f"ì§„í–‰ë¥ : {progress_pct:.1f}% ({current_frame}/{total_frames} í”„ë ˆì„)")
            last_progress_time = current_time_now

    cap.release()
    
    # ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ í™•ì¸
    if out:
        out.release()
        # íŒŒì¼ì´ ì‹¤ì œë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if os.path.exists(output_video_path):
            file_size = os.path.getsize(output_video_path)
            print(f"âœ… ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {output_video_path}")
            print(f"   íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({file_size/(1024*1024):.1f} MB)")
        else:
            print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {output_video_path}")
    else:
        print("âŒ VideoWriterê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë¹„ë””ì˜¤ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ì˜ìƒ ë¶„ì„ ì™„ë£Œ í›„ ëˆ„ì  trail ìŠ¤í¬ë¦°ìƒ· ì €ì¥
    if accumulated_trail_paths and len(accumulated_trail_paths) > 0:
        print("ğŸ“¸ ëˆ„ì  trail ìŠ¤í¬ë¦°ìƒ·ì„ ìƒì„±í•©ë‹ˆë‹¤...")
        
        # ë§ˆì§€ë§‰ í”„ë ˆì„ì„ ë‹¤ì‹œ ì—´ì–´ì„œ trail stack ì´ë¯¸ì§€ ìƒì„±
        cap_screenshot = cv2.VideoCapture(input_video_path)
        if cap_screenshot.isOpened():
            # ë§ˆì§€ë§‰ í”„ë ˆì„ìœ¼ë¡œ ì´ë™
            cap_screenshot.set(cv2.CAP_PROP_POS_FRAMES, max(0, total_frames - 1))
            ret_ss, frame_ss = cap_screenshot.read()
            
            if ret_ss:
                # ëˆ„ì  trail overlay ìƒì„±
                overlay_ss = frame_ss.copy()
                
                for obj_id, path in accumulated_trail_paths.items():
                    if len(path) > 1:
                        # ë¶ˆíˆ¬ëª… ìƒ‰ìƒë“¤
                        trail_colors = [
                            (50, 150, 255),   # ì—°í•œ ì£¼í™©
                            (50, 255, 150),   # ì—°í•œ ì´ˆë¡
                            (255, 150, 50),   # ì—°í•œ íŒŒë‘
                            (150, 255, 255),  # ì—°í•œ ë…¸ë‘
                            (255, 50, 255),   # ì—°í•œ ìí™
                            (255, 255, 50),   # ì—°í•œ ì‹œì•ˆ
                            (180, 50, 180),   # ì—°í•œ ë³´ë¼
                            (255, 200, 50),   # ì—°í•œ ì§„í•œ ì£¼í™©
                        ]
                        
                        color = trail_colors[obj_id % len(trail_colors)]
                        
                        # overlayì— ì „ì²´ ê²½ë¡œ ê·¸ë¦¬ê¸°
                        for i in range(1, len(path)):
                            pt1 = (int(path[i-1][0]), int(path[i-1][1]))
                            pt2 = (int(path[i][0]), int(path[i][1]))
                            cv2.line(overlay_ss, pt1, pt2, color, 5)
                
                # ë¶ˆíˆ¬ëª…ë„ ì ìš©í•˜ì—¬ ë¸”ë Œë”©
                alpha = 0.4  # ìŠ¤í¬ë¦°ìƒ·ìš©ìœ¼ë¡œ ì¡°ê¸ˆ ë” ì§„í•˜ê²Œ
                cv2.addWeighted(overlay_ss, alpha, frame_ss, 1 - alpha, 0, frame_ss)
                
                # ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ê²½ë¡œ
                screenshot_filename = f"{video_name_without_ext}_trail_analysis.png"
                screenshot_path = os.path.join(output_folder_path, screenshot_filename)
                
                # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
                if cv2.imwrite(screenshot_path, frame_ss):
                    print(f"âœ… ëˆ„ì  trail ë¶„ì„ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {screenshot_path}")
                    print(f"   ë¶„ì„ëœ ê°ì²´ ìˆ˜: {len(accumulated_trail_paths)}ê°œ")
                else:
                    print(f"âŒ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì‹¤íŒ¨: {screenshot_path}")
            
            cap_screenshot.release()
        else:
            print("âŒ ìŠ¤í¬ë¦°ìƒ· ìƒì„±ì„ ìœ„í•œ ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨")
    else:
        print("ğŸ“¸ ëˆ„ì  trail ë°ì´í„°ê°€ ì—†ì–´ ìŠ¤í¬ë¦°ìƒ·ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    cv2.destroyAllWindows()

    # --- ì£¼í–‰ëª¨ë“œ ë°ì´í„° ì €ì¥ ---
    if driving_mode and sign_detection_data:
        print("ğŸ’¾ í‘œì§€íŒ ê²€ì§€ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥í•˜ëŠ” ì¤‘...")
        
        # í‘œì§€íŒ ê²€ì§€ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df_signs = pd.DataFrame(sign_detection_data)
        
        # ì‹œê°„ í¬ë§· ì¶”ê°€
        df_signs['timestamp_formatted'] = df_signs['timestamp'].dt.strftime('%H:%M:%S.%f').str[:-3]
        df_signs['video_time_sec'] = df_signs['frame_number'] / fps if fps > 0 else 0
        df_signs['video_time_formatted'] = df_signs['video_time_sec'].apply(
            lambda x: f"{int(x//3600):02d}:{int((x%3600)//60):02d}:{int(x%60):02d}.{int((x%1)*1000):03d}"
        )
        
        # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
        sign_columns = [
            'timestamp_formatted', 'video_time_formatted', 'video_time_sec',
            'frame_number', 'class_name', 'confidence',
            'bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2', 'screenshot_path'
        ]
        df_signs = df_signs[sign_columns]
        
        # CSV íŒŒì¼ ì €ì¥
        signs_csv_path = os.path.join(output_folder_path, f"{current_datetime}_{clean_model_name}_{clean_video_name}_sign_detections.csv")
        df_signs.to_csv(signs_csv_path, index=False, encoding='utf-8-sig')
        
        print(f"âœ… í‘œì§€íŒ ê²€ì§€ CSV ì €ì¥ ì™„ë£Œ: {signs_csv_path}")
        print(f"   ì´ ê²€ì§€ëœ í‘œì§€íŒ: {len(df_signs)}ê°œ")
        print(f"   ê²€ì§€ëœ í‘œì§€íŒ ì¢…ë¥˜: {', '.join(df_signs['class_name'].unique())}")
        
        # ê°„ë‹¨í•œ í†µê³„ ì •ë³´ ìƒì„±
        sign_summary = df_signs['class_name'].value_counts().to_dict()
        summary_text = []
        summary_text.append("=== í‘œì§€íŒ ê²€ì§€ ìš”ì•½ ===")
        summary_text.append(f"ì´ ê²€ì§€ ìˆ˜: {len(df_signs)}ê°œ")
        summary_text.append("í‘œì§€íŒë³„ ê²€ì§€ ìˆ˜:")
        for sign_type, count in sign_summary.items():
            summary_text.append(f"  - {sign_type}: {count}ê°œ")
        summary_text.append(f"ë¶„ì„ ì˜ìƒ: {video_name}")
        summary_text.append(f"ì‚¬ìš© ëª¨ë¸: {model_name}")
        
        # ìš”ì•½ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        summary_path = os.path.join(output_folder_path, f"{current_datetime}_{clean_model_name}_{clean_video_name}_sign_summary.txt")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(summary_text))
        
        print(f"âœ… í‘œì§€íŒ ê²€ì§€ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_path}")
        
        # ê°€ë ¤ì§ ì´ë²¤íŠ¸ CSV ì €ì¥
        if occlusion_events:
            df_occlusion = pd.DataFrame(occlusion_events)
            
            # ì‹œê°„ í¬ë§· ì¶”ê°€
            df_occlusion['timestamp_formatted'] = df_occlusion['timestamp'].dt.strftime('%H:%M:%S.%f').str[:-3]
            df_occlusion['video_time_sec'] = df_occlusion['frame_number'] / fps if fps > 0 else 0
            df_occlusion['video_time_formatted'] = df_occlusion['video_time_sec'].apply(
                lambda x: f"{int(x//3600):02d}:{int((x%3600)//60):02d}:{int(x%60):02d}.{int((x%1)*1000):03d}"
            )
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ë¶„ë¦¬
            df_occlusion[['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']] = pd.DataFrame(
                df_occlusion['bbox'].tolist(), index=df_occlusion.index)
            df_occlusion = df_occlusion.drop('bbox', axis=1)
            
            # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
            occlusion_columns = [
                'timestamp_formatted', 'video_time_formatted', 'video_time_sec',
                'frame_number', 'track_id', 'class_name', 'occ_ratio', 'severity', 'method',
                'is_edge_cut', 'bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2'
            ]
            df_occlusion = df_occlusion[occlusion_columns]
            
            # ê°€ë ¤ì§ CSV ì €ì¥
            occlusion_csv_path = os.path.join(output_folder_path, 
                f"{current_datetime}_{clean_model_name}_{clean_video_name}_occlusion_events.csv")
            df_occlusion.to_csv(occlusion_csv_path, index=False, encoding='utf-8-sig')
            
            print(f"âœ… ê°€ë ¤ì§ ì´ë²¤íŠ¸ CSV ì €ì¥ ì™„ë£Œ: {occlusion_csv_path}")
            print(f"   ì´ ê°€ë ¤ì§ ì´ë²¤íŠ¸: {len(df_occlusion)}ê°œ")
        
        # ê²°ê³¼ í´ë” ìë™ìœ¼ë¡œ ì—´ê¸°
        try:
            if os.name == 'nt':  # Windows
                os.startfile(output_folder_path)
            elif os.name == 'posix':  # macOS/Linux
                subprocess.run(['open', output_folder_path], check=True)
            print(f"ğŸ“‚ ê²°ê³¼ í´ë”ë¥¼ ì—´ì—ˆìŠµë‹ˆë‹¤: {output_folder_path}")
        except Exception as e:
            print(f"âš ï¸ í´ë” ì—´ê¸° ì‹¤íŒ¨: {e}")
        
        # ì£¼í–‰ëª¨ë“œì—ì„œëŠ” ì—¬ê¸°ì„œ ì¢…ë£Œ
        messagebox.showinfo("ë¶„ì„ ì™„ë£Œ", 
                           f"í‘œì§€íŒ ê²€ì§€ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n\n"
                           f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_folder_path}\n"
                           f"- í‘œì§€íŒ ê²€ì§€ CSV: {len(df_signs)}ê°œ ê¸°ë¡\n"
                           f"- í‘œì§€íŒ ìŠ¤í¬ë¦°ìƒ·: {len([f for f in os.listdir(output_folder_path) if f.startswith('sign_')])}ê°œ\n"
                           f"- ê°€ë ¤ì§ ì´ë²¤íŠ¸ CSV: {len(occlusion_events)}ê°œ ê¸°ë¡\n"
                           f"- ë¶„ì„ ì˜ìƒ\n"
                           f"- ê²€ì§€ ìš”ì•½ íŒŒì¼\n\n"
                           f"ğŸ“‚ ê²°ê³¼ í´ë”ê°€ ìë™ìœ¼ë¡œ ì—´ë ¸ìŠµë‹ˆë‹¤!")
        return
    elif driving_mode:
        # í‘œì§€íŒì´ ê²€ì§€ë˜ì§€ ì•Šì•˜ì–´ë„ ë¹ˆ CSVì™€ ìš”ì•½ íŒŒì¼ ìƒì„±
        print("âš ï¸ ì£¼í–‰ëª¨ë“œì—ì„œ í‘œì§€íŒì´ ê²€ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¹ˆ ê²°ê³¼ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        
        # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        empty_sign_columns = [
            'timestamp_formatted', 'video_time_formatted', 'video_time_sec',
            'frame_number', 'class_name', 'confidence',
            'bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2', 'screenshot_path'
        ]
        df_signs_empty = pd.DataFrame(columns=empty_sign_columns)
        
        # ë¹ˆ CSV íŒŒì¼ ì €ì¥
        signs_csv_path = os.path.join(output_folder_path, f"{current_datetime}_{clean_model_name}_{clean_video_name}_sign_detections.csv")
        df_signs_empty.to_csv(signs_csv_path, index=False, encoding='utf-8-sig')
        
        # ë¹ˆ ìš”ì•½ ì •ë³´ ìƒì„±
        summary_text = []
        summary_text.append("=== í‘œì§€íŒ ê²€ì§€ ìš”ì•½ ===")
        summary_text.append("ì´ ê²€ì§€ ìˆ˜: 0ê°œ")
        summary_text.append("ê²€ì§€ëœ í‘œì§€íŒ: ì—†ìŒ")
        summary_text.append(f"ë¶„ì„ ì˜ìƒ: {video_name}")
        summary_text.append(f"ì‚¬ìš© ëª¨ë¸: {model_name}")
        summary_text.append("\nâ€» í‘œì§€íŒì´ ê²€ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        summary_text.append("- ëª¨ë¸ì´ í‘œì§€íŒ ê²€ì§€ìš©ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        summary_text.append("- ì˜ìƒì— í‘œì§€íŒì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        summary_text.append("- ê²€ì§€ ì„ê³„ê°’(confidence threshold)ì„ ë‚®ì¶°ë³´ì„¸ìš”.")
        
        # ìš”ì•½ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        summary_path = os.path.join(output_folder_path, f"{current_datetime}_{clean_model_name}_{clean_video_name}_sign_summary.txt")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(summary_text))
        
        print(f"âœ… ë¹ˆ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ")
        print(f"   CSV íŒŒì¼: {signs_csv_path}")
        print(f"   ìš”ì•½ íŒŒì¼: {summary_path}")
        
        # í‘œì§€íŒì´ ê²€ì§€ë˜ì§€ ì•Šì•„ë„ ê²°ê³¼ í´ë” ì—´ê¸°
        try:
            if os.name == 'nt':  # Windows
                os.startfile(output_folder_path)
            elif os.name == 'posix':  # macOS/Linux
                subprocess.run(['open', output_folder_path], check=True)
            print(f"ğŸ“‚ ê²°ê³¼ í´ë”ë¥¼ ì—´ì—ˆìŠµë‹ˆë‹¤: {output_folder_path}")
        except Exception as e:
            print(f"âš ï¸ í´ë” ì—´ê¸° ì‹¤íŒ¨: {e}")
        
        messagebox.showinfo("ë¶„ì„ ì™„ë£Œ", 
                           f"ì£¼í–‰ëª¨ë“œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                           f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_folder_path}\n"
                           f"- í‘œì§€íŒ ê²€ì§€ CSV: 0ê°œ ê¸°ë¡ (ë¹ˆ íŒŒì¼)\n"
                           f"- ë¶„ì„ ì˜ìƒ\n"
                           f"- ê²€ì§€ ìš”ì•½ íŒŒì¼\n\n"
                           f"â€» í‘œì§€íŒì´ ê²€ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                           f"ëª¨ë¸ì´ í‘œì§€íŒ ê²€ì§€ìš©ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\n\n"
                           f"ğŸ“‚ ê²°ê³¼ í´ë”ê°€ ìë™ìœ¼ë¡œ ì—´ë ¸ìŠµë‹ˆë‹¤!")
        return

    # --- ë°ì´í„°í”„ë ˆì„ ìƒì„± ---
    df_crossing_data = pd.DataFrame(crossing_data)
    df_speed = pd.DataFrame(speed_records)
    df_all_vehicles = pd.DataFrame(presence_data)
    df_distance_deceleration = pd.DataFrame(distance_deceleration_records)

    if df_crossing_data.empty:
        df_crossing_data = pd.DataFrame(columns=['ID','Class','Time','Line','IsInitialLine'])

    avg_speed_df_cols = ['ID','Class','IsInitialLine','IsSteppedTrackingLine','Avg Speed(km/h)']
    if not df_speed.empty:
        avg_speed_df_temp = df_speed.groupby(['ID','Class','IsInitialLine','IsSteppedTrackingLine'], as_index=False).mean()
        if not avg_speed_df_temp.empty and 'Speed(km/h)' in avg_speed_df_temp.columns :
            avg_speed_df = avg_speed_df_temp[['ID','Class','IsInitialLine','IsSteppedTrackingLine', 'Speed(km/h)']].copy()
            avg_speed_df.rename(columns={'Speed(km/h)': 'Avg Speed(km/h)'}, inplace=True)
        else:
            avg_speed_df = pd.DataFrame(columns=avg_speed_df_cols)
    else:
        avg_speed_df = pd.DataFrame(columns=avg_speed_df_cols)

    df_presence = pd.DataFrame(presence_data) # df_all_vehiclesì™€ ë™ì¼í•  ìˆ˜ ìˆìŒ, í•„ìš”ì‹œ ì •ë¦¬

    if not df_crossing_data.empty:
        crossed_initial_ids = df_crossing_data[df_crossing_data['IsInitialLine'] == True]['ID'].unique()
        df_crossing_data.loc[df_crossing_data['ID'].isin(crossed_initial_ids), 'IsInitialLine'] = True
    else:
        crossed_initial_ids = np.array([])

    for df_to_update in [df_speed, df_all_vehicles, df_presence]: # df_presence ì¶”ê°€
        if not df_to_update.empty and 'ID' in df_to_update.columns and 'IsInitialLine' in df_to_update.columns:
            df_to_update.loc[df_to_update['ID'].isin(crossed_initial_ids), 'IsInitialLine'] = True
    
    # --- ìƒˆë¡œìš´ ìƒì„¸ íƒ€ì„ë¼ì¸ ë¡œê·¸ ìƒì„± ---
    # 1. ë¨¼ì € IDë³„ë¡œ ìµœì¢…ì ìœ¼ë¡œ ë°Ÿì€ tracking lineì„ ìˆ˜ì§‘
    vehicle_crossed_lines = defaultdict(set)
    if not df_crossing_data.empty:
        # ëª¨ë“  tracking line êµì°¨ ì´ë²¤íŠ¸ì—ì„œ IDë³„ë¡œ í†µê³¼í•œ ë¼ì¸ ìˆ˜ì§‘
        for _, row in df_crossing_data.iterrows():
            vehicle_id = row['ID']
            line_name = row['Line']
            if 'Tracking Line' in line_name:  # 'Tracking Line'ì´ í¬í•¨ëœ ì´ë¦„ë§Œ í•„í„°ë§
                vehicle_crossed_lines[vehicle_id].add(line_name)
    
    # 2. ë°ì´í„° ìƒì„± - ê° IDì— ëŒ€í•´ ëª¨ë“  ì‹œì ì˜ ë¡œê·¸ë¥¼ ìƒì„±í•˜ë˜, ìµœì¢…ì ìœ¼ë¡œ tracking lineì„ ë°Ÿì€ ì°¨ëŸ‰ë§Œ í¬í•¨
    timeline_logs = []
    
    # 3. êµì°¨ ì´ë²¤íŠ¸ ë°ì´í„°ë¥¼ ì‹œê°„ë³„ë¡œ ì •ë¦¬ (IDë³„ ì •í™•í•œ êµì°¨ ì‹œì  ìº¡ì²˜ë¥¼ ìœ„í•´)
    crossing_events_by_id_time = defaultdict(list)
    if not df_crossing_data.empty:
        for _, event in df_crossing_data.iterrows():
            vehicle_id = event['ID']
            if 'Tracking Line' in event['Line']:  # Tracking Line ì´ë²¤íŠ¸ë§Œ ì €ì¥
                crossing_events_by_id_time[(vehicle_id, event['Time'])].append(event['Line'])
    
    # ì´ë¯¸ IDë³„ë¡œ ë°Ÿì€ ë¼ì¸ì„ ìˆ˜ì§‘í–ˆìœ¼ë¯€ë¡œ, ì´ì œ í•´ë‹¹ IDì˜ ëª¨ë“  ê¸°ë¡ ìˆ˜ì§‘
    if not df_speed.empty and vehicle_crossed_lines:
        # tracking lineì„ ë°Ÿì€ ëª¨ë“  ì°¨ëŸ‰ ID ëª©ë¡
        crossed_vehicles = set(vehicle_crossed_lines.keys())
        
        # speed_records ë°ì´í„°ì—ì„œ í•„ìš”í•œ ì°¨ëŸ‰ë§Œ í•„í„°ë§
        for _, record in df_speed.iterrows():
            vehicle_id = record['ID']
            
            # ìµœì¢…ì ìœ¼ë¡œ ì–´ë–¤ tracking lineì´ë¼ë„ ë°Ÿì€ ì°¨ëŸ‰ë§Œ ì„ íƒ
            if vehicle_id in crossed_vehicles:
                time_point = record['Time']
                
                # ì´ ì‹œì ì— ì°¨ëŸ‰ì´ ë°Ÿê³  ìˆëŠ” ë¼ì¸ ì°¾ê¸°
                current_line = "None"
                
                # ì •í™•í•œ ì‹œê°„ì— êµì°¨ ì´ë²¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì •í™•í•œ êµì°¨ ì‹œì  ìº¡ì²˜)
                if (vehicle_id, time_point) in crossing_events_by_id_time:
                    # ë™ì¼í•œ ì‹œê°„ì— ì—¬ëŸ¬ ë¼ì¸ì„ ë°Ÿì•˜ì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ëª¨ë‘ í¬í•¨
                    current_line = "; ".join(crossing_events_by_id_time[(vehicle_id, time_point)])
                
                # ìµœì¢…ì ìœ¼ë¡œ ì´ ì°¨ëŸ‰ì´ ë°Ÿì€ ëª¨ë“  tracking line ëª©ë¡ (ì •ë ¬ëœ ë¬¸ìì—´)
                final_lines_crossed = "; ".join(sorted(vehicle_crossed_lines[vehicle_id]))
                
                timeline_logs.append({
                    'ID': vehicle_id,
                    'Time(s)': time_point,
                    'Class': record['Class'],
                    'Current_Line_Status': current_line,
                    'Speed(km/h)': record.get('Speed(km/h)', None),
                    'Acceleration(m/sÂ²)': record.get('Acceleration(m/sÂ²)', None),
                    'Deceleration(m/sÂ²)': record.get('Deceleration(m/sÂ²)', None),
                    'Final_Crossed_Lines': final_lines_crossed
                })
    
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    df_timeline = pd.DataFrame(timeline_logs)

    df_region = pd.DataFrame(region_records)
    if df_region.empty:
        df_region = pd.DataFrame(columns=['Region', 'ID', 'Class', 'SteppedTrackingLine', 'Speed(km/h)', 'Acceleration(m/sÂ²)', 'Deceleration(m/sÂ²)', 'LinesStepped'])

    df_region_line_stats_cols = ['Region', 'Class', 'SteppedTrackingLine_True', 'SteppedTrackingLine_False', 'TotalInRegion']
    df_region_line_stats = pd.DataFrame(columns=df_region_line_stats_cols)
    if not df_region.empty and all(col in df_region.columns for col in ['Region', 'ID', 'Class', 'SteppedTrackingLine']):
        df_region_unique = (
            df_region
            .groupby(['Region', 'ID', 'Class'], as_index=False)
            .agg(SteppedTrackingLine=('SteppedTrackingLine', 'max'))
        )
        summary_rows = []
        if not df_region_unique.empty:
            for (reg, cls), grp in df_region_unique.groupby(['Region', 'Class']):
                total = len(grp)
                stepped = grp['SteppedTrackingLine'].sum()
                not_stepped = total - stepped
                summary_rows.append({
                    'Region': reg, 'Class': cls,
                    'SteppedTrackingLine_True': int(stepped),
                    'SteppedTrackingLine_False': int(not_stepped),
                    'TotalInRegion': int(total)
                })
        if summary_rows:
             df_region_line_stats = pd.DataFrame(summary_rows, columns=df_region_line_stats_cols)


    df_region_summary = pd.DataFrame(
        [{"Region": idx + 1, "TotalUniqueVehiclesInRegion": len(ids)}
            for idx, ids in region_unique_ids.items()]
    )
    if df_region_summary.empty:
        df_region_summary = pd.DataFrame(columns=["Region", "TotalUniqueVehiclesInRegion"])

    hourly_data_cols = ['Hour', 'Cat1_CrossInit_NoStep', 'Cat2_CrossInit_Step', 'Cat3_NoCrossInit_NoStep', 'Cat4_NoCrossInit_Step']
    hourly_data = pd.DataFrame(columns=hourly_data_cols)
    category_dfs_names = ['category1_ids', 'category2_ids', 'category3_ids', 'category4_ids']
    # ê° ì¹´í…Œê³ ë¦¬ë³„ ID ëª©ë¡ì„ ì €ì¥í•  DataFrame ì´ˆê¸°í™” (CSVë¡œ ì €ì¥ë  ê²ƒì„)
    category_id_dfs = {name: pd.DataFrame(columns=['Hour', 'Vehicle_ID', 'Class']) for name in category_dfs_names}

    # interval_df ì´ˆê¸°í™” (ì¡°ê±´ë¬¸ ë°–ì—ì„œ)
    interval_df_cols = ["Interval_Start", "Interval_End", "ID", "Class", "IsInitialLine", "IsSteppedTrackingLine"]
    interval_df = pd.DataFrame(columns=interval_df_cols)

    if not df_all_vehicles.empty and 'Time' in df_all_vehicles.columns and 'ID' in df_all_vehicles.columns:
        df_all_vehicles['Hour'] = (df_all_vehicles['Time'] // 3600).astype(int)
        grouped_hourly_id_status = df_all_vehicles.groupby(['Hour','ID'], as_index=False).agg(
            IsInitialLine=('IsInitialLine', 'max'),
            IsSteppedTrackingLine=('IsSteppedTrackingLine', 'max'),
            Class=('Class', 'first')
        )

        if not grouped_hourly_id_status.empty:
            category_id_dfs['category1_ids'] = grouped_hourly_id_status[
                (grouped_hourly_id_status['IsInitialLine'] == True) &
                (grouped_hourly_id_status['IsSteppedTrackingLine'] == False)
            ][['Hour','ID', 'Class']].drop_duplicates().rename(columns={'ID': 'Vehicle_ID'})

            category_id_dfs['category2_ids'] = grouped_hourly_id_status[
                (grouped_hourly_id_status['IsInitialLine'] == True) &
                (grouped_hourly_id_status['IsSteppedTrackingLine'] == True)
            ][['Hour','ID', 'Class']].drop_duplicates().rename(columns={'ID': 'Vehicle_ID'})

            category_id_dfs['category3_ids'] = grouped_hourly_id_status[
                (grouped_hourly_id_status['IsInitialLine'] == False) &
                (grouped_hourly_id_status['IsSteppedTrackingLine'] == False)
            ][['Hour','ID', 'Class']].drop_duplicates().rename(columns={'ID': 'Vehicle_ID'})

            category_id_dfs['category4_ids'] = grouped_hourly_id_status[
                (grouped_hourly_id_status['IsInitialLine'] == False) &
                (grouped_hourly_id_status['IsSteppedTrackingLine'] == True)
            ][['Hour','ID', 'Class']].drop_duplicates().rename(columns={'ID': 'Vehicle_ID'})

            hourly_summary_counts_dict = {
                'Cat1_CrossInit_NoStep': grouped_hourly_id_status[(grouped_hourly_id_status['IsInitialLine'] == True) & (grouped_hourly_id_status['IsSteppedTrackingLine'] == False)].groupby('Hour')['ID'].nunique(),
                'Cat2_CrossInit_Step': grouped_hourly_id_status[(grouped_hourly_id_status['IsInitialLine'] == True) & (grouped_hourly_id_status['IsSteppedTrackingLine'] == True)].groupby('Hour')['ID'].nunique(),
                'Cat3_NoCrossInit_NoStep': grouped_hourly_id_status[(grouped_hourly_id_status['IsInitialLine'] == False) & (grouped_hourly_id_status['IsSteppedTrackingLine'] == False)].groupby('Hour')['ID'].nunique(),
                'Cat4_NoCrossInit_Step': grouped_hourly_id_status[(grouped_hourly_id_status['IsInitialLine'] == False) & (grouped_hourly_id_status['IsSteppedTrackingLine'] == True)].groupby('Hour')['ID'].nunique()
            }
            hourly_data_temp = pd.DataFrame(hourly_summary_counts_dict).reset_index()
            hourly_data_temp.fillna(0, inplace=True)
            for col in hourly_data_cols:
                if col != 'Hour' and col in hourly_data_temp.columns : hourly_data_temp[col] = hourly_data_temp[col].astype(int)
            hourly_data = hourly_data_temp.reindex(columns=hourly_data_cols).fillna(0)

        # df_presenceëŠ” df_all_vehiclesì™€ ë‚´ìš©ì´ ì¤‘ë³µë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ df_all_vehiclesë¥¼ ì‚¬ìš©
        if not df_all_vehicles.empty and 'Time' in df_all_vehicles.columns:
            max_time_val = df_all_vehicles["Time"].max()
            intervals = np.arange(0, max_time_val + 5, 5)
            interval_records = []
            if len(intervals) > 1:
                for start_t in intervals[:-1]:
                    end_t = start_t + 5
                    # df_presence ëŒ€ì‹  df_all_vehicles ì‚¬ìš©
                    subset = df_all_vehicles[(df_all_vehicles["Time"] >= start_t) & (df_all_vehicles["Time"] < end_t)]
                    if not subset.empty and all(c in subset.columns for c in ['ID', 'Class', 'IsInitialLine', 'IsSteppedTrackingLine']):
                        grouped_subset_interval = subset.groupby(['ID','Class'], as_index=False).agg(
                            IsInitialLine=('IsInitialLine', 'max'),
                            IsSteppedTrackingLine=('IsSteppedTrackingLine', 'max')
                        )
                        for _, row in grouped_subset_interval.iterrows():
                            interval_records.append({
                                "Interval_Start": start_t, "Interval_End": end_t,
                                "ID": row["ID"], "Class": row["Class"],
                                "IsInitialLine": row["IsInitialLine"],
                                "IsSteppedTrackingLine": row["IsSteppedTrackingLine"]
                            })
            if interval_records:
                 interval_df = pd.DataFrame(interval_records, columns=interval_df_cols)
            else:
                 interval_df = pd.DataFrame(columns=interval_df_cols)


    total_duration_hours = (total_frames / fps / 3600) if fps > 0 else 0

    total_vehicles_all_unique = df_all_vehicles['ID'].nunique() if not df_all_vehicles.empty and 'ID' in df_all_vehicles.columns else 0
    total_vehicles_cross_O_unique = df_all_vehicles[(df_all_vehicles['IsInitialLine'] == True) & ('ID' in df_all_vehicles.columns)]['ID'].nunique() if not df_all_vehicles.empty else 0
    total_vehicles_not_cross_O_unique = total_vehicles_all_unique - total_vehicles_cross_O_unique

    traffic_flow_all_ph = (total_vehicles_all_unique / total_duration_hours) if total_duration_hours > 0 else 0
    traffic_flow_cross_O_ph = (total_vehicles_cross_O_unique / total_duration_hours) if total_duration_hours > 0 else 0
    traffic_flow_not_cross_O_ph = (total_vehicles_not_cross_O_unique / total_duration_hours) if total_duration_hours > 0 else 0

    traffic_summary_overall = pd.DataFrame({
        'Metric': ['Total Unique Vehicles (Initial Line Crossed)', 'Total Unique Vehicles (Initial Line Not Crossed)',
                   'Total Unique Vehicles (All)', 'Traffic Flow (Initial Line Crossed) [vehicles/hour]',
                   'Traffic Flow (Initial Line Not Crossed) [vehicles/hour]', 'Traffic Flow (All Vehicles) [vehicles/hour]'],
        'Value': [total_vehicles_cross_O_unique, total_vehicles_not_cross_O_unique, total_vehicles_all_unique,
                  traffic_flow_cross_O_ph, traffic_flow_not_cross_O_ph, traffic_flow_all_ph]
    })

    traffic_summary_per_class_cols = ['Class', 'Vehicles_Crossed_Initial_Line', 'Vehicles_Not_Crossed_Initial_Line', 'Total_Vehicles',
                                      'Traffic_Flow_All_Vehicles_[vehicles/hour]', 'Traffic_Flow_Crossed_Initial_Line_[vehicles/hour]',
                                      'Traffic_Flow_Not_Crossed_Initial_Line_[vehicles/hour]']
    traffic_summary_per_class = pd.DataFrame(columns=traffic_summary_per_class_cols)
    if not df_all_vehicles.empty and total_duration_hours > 0 and all(c in df_all_vehicles.columns for c in ['Class', 'ID', 'IsInitialLine']):
        class_summary_status = df_all_vehicles.groupby(['Class', 'ID'], as_index=False).agg(
            IsInitialLine=('IsInitialLine', 'max')
        ).groupby('Class', as_index=False).agg(
            Total_Vehicles=('ID', 'nunique'),
            Vehicles_Crossed_Initial_Line=('IsInitialLine', lambda x: x.astype(bool).sum())
        )
        if not class_summary_status.empty:
            class_summary_status['Vehicles_Not_Crossed_Initial_Line'] = class_summary_status['Total_Vehicles'] - class_summary_status['Vehicles_Crossed_Initial_Line']
            class_summary_status['Traffic_Flow_All_Vehicles_[vehicles/hour]'] = class_summary_status['Total_Vehicles'] / total_duration_hours
            class_summary_status['Traffic_Flow_Crossed_Initial_Line_[vehicles/hour]'] = class_summary_status['Vehicles_Crossed_Initial_Line'] / total_duration_hours
            class_summary_status['Traffic_Flow_Not_Crossed_Initial_Line_[vehicles/hour]'] = class_summary_status['Vehicles_Not_Crossed_Initial_Line'] / total_duration_hours
            traffic_summary_per_class = class_summary_status.reindex(columns=traffic_summary_per_class_cols, fill_value=0)


    df_cross_for_profile = df_crossing_data.copy()
    profile_cols = ['ID', 'Class', 'First_Cross', 'Last_Cross', 'Lines_List', 'Lines_Count', 'Avg_Speed', 'Peak_Speed']
    profile = pd.DataFrame(columns=profile_cols)
    if not df_cross_for_profile.empty and all(c in df_cross_for_profile.columns for c in ['ID', 'Class', 'Time', 'Line']):
        profile_temp = (
            df_cross_for_profile.groupby('ID', as_index=False).agg(
                Class=('Class', 'first'), First_Cross=('Time', 'min'), Last_Cross=('Time', 'max'),
                Lines_List=('Line', lambda x: ';'.join(sorted(x.unique()))), Lines_Count=('Line', 'nunique')))
        if not df_speed.empty and 'ID' in df_speed.columns and 'Speed(km/h)' in df_speed.columns:
            spd_stat = (df_speed.groupby('ID', as_index=False)['Speed(km/h)'].agg(Avg_Speed='mean', Peak_Speed='max'))
            profile_temp = profile_temp.merge(spd_stat, on='ID', how='left')
        else:
            profile_temp['Avg_Speed'] = np.nan
            profile_temp['Peak_Speed'] = np.nan
        profile = profile_temp.reindex(columns=profile_cols)


    pivot_cols = ['Lines_Count'] # ì‹¤ì œ í”¼ë²— ê²°ê³¼ì— ë”°ë¼ ì»¬ëŸ¼ì´ ë™ì ìœ¼ë¡œ ì¶”ê°€ë¨
    pivot = pd.DataFrame(columns=pivot_cols) # ì´ˆê¸°í™”
    if not profile.empty and all(c in profile.columns for c in ['Lines_Count', 'Class', 'ID']):
        try:
            pivot_temp = (profile.pivot_table(index='Lines_Count', columns='Class', values='ID', aggfunc='nunique', fill_value=0))
            pivot = pivot_temp.reset_index() # 'Lines_Count'ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë§Œë“¦
            # í”¼ë²— í…Œì´ë¸”ì˜ ì»¬ëŸ¼ì€ ['Lines_Count', 'car', 'truck', ...] ë“±ì´ ë¨
        except Exception as e:
            print(f"í”¼ë²— í…Œì´ë¸” ìƒì„± ì˜¤ë¥˜: {e}")


    bucket_cols = ['Bucket', 'Vehicle_Count', 'Avg_Speed']
    bucket = pd.DataFrame(columns=bucket_cols)
    if not profile.empty and all(c in profile.columns for c in ['Lines_Count', 'ID']):
        profile_b = profile.copy()
        profile_b['Bucket'] = np.where(profile_b['Lines_Count'] >= 3, '3+', profile_b['Lines_Count'].astype(str))
        agg_config_bucket = {'Vehicle_Count': ('ID', 'nunique')}
        if 'Avg_Speed' in profile_b.columns:
            agg_config_bucket['Avg_Speed'] = ('Avg_Speed', 'mean')
        bucket_temp = profile_b.groupby('Bucket', as_index=False).agg(**agg_config_bucket)
        bucket = bucket_temp.reindex(columns=bucket_cols)


    tracking_line_by_class_df_cols = ['Line', 'Class', 'Count']
    tracking_line_by_class_df = pd.DataFrame(columns=tracking_line_by_class_df_cols)
    if not df_crossing_data.empty and all(c in df_crossing_data.columns for c in ['Line', 'Class']):
        try:
            # IDë³„ë¡œ í•œ ë²ˆë§Œ ì¹´ìš´íŠ¸í•˜ë ¤ë©´ nunique() ì‚¬ìš©
            # grouped_size_series = df_crossing_data.groupby(['Line', 'Class'])['ID'].nunique()
            # ë§Œì•½ ê° í†µê³¼ ì´ë²¤íŠ¸ë¥¼ ëª¨ë‘ ì¹´ìš´íŠ¸í•˜ë ¤ë©´ size() ì‚¬ìš©
            grouped_size_series = df_crossing_data.groupby(['Line', 'Class']).size()
            df_with_count = grouped_size_series.reset_index(name='Count')
            tracking_line_by_class_df = df_with_count.reindex(columns=tracking_line_by_class_df_cols)
        except Exception as e_groupby_size:
            print(f"Error creating 'tracking_line_by_class_df': {e_groupby_size}")
            tracking_line_by_class_df = pd.DataFrame(columns=tracking_line_by_class_df_cols)


    df_id_avg_cols = ['ID', 'Class', 'IsInitialLine', 'Speed(km/h)', 'Acceleration(m/sÂ²)',
                      'Deceleration(m/sÂ²)', 'Acceleration (%)', 'Deceleration (%)']
    df_id_avg = pd.DataFrame(columns=df_id_avg_cols)
    if not df_distance_deceleration.empty:
        df_id1_list, df_id2_list = [], []
        map_id1 = {'ID1':'ID', 'Class1':'Class', 'Speed1(km/h)':'Speed(km/h)', 'Acceleration1(m/sÂ²)':'Acceleration(m/sÂ²)',
                   'Deceleration1(m/sÂ²)':'Deceleration(m/sÂ²)', 'Acceleration1 (%)':'Acceleration (%)',
                   'Deceleration1 (%)':'Deceleration (%)', 'IsInitialLine_ID1':'IsInitialLine'}
        map_id2 = {'ID2':'ID', 'Class2':'Class', 'Speed2(km/h)':'Speed(km/h)', 'Acceleration2(m/sÂ²)':'Acceleration(m/sÂ²)',
                   'Deceleration2(m/sÂ²)':'Deceleration(m/sÂ²)', 'Acceleration2 (%)':'Acceleration (%)',
                   'Deceleration2 (%)':'Deceleration (%)', 'IsInitialLine_ID2':'IsInitialLine'}

        for _, row_dd in df_distance_deceleration.iterrows():
            id1_data = {target_col: row_dd.get(source_col, np.nan) for source_col, target_col in map_id1.items()}
            df_id1_list.append(id1_data)
            id2_data = {target_col: row_dd.get(source_col, np.nan) for source_col, target_col in map_id2.items()}
            df_id2_list.append(id2_data)

        df_all_ids_for_avg = pd.concat([pd.DataFrame(df_id1_list), pd.DataFrame(df_id2_list)], ignore_index=True)

        if not df_all_ids_for_avg.empty and all(c in df_all_ids_for_avg.columns for c in ['ID', 'Class', 'IsInitialLine']):
            group_keys = ['ID', 'Class', 'IsInitialLine']
            numeric_cols_for_mean = [col for col in df_id_avg_cols if col not in group_keys and col in df_all_ids_for_avg.columns]

            if numeric_cols_for_mean:
                df_id_avg_temp = df_all_ids_for_avg.groupby(group_keys, as_index=False)[numeric_cols_for_mean].mean()
                df_id_avg = df_id_avg_temp.reindex(columns=df_id_avg_cols)
            else:
                df_id_avg_temp = df_all_ids_for_avg[group_keys].drop_duplicates().reset_index(drop=True)
                df_id_avg = df_id_avg_temp.reindex(columns=df_id_avg_cols, fill_value=np.nan)

    # --- CSV íŒŒì¼ ì €ì¥ ë¡œì§ ---
    def save_df_to_csv(df, file_name_suffix):
        if not df.empty:
            # íŒŒì¼ëª…ì„ ë” ì§§ê²Œ êµ¬ì„± - ë‚ ì§œëŠ” ê°„ì†Œí™”, ë¹„ë””ì˜¤ëª…ì€ ì¶•ì•½
            date_short = current_datetime.split('_')[0]  # ë‚ ì§œë§Œ ì¶”ì¶œ (ì‹œê°„ ì œì™¸)
            safe_file_name_suffix = sanitize_filename(file_name_suffix, 30)
            safe_video_name_csv = sanitize_filename(video_name_without_ext, 40)
            safe_model_name_csv = sanitize_filename(model_name.replace('.pt', ''), 15)
            
            file_name = f"{date_short}_{safe_model_name_csv}_{safe_video_name_csv}_{safe_file_name_suffix}.csv"
            file_name = sanitize_filename(file_name, 150)  # Windows íŒŒì¼ëª… ê¸¸ì´ ì œí•œ ê°•í™”
            
            file_path = os.path.join(output_folder_path, file_name)
            # ê²½ë¡œ ì •ê·œí™”
            file_path = os.path.normpath(file_path)
            
            print(f"CSV ì €ì¥ ì‹œë„: {file_path}")
            
            try:
                # ì¶œë ¥ í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
                if not os.path.exists(output_folder_path):
                    print(f"ì¶œë ¥ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ. ìƒì„± ì‹œë„: {output_folder_path}")
                    os.makedirs(output_folder_path, exist_ok=True)
                    print(f"ì¶œë ¥ í´ë” ìƒì„± ì™„ë£Œ: {output_folder_path}")
                
                # íŒŒì¼ ë””ë ‰í† ë¦¬ í™•ì¸
                file_dir = os.path.dirname(file_path)
                if not os.path.exists(file_dir):
                    print(f"íŒŒì¼ ë””ë ‰í† ë¦¬ ìƒì„±: {file_dir}")
                    os.makedirs(file_dir, exist_ok=True)
                
                # CSV íŒŒì¼ ì €ì¥
                df.to_csv(file_path, index=False, encoding='utf-8-sig')
                print(f"ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨: {file_path}")
                
                # íŒŒì¼ ì €ì¥ í™•ì¸
                if os.path.exists(file_path):
                    file_size = os.path.getsize(file_path)
                    print(f"ì €ì¥ëœ íŒŒì¼ í¬ê¸°: {file_size} bytes")
                else:
                    print("ê²½ê³ : íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    
            except PermissionError as e:
                print(f"ê¶Œí•œ ì˜¤ë¥˜ë¡œ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
                # í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥ ì‹œë„
                fallback_path = os.path.join(os.getcwd(), file_name)
                try:
                    df.to_csv(fallback_path, index=False, encoding='utf-8-sig')
                    print(f"Fallbackìœ¼ë¡œ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥ë¨: {fallback_path}")
                except Exception as e2:
                    print(f"Fallback ì €ì¥ë„ ì‹¤íŒ¨: {e2}")
            except Exception as e:
                print(f"{file_name_suffix} CSV íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print(f"ì‹œë„í•œ íŒŒì¼ ê²½ë¡œ: {file_path}")
                print(f"ì¶œë ¥ í´ë” ì¡´ì¬ ì—¬ë¶€: {os.path.exists(output_folder_path)}")
                print(f"ì¶œë ¥ í´ë” ì“°ê¸° ê¶Œí•œ: {os.access(output_folder_path, os.W_OK) if os.path.exists(output_folder_path) else 'N/A'}")
                import traceback
                traceback.print_exc()
        else:
            print(f"{file_name_suffix} DataFrameì´ ë¹„ì–´ ìˆì–´ CSV íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    try:
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_df = pd.DataFrame({
            "Info": ["Date/Time", "Video Name", "Model Name", "Analysis Duration (hours)", "FPS"],
            "Value": [current_datetime, video_name, model_name, f"{total_duration_hours:.2f}", f"{fps:.2f}"]
        })
        save_df_to_csv(metadata_df, "Metadata")

        save_df_to_csv(traffic_summary_overall, "OverallSummary")
        save_df_to_csv(traffic_summary_per_class, "ClassSummary")
        save_df_to_csv(df_crossing_data, "CrossingEvents")
        save_df_to_csv(df_speed, "SpeedLog")
        save_df_to_csv(avg_speed_df, "AvgSpeedPerID")
        save_df_to_csv(df_all_vehicles, "AllDetectionsLog") # df_presenceì™€ ë‚´ìš©ì´ ìœ ì‚¬í•˜ë¯€ë¡œ í•˜ë‚˜ë§Œ ì €ì¥
        save_df_to_csv(df_distance_deceleration, "InterVehicleDistance")
        save_df_to_csv(hourly_data, "HourlyCounts")
        save_df_to_csv(interval_df, "5secIntervalLog")

        for cat_idx, (cat_name, cat_df) in enumerate(category_id_dfs.items()):
            save_df_to_csv(cat_df, f"IDs_Cat{cat_idx+1}")

        save_df_to_csv(df_region, "RegionLog")
        save_df_to_csv(df_region_summary, "RegionSummaryCounts")
        save_df_to_csv(df_region_line_stats, "RegionLineCrossingStats")

        df_class_initial_pass_counts_list = []
        if unique_ids_per_class:
            for cls_item, total_ids_set in unique_ids_per_class.items():
                passed_initial_count = class_counts_O.get(cls_item, 0)
                not_passed_initial_count = len(total_ids_set) - passed_initial_count
                df_class_initial_pass_counts_list.append({
                    'Class': cls_item, 'Passed_Initial_Line': passed_initial_count,
                    'Not_Passed_Initial_Line': not_passed_initial_count, 'Total_Unique_IDs': len(total_ids_set)})
            if df_class_initial_pass_counts_list:
                save_df_to_csv(pd.DataFrame(df_class_initial_pass_counts_list), "ClassInitialLinePassCounts")

        lane_events_df = pd.DataFrame(lane_events, columns=["Frame", "Time", "Lane", "Status", "Count", "Details"])
        save_df_to_csv(lane_events_df, "LaneEvents")

        lane_thresh_list = []
        if lane_thresholds or lane_polygons:
             for i in range(len(lane_polygons)):
                 lane_thresh_list.append({'Lane': f'Lane {i+1}', 'Threshold': lane_thresholds.get(i, "N/A")})
        if lane_thresh_list:
             save_df_to_csv(pd.DataFrame(lane_thresh_list), "LaneThresholds")
        elif not lane_thresh_list and (lane_thresholds or lane_polygons):
             save_df_to_csv(pd.DataFrame(columns=['Lane', 'Threshold']), "LaneThresholds_Empty")


        save_df_to_csv(profile, "Vehicle_Line_Profile")
        save_df_to_csv(pivot, "LineCount_Pivot") # pivotì˜ ì»¬ëŸ¼ì€ ë™ì ì´ë¯€ë¡œ, CSVë¡œ ì €ì¥í•˜ë©´ ëª¨ë“  ì»¬ëŸ¼ì´ ì˜ ì €ì¥ë¨
        save_df_to_csv(bucket, "Lines_Count_Buckets")
        save_df_to_csv(tracking_line_by_class_df, "TrackingLineByClass")
        save_df_to_csv(df_id_avg, "DecelDist_Avg_per_ID")
        
        # ìƒˆë¡œìš´ CSV íŒŒì¼ ì €ì¥ - ì°¨ëŸ‰ë³„ íƒ€ì„ë¼ì¸ ë¡œê·¸
        save_df_to_csv(df_timeline, "Vehicle_Timeline_Detail")

        print(f"Processing ì™„ë£Œ. ê²°ê³¼ CSV íŒŒì¼ë“¤ì€ {output_folder_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"CSV íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\n{error_details}")
        messagebox.showerror("ì˜¤ë¥˜", f"CSV íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\n\nì„¸ë¶€ ì •ë³´:\n{error_details[:500]}...")
        return

    messagebox.showinfo("ì™„ë£Œ", "ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ëŠ” CSV íŒŒì¼ë“¤ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    open_folder(output_folder_path)


def perform_setup(input_video_path):
    cap = cv2.VideoCapture(input_video_path)
    if not cap.isOpened():
        messagebox.showerror("ì˜¤ë¥˜", f"ì˜ìƒ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_video_path}")
        return None, None, None
    ret, frame_setup = cap.read()
    if not ret:
        messagebox.showerror("ì˜¤ë¥˜", "ì²« í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
        cap.release()
        return None, None, None

    cv2.namedWindow('Setup ROI', cv2.WINDOW_NORMAL)
    # setup_win_w = frame_setup.shape[1] // 2 # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
    # setup_win_h = frame_setup.shape[0] // 2 # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
    # cv2.resizeWindow('Setup ROI', setup_win_w, setup_win_h) # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ


    rect_points_setup = []
    current_dragging_point_idx = None

    def mouse_roi_setup(event, x, y, flags, param):
        nonlocal rect_points_setup, current_dragging_point_idx

        if event == cv2.EVENT_LBUTTONDOWN:
            if len(rect_points_setup) < 4:
                rect_points_setup.append((x, y))
            else:
                for i, p_roi in enumerate(rect_points_setup):
                    if dist(p_roi, (x, y)) < Constants.DRAG_DETECTION_RADIUS:
                        current_dragging_point_idx = i
                        break

        elif event == cv2.EVENT_MOUSEMOVE:
            if current_dragging_point_idx is not None and flags & cv2.EVENT_FLAG_LBUTTON:
                if 0 <= current_dragging_point_idx < len(rect_points_setup):
                    rect_points_setup[current_dragging_point_idx] = (x, y)

        elif event == cv2.EVENT_LBUTTONUP:
            current_dragging_point_idx = None

    cv2.setMouseCallback('Setup ROI', mouse_roi_setup)

    help_text_roi = "Click 4 points for ROI (e.g., TL, TR, BR, BL).\nDrag to adjust. 'r':Reset, 's':Save, 'q':Quit"

    while True:
        draw_frame_setup = frame_setup.copy()

        y_text_start = 30
        for i, line_text in enumerate(help_text_roi.split('\n')): # ë³€ìˆ˜ëª… ë³€ê²½ line -> line_text
            cv2.putText(draw_frame_setup, line_text, (30, y_text_start + i * 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (50,200,50), 1)


        for i, p_roi_draw in enumerate(rect_points_setup):
            color = (0,0,255)
            cv2.circle(draw_frame_setup, p_roi_draw, 7, color, -1)
            cv2.putText(draw_frame_setup, str(i+1), (int(p_roi_draw[0])+10, int(p_roi_draw[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)

        if len(rect_points_setup) == 4:
            pts_arr = np.array(rect_points_setup, dtype=np.int32)
            cv2.polylines(draw_frame_setup, [pts_arr], True, (0,255,0), 2)

        cv2.imshow('Setup ROI', draw_frame_setup)
        key = cv2.waitKey(1) & 0xFF

        if key == ord('r'):
            rect_points_setup.clear()
            current_dragging_point_idx = None
        elif key == ord('s'):
            if len(rect_points_setup) == 4:
                popup_root = tk.Tk()
                popup_root.withdraw()
                try:
                    width_m_input = simpledialog.askfloat("Input Real Width", "Enter REAL WORLD width of the ROI (meters):", parent=popup_root, minvalue=0.01)
                    if width_m_input is None: raise ValueError("Width input cancelled.")

                    height_m_input = simpledialog.askfloat("Input Real Height", "Enter REAL WORLD height of the ROI (meters):", parent=popup_root, minvalue=0.01)
                    if height_m_input is None: raise ValueError("Height input cancelled.")

                    popup_root.destroy()
                    cv2.destroyWindow('Setup ROI')
                    cap.release()
                    return rect_points_setup, width_m_input, height_m_input

                except ValueError as e:
                    messagebox.showerror("Error", f"Invalid input: {e}", parent=popup_root)
                    if popup_root.winfo_exists(): popup_root.destroy()
                except Exception as e:
                    messagebox.showerror("Error", f"An unexpected error occurred: {e}", parent=popup_root)
                    if popup_root.winfo_exists(): popup_root.destroy()
            else:
                messagebox.showwarning("Setup ROI", "Please define 4 points for the ROI first.")

        elif key == ord('q'):
            cv2.destroyWindow('Setup ROI')
            cap.release()
            return None, None, None

def perform_aerial_setup(input_video_path):
    """í•­ê³µë·° ëª¨ë“œì—ì„œ ë‹¤ì¤‘ ê¸°ì¤€ì„  ì„¤ì •ì„ ìœ„í•œ í•¨ìˆ˜"""
    cap = cv2.VideoCapture(input_video_path)
    if not cap.isOpened():
        messagebox.showerror("ì˜¤ë¥˜", f"ì˜ìƒ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_video_path}")
        return None
    
    ret, frame_setup = cap.read()
    if not ret:
        messagebox.showerror("ì˜¤ë¥˜", "ì²« í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
        cap.release()
        return None
    
    cv2.namedWindow('Aerial View Setup', cv2.WINDOW_NORMAL)
    
    # í•­ê³µë·° ì„¤ì •ìš© ë³€ìˆ˜ë“¤
    scale_lines = []  # ì—¬ëŸ¬ ê¸°ì¤€ì„ ì„ ì €ì¥
    current_line_points = []
    drawing_line = False
    start_point = None
    
    # í™•ëŒ€/ì¶•ì†Œ ê´€ë ¨ ë³€ìˆ˜
    zoom_factor = 1.0
    pan_x, pan_y = 0, 0
    original_frame = frame_setup.copy()
    
    def get_frame_with_zoom():
        """í™•ëŒ€/ì¶•ì†Œê°€ ì ìš©ëœ í”„ë ˆì„ ë°˜í™˜"""
        if zoom_factor == 1.0:
            return original_frame.copy()
        
        # í™•ëŒ€ëœ í”„ë ˆì„ ìƒì„±
        h, w = original_frame.shape[:2]
        new_w, new_h = int(w * zoom_factor), int(h * zoom_factor)
        zoomed = cv2.resize(original_frame, (new_w, new_h))
        
        # íŒ¬ë‹ ì ìš©
        start_x = max(0, min(new_w - w, pan_x))
        start_y = max(0, min(new_h - h, pan_y))
        end_x = min(new_w, start_x + w)
        end_y = min(new_h, start_y + h)
        
        return zoomed[start_y:end_y, start_x:end_x]
    
    def screen_to_original_coords(x, y):
        """í™”ë©´ ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜"""
        if zoom_factor == 1.0:
            return x, y
        
        orig_x = int((x + pan_x) / zoom_factor)
        orig_y = int((y + pan_y) / zoom_factor)
        return orig_x, orig_y
    
    # íœ  ë“œë˜ê·¸ íŒ¬ë‹ ê´€ë ¨ ë³€ìˆ˜
    panning = False
    last_pan_pos = None
    
    def aerial_setup_mouse_callback(event, x, y, flags, param):
        nonlocal current_line_points, drawing_line, start_point
        nonlocal panning, last_pan_pos, pan_x, pan_y
        
        # í™”ë©´ ì¢Œí‘œë¥¼ ì›ë³¸ ì¢Œí‘œë¡œ ë³€í™˜
        orig_x, orig_y = screen_to_original_coords(x, y)
        
        if event == cv2.EVENT_MBUTTONDOWN:  # íœ (ê°€ìš´ë°) ë²„íŠ¼ ëˆŒë¦¼
            panning = True
            last_pan_pos = (x, y)
            print("íŒ¬ë‹ ëª¨ë“œ ì‹œì‘ (íœ  ë“œë˜ê·¸)")
        
        elif event == cv2.EVENT_MBUTTONUP:  # íœ  ë²„íŠ¼ ë—Œ
            panning = False
            last_pan_pos = None
            print("íŒ¬ë‹ ëª¨ë“œ ì¢…ë£Œ")
        
        elif event == cv2.EVENT_MOUSEMOVE and panning:  # íœ  ë²„íŠ¼ ëˆ„ë¥¸ ì±„ë¡œ ë“œë˜ê·¸
            if last_pan_pos is not None and zoom_factor > 1.0:
                # ë§ˆìš°ìŠ¤ ì´ë™ëŸ‰ ê³„ì‚°
                dx = x - last_pan_pos[0]
                dy = y - last_pan_pos[1]
                
                # íŒ¬ë‹ ì—…ë°ì´íŠ¸ (ì´ë™ ë°©í–¥ ë°˜ëŒ€ë¡œ)
                pan_x = max(0, pan_x - dx)
                pan_y = max(0, pan_y - dy)
                
                last_pan_pos = (x, y)
        
        elif event == cv2.EVENT_LBUTTONDOWN and not panning:  # ì¢Œí´ë¦­ (íŒ¬ë‹ ì¤‘ì´ ì•„ë‹ ë•Œë§Œ)
            start_point = (orig_x, orig_y)
            drawing_line = True
        
        elif event == cv2.EVENT_LBUTTONUP and not panning:  # ì¢Œí´ë¦­ ë—Œ
            if drawing_line and start_point:
                # ìƒˆ ë¼ì¸ ì™„ì„±
                end_point = (orig_x, orig_y)
                line_length = math.sqrt((end_point[0] - start_point[0])**2 + 
                                      (end_point[1] - start_point[1])**2)
                if line_length > 5:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                    current_line_points = [start_point, end_point]
                drawing_line = False
                start_point = None
        
        elif event == cv2.EVENT_MOUSEWHEEL:  # ë§ˆìš°ìŠ¤ íœ  ìŠ¤í¬ë¡¤
            if flags > 0:  # íœ  ì—… (í™•ëŒ€)
                zoom_factor = min(5.0, zoom_factor * 1.1)
                print(f"íœ  í™•ëŒ€: {zoom_factor:.1f}x")
            else:  # íœ  ë‹¤ìš´ (ì¶•ì†Œ)
                zoom_factor = max(0.5, zoom_factor / 1.1)
                if zoom_factor == 1.0:
                    pan_x = pan_y = 0  # 1xì¼ ë•Œ íŒ¬ ë¦¬ì…‹
                print(f"íœ  ì¶•ì†Œ: {zoom_factor:.1f}x")
    
    cv2.setMouseCallback('Aerial View Setup', aerial_setup_mouse_callback)
    
    messagebox.showinfo("í•­ê³µë·° ë‹¤ì¤‘ ê¸°ì¤€ì„  ì„¤ì •", 
                        "í•­ê³µë·° ë‹¤ì¤‘ ê¸°ì¤€ì„  ì„¤ì • (ì˜¤ì°¨ ê°ì†Œ)\n\n"
                        "ğŸ–±ï¸ ë§ˆìš°ìŠ¤ ì¡°ì‘:\n"
                        "â€¢ ì¢Œí´ë¦­ ë“œë˜ê·¸: ê¸°ì¤€ì„  ê·¸ë¦¬ê¸° (ì°¨ì„  í­ 3.5m ê¶Œì¥)\n"
                        "â€¢ íœ  ìŠ¤í¬ë¡¤: í™•ëŒ€/ì¶•ì†Œ\n"
                        "â€¢ íœ  ë²„íŠ¼ ë“œë˜ê·¸: í™”ë©´ ì´ë™ (AutoCAD ë°©ì‹)\n\n"
                        "âŒ¨ï¸ í‚¤ë³´ë“œ ì¡°ì‘:\n"
                        "â€¢ 'a' í‚¤: í˜„ì¬ ì„ ì„ ê¸°ì¤€ì„  ëª©ë¡ì— ì¶”ê°€\n"
                        "â€¢ 'd' í‚¤: ë§ˆì§€ë§‰ ê¸°ì¤€ì„  ì‚­ì œ\n"
                        "â€¢ '+/-' í‚¤: í‚¤ë³´ë“œë¡œ í™•ëŒ€/ì¶•ì†Œ\n"
                        "â€¢ 'r' í‚¤: í™•ëŒ€/íŒ¬ ì´ˆê¸°í™”\n"
                        "â€¢ Enter: ì„¤ì • ì™„ë£Œ | q: ì·¨ì†Œ\n\n"
                        "ğŸ’¡ ì—¬ëŸ¬ ê°œ ê¸°ì¤€ì„ ì„ ì¸¡ì •í•˜ë©´ í‰ê· ê°’ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ!")
    
    while True:
        # í™•ëŒ€/ì¶•ì†Œê°€ ì ìš©ëœ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
        frame_copy = get_frame_with_zoom()
        
        def original_to_screen_coords(orig_x, orig_y):
            """ì›ë³¸ ì¢Œí‘œë¥¼ í™”ë©´ ì¢Œí‘œë¡œ ë³€í™˜"""
            if zoom_factor == 1.0:
                return orig_x, orig_y
            screen_x = int(orig_x * zoom_factor - pan_x)
            screen_y = int(orig_y * zoom_factor - pan_y)
            return screen_x, screen_y
        
        # ì €ì¥ëœ ê¸°ì¤€ì„ ë“¤ í‘œì‹œ (íŒŒë€ìƒ‰)
        for i, line in enumerate(scale_lines):
            p1_screen = original_to_screen_coords(line['points'][0][0], line['points'][0][1])
            p2_screen = original_to_screen_coords(line['points'][1][0], line['points'][1][1])
            
            # í™”ë©´ ë²”ìœ„ ë‚´ì— ìˆëŠ” ì„ ë§Œ ê·¸ë¦¬ê¸°
            h, w = frame_copy.shape[:2]
            if (0 <= p1_screen[0] <= w and 0 <= p1_screen[1] <= h) or \
               (0 <= p2_screen[0] <= w and 0 <= p2_screen[1] <= h):
                cv2.line(frame_copy, p1_screen, p2_screen, (255, 0, 0), 3)  # íŒŒë€ìƒ‰
                cv2.circle(frame_copy, p1_screen, 5, (255, 0, 0), -1)
                cv2.circle(frame_copy, p2_screen, 5, (255, 0, 0), -1)
                
                # ê¸°ì¤€ì„  ë²ˆí˜¸ í‘œì‹œ
                mid_point = ((p1_screen[0] + p2_screen[0]) // 2,
                           (p1_screen[1] + p2_screen[1]) // 2)
                cv2.putText(frame_copy, f"Line {i+1}: {line['length']:.1f}px", 
                          (mid_point[0] - 60, mid_point[1] - 15), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
        
        # í˜„ì¬ ê·¸ë¦¬ê³  ìˆëŠ” ê¸°ì¤€ì„  í‘œì‹œ (ì´ˆë¡ìƒ‰)
        if len(current_line_points) == 2:
            p1_screen = original_to_screen_coords(current_line_points[0][0], current_line_points[0][1])
            p2_screen = original_to_screen_coords(current_line_points[1][0], current_line_points[1][1])
            
            cv2.line(frame_copy, p1_screen, p2_screen, (0, 255, 0), 3)  # ì´ˆë¡ìƒ‰
            cv2.circle(frame_copy, p1_screen, 5, (0, 255, 0), -1)
            cv2.circle(frame_copy, p2_screen, 5, (0, 255, 0), -1)
            
            # í˜„ì¬ ì„  ê¸¸ì´ í‘œì‹œ
            line_length = math.sqrt((current_line_points[1][0] - current_line_points[0][0])**2 + 
                                  (current_line_points[1][1] - current_line_points[0][1])**2)
            mid_point = ((p1_screen[0] + p2_screen[0]) // 2,
                        (p1_screen[1] + p2_screen[1]) // 2)
            cv2.putText(frame_copy, f"Current: {line_length:.1f}px", 
                       (mid_point[0] - 60, mid_point[1] - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        # ìƒíƒœ ì •ë³´ í‘œì‹œ
        info_text = f"Lines: {len(scale_lines)} | Zoom: {zoom_factor:.1f}x"
        cv2.putText(frame_copy, info_text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # ë„ì›€ë§ í‘œì‹œ
        help_text = "a:Add  d:Delete  Wheel:Zoom  MiddleDrag:Pan  r:Reset  Enter:Done"
        cv2.putText(frame_copy, help_text, (10, frame_copy.shape[0] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
        
        # íŒ¬ë‹ ëª¨ë“œ í‘œì‹œ
        if panning:
            cv2.putText(frame_copy, "PANNING MODE", (10, 60), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        
        # ì•ˆë‚´ í…ìŠ¤íŠ¸
        cv2.putText(frame_copy, "Drag to draw reference line", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(frame_copy, "Press Enter to confirm, q to cancel", (10, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        cv2.imshow('Aerial View Setup', frame_copy)
        
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('a'):  # í˜„ì¬ ì„ ì„ ê¸°ì¤€ì„  ëª©ë¡ì— ì¶”ê°€
            if len(current_line_points) == 2:
                line_length = math.sqrt((current_line_points[1][0] - current_line_points[0][0])**2 + 
                                      (current_line_points[1][1] - current_line_points[0][1])**2)
                scale_lines.append({
                    'points': current_line_points.copy(),
                    'length': line_length
                })
                current_line_points.clear()
                print(f"ê¸°ì¤€ì„  ì¶”ê°€ë¨. ì´ {len(scale_lines)}ê°œ ê¸°ì¤€ì„ ")
            else:
                print("ë¨¼ì € ê¸°ì¤€ì„ ì„ ê·¸ë ¤ì£¼ì„¸ìš”.")
        
        elif key == ord('d'):  # ë§ˆì§€ë§‰ ê¸°ì¤€ì„  ì‚­ì œ
            if scale_lines:
                scale_lines.pop()
                print(f"ë§ˆì§€ë§‰ ê¸°ì¤€ì„  ì‚­ì œ. ì´ {len(scale_lines)}ê°œ ê¸°ì¤€ì„ ")
            else:
                print("ì‚­ì œí•  ê¸°ì¤€ì„ ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        elif key == ord('+') or key == ord('='):  # í™•ëŒ€
            zoom_factor = min(5.0, zoom_factor * 1.2)
            print(f"í™•ëŒ€: {zoom_factor:.1f}x")
        
        elif key == ord('-') or key == ord('_'):  # ì¶•ì†Œ
            zoom_factor = max(0.5, zoom_factor / 1.2)
            if zoom_factor == 1.0:
                pan_x = pan_y = 0  # 1xì¼ ë•Œ íŒ¬ ë¦¬ì…‹
            print(f"ì¶•ì†Œ: {zoom_factor:.1f}x")
        
        elif key == ord('r'):  # í™•ëŒ€/íŒ¬ ì´ˆê¸°í™”
            zoom_factor = 1.0
            pan_x = pan_y = 0
            print("í™•ëŒ€/íŒ¬ ì´ˆê¸°í™”")
        
        elif key == 13:  # Enter í‚¤ - ì„¤ì • ì™„ë£Œ
            if len(scale_lines) > 0:
                # ê±°ë¦¬ ì…ë ¥ ë°›ê¸°
                distance_input = simpledialog.askfloat("ìŠ¤ì¼€ì¼ ì„¤ì •", 
                                                     f"ê¸°ì¤€ì„  {len(scale_lines)}ê°œì˜ ì‹¤ì œ ê±°ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”:\n"
                                                     "(ì˜ˆ: ì°¨ì„  í­ 3.5m)", 
                                                     minvalue=0.1, maxvalue=1000.0)
                if distance_input:
                    # ì—¬ëŸ¬ ê¸°ì¤€ì„ ì˜ í‰ê·  í”½ì…€ ê±°ë¦¬ ê³„ì‚°
                    total_pixel_length = sum(line['length'] for line in scale_lines)
                    avg_pixel_length = total_pixel_length / len(scale_lines)
                    
                    meters_per_pixel = distance_input / avg_pixel_length
                    
                    # ì •í™•ë„ ì •ë³´ í‘œì‹œ
                    pixel_lengths = [line['length'] for line in scale_lines]
                    std_dev = (sum((x - avg_pixel_length)**2 for x in pixel_lengths) / len(pixel_lengths))**0.5
                    accuracy_percent = (1 - std_dev / avg_pixel_length) * 100 if avg_pixel_length > 0 else 0
                    
                    cv2.destroyWindow('Aerial View Setup')
                    cap.release()
                    
                    messagebox.showinfo("ìŠ¤ì¼€ì¼ ì„¤ì • ì™„ë£Œ", 
                                      f"ê¸°ì¤€ì„  {len(scale_lines)}ê°œ í‰ê· ê°’ ì‚¬ìš©\n"
                                      f"í‰ê·  í”½ì…€ ê¸¸ì´: {avg_pixel_length:.1f}px\n"
                                      f"ìŠ¤ì¼€ì¼: {meters_per_pixel:.6f} m/px\n"
                                      f"ì¸¡ì • ì •í™•ë„: {accuracy_percent:.1f}%")
                    
                    return meters_per_pixel
                else:
                    messagebox.showwarning("ê²½ê³ ", "ê±°ë¦¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                messagebox.showwarning("ê²½ê³ ", "'a' í‚¤ë¡œ ê¸°ì¤€ì„ ì„ í•˜ë‚˜ ì´ìƒ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        
        elif key == ord('q'):
            cv2.destroyWindow('Aerial View Setup')
            cap.release()
            return None

def lane_setup(frame_for_lane):
    global lane_polygons, lane_thresholds
    lane_polygons_temp = []
    lane_thresholds_temp = {}

    current_lane_points = []

    def lane_mouse_callback(event, x, y, flags, param):
        nonlocal current_lane_points
        if event == cv2.EVENT_LBUTTONDOWN:
            current_lane_points.append((x, y))
        elif event == cv2.EVENT_RBUTTONDOWN:
            if len(current_lane_points) >= 3:
                lane_idx_temp = len(lane_polygons_temp)
                lane_polygons_temp.append(np.array(current_lane_points, dtype=np.int32))

                popup_root_lane = tk.Tk(); popup_root_lane.withdraw()
                threshold_val = simpledialog.askfloat("Lane Threshold", f"Enter threshold for Lane {lane_idx_temp + 1}:", parent=popup_root_lane, minvalue=0)
                popup_root_lane.destroy()

                if threshold_val is not None:
                    lane_thresholds_temp[lane_idx_temp] = threshold_val
                else:
                    lane_polygons_temp.pop()
                    messagebox.showinfo("Lane Setup", f"Lane {lane_idx_temp + 1} setup cancelled (no threshold).")
                current_lane_points.clear()
            else:
                messagebox.showwarning("Lane Setup", "A lane polygon needs at least 3 points.")

    cv2.namedWindow('Lane Setup', cv2.WINDOW_NORMAL)
    # setup_win_w_lane = frame_for_lane.shape[1] // 2 # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
    # setup_win_h_lane = frame_for_lane.shape[0] // 2 # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
    # cv2.resizeWindow('Lane Setup', setup_win_w_lane, setup_win_h_lane) # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
    cv2.setMouseCallback('Lane Setup', lane_mouse_callback)

    help_text_lane = "Lanes: LClick=add pt, RClick=finish poly. 's':Save&Exit, 'r':Reset last, 'c':Clear all, 'q':Quit"

    while True:
        display_frame_lane = frame_for_lane.copy()
        y_text_start_lane = 20
        for i, line_text in enumerate(help_text_lane.split('\n')): # ë³€ìˆ˜ëª… ë³€ê²½ line -> line_text
             cv2.putText(display_frame_lane, line_text, (20, y_text_start_lane + i*20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (50,200,50), 1)


        if len(current_lane_points) > 0:
            pts_current_lane = np.array(current_lane_points, dtype=np.int32)
            cv2.polylines(display_frame_lane, [pts_current_lane], False, (0,255,255), 2)
            for pt_cl in current_lane_points:
                cv2.circle(display_frame_lane, pt_cl, 4, (0,255,255), -1)

        for idx_lp, poly_pts_lp in enumerate(lane_polygons_temp):
            cv2.polylines(display_frame_lane, [poly_pts_lp], True, (255,0,0), 2)
            text_pos_lp = tuple(np.mean(poly_pts_lp, axis=0, dtype=np.int32))
            thresh_lp = lane_thresholds_temp.get(idx_lp, "N/A")
            cv2.putText(display_frame_lane, f"L{idx_lp+1}({thresh_lp})", text_pos_lp, cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)

        cv2.imshow('Lane Setup', display_frame_lane)
        key = cv2.waitKey(1) & 0xFF

        if key == ord('s'):
            lane_polygons = lane_polygons_temp
            lane_thresholds = lane_thresholds_temp
            messagebox.showinfo("Lane Setup", f"{len(lane_polygons)} lane(s) saved.")
            break
        elif key == ord('r'):
            if current_lane_points: current_lane_points.clear()
            elif lane_polygons_temp:
                removed_idx = len(lane_polygons_temp) - 1
                lane_polygons_temp.pop()
                if removed_idx in lane_thresholds_temp: del lane_thresholds_temp[removed_idx]
        elif key == ord('c'):
            current_lane_points.clear(); lane_polygons_temp.clear(); lane_thresholds_temp.clear()
        elif key == ord('q'):
            messagebox.showinfo("Lane Setup", "Lane setup quit. No changes saved to global variables.")
            break

    cv2.destroyWindow('Lane Setup')


def start_analysis(input_video_path, model_path, help_text_main_unused, analysis_mode="homography"):
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì˜ìƒ íŒŒì¼ ì²´í¬
    if analysis_mode != "test":
        if not input_video_path or not model_path:
            messagebox.showwarning("ê²½ê³ ", "ì˜ìƒ íŒŒì¼ê³¼ ëª¨ë¸ íŒŒì¼ì„ ëª¨ë‘ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
    else:
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ëª¨ë¸ íŒŒì¼ë§Œ ì²´í¬
        if not model_path:
            messagebox.showwarning("ê²½ê³ ", "ëª¨ë¸ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return

    if analysis_mode == "test":
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œìš© ì¶œë ¥ í´ë” ìƒì„±
        output_folder_path, current_datetime = create_test_output_folder(model_path)
    else:
        output_folder_path, current_datetime, video_name_str, model_name_str, video_name_no_ext_str = \
            create_output_folder(input_video_path, model_path)

    global aerial_view_mode, aerial_meters_per_pixel, driving_mode
    
    if analysis_mode == "test":
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: PT ëª¨ë¸ + ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸
        if not model_path:
            messagebox.showwarning("ê²½ê³ ", "í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” PT ëª¨ë¸ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì„ íƒ
        test_images = filedialog.askopenfilenames(
            title="í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì„ íƒ",
            filetypes=[
                ("ì´ë¯¸ì§€ íŒŒì¼", "*.jpg *.jpeg *.png *.bmp *.tiff"),
                ("ëª¨ë“  íŒŒì¼", "*.*")
            ]
        )
        
        if not test_images:
            messagebox.showinfo("ì •ë³´", "í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        messagebox.showinfo("í…ŒìŠ¤íŠ¸ ëª¨ë“œ", 
                           f"PT ëª¨ë¸ + ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ ëª¨ë“œì…ë‹ˆë‹¤.\n"
                           f"ì„ íƒëœ ì´ë¯¸ì§€: {len(test_images)}ì¥\n"
                           f"ì‚¬ìš©í•  ëª¨ë¸: {os.path.basename(model_path)}")
        
        run_image_test_mode(test_images, model_path, output_folder_path, current_datetime)
        return
    elif analysis_mode == "driving":
        # ì£¼í–‰ëª¨ë“œ: í‘œì§€íŒ ê²€ì§€ ì „ìš©
        driving_mode = True
        aerial_view_mode = False
        aerial_meters_per_pixel = None
        
        # ë”ë¯¸ ROI ë°ì´í„° (ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„ ìœ„í•´)
        rect_points_roi = np.array([[0, 0], [100, 0], [100, 100], [0, 100]], dtype=np.float32)
        width_m_val = 1.0   # ë”ë¯¸ ê°’
        height_m_val = 1.0  # ë”ë¯¸ ê°’
        
        messagebox.showinfo("ì£¼í–‰ëª¨ë“œ ì‹œì‘", 
                           "ì£¼í–‰ëª¨ë“œê°€ ì‹œì‘ë©ë‹ˆë‹¤.\n"
                           "í‘œì§€íŒ ê²€ì§€ ì‹œ ìë™ìœ¼ë¡œ ìŠ¤í¬ë¦°ìƒ·ì„ ìº¡ì²˜í•˜ê³ \n"
                           "CSV íŒŒì¼ë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
    elif analysis_mode == "aerial":
        # í•­ê³µë·° ëª¨ë“œ: ê¸°ì¤€ì„  ì„¤ì • ë¨¼ì € ìˆ˜í–‰
        aerial_view_mode = True
        aerial_meters_per_pixel = perform_aerial_setup(input_video_path)
        if aerial_meters_per_pixel is None:
            messagebox.showinfo("ì •ë³´", "í•­ê³µë·° ì„¤ì •ì´ ì·¨ì†Œë˜ì–´ ë¶„ì„ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë”ë¯¸ ROI ë°ì´í„° (í˜¸ëª¨ê·¸ë˜í”¼ í•¨ìˆ˜ í˜¸ì¶œì„ ìœ„í•´ í•„ìš”)
        rect_points_roi = np.array([[0, 0], [100, 0], [100, 100], [0, 100]], dtype=np.float32)
        width_m_val = 1.0   # ë”ë¯¸ ê°’ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        height_m_val = 1.0  # ë”ë¯¸ ê°’ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        
        messagebox.showinfo("í•­ê³µë·° ì„¤ì • ì™„ë£Œ", 
                           f"í•­ê³µë·° ìŠ¤ì¼€ì¼ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
                           f"ìŠ¤ì¼€ì¼: {aerial_meters_per_pixel:.6f} m/pixel\n\n"
                           "ì´ì œ ì˜ìƒ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        # í˜¸ëª¨ê·¸ë˜í”¼ ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹
        driving_mode = False
        aerial_view_mode = False
        aerial_meters_per_pixel = None
        rect_points_roi, width_m_val, height_m_val = perform_setup(input_video_path)
        if rect_points_roi is None:
            messagebox.showinfo("ì •ë³´", "ROI ì„¤ì •ì´ ì·¨ì†Œë˜ì–´ ë¶„ì„ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

    cap_temp_lane = cv2.VideoCapture(input_video_path)
    ret_lane, first_frame_lane = cap_temp_lane.read()
    cap_temp_lane.release()

    global lane_polygons, lane_thresholds
    lane_polygons = []
    lane_thresholds = {}

    if analysis_mode == "aerial":
        # í•­ê³µë·° ëª¨ë“œì—ì„œëŠ” ì°¨ì„  ì„¤ì • ìŠ¤í‚µ (í•„ìš”ì‹œ ë‚˜ì¤‘ì— ì¶”ê°€ ê°€ëŠ¥)
        pass
    elif analysis_mode == "driving":
        # ì£¼í–‰ëª¨ë“œì—ì„œëŠ” ì°¨ì„  ì„¤ì • ìŠ¤í‚µ
        pass
    else:
        # í˜¸ëª¨ê·¸ë˜í”¼ ëª¨ë“œì—ì„œë§Œ ì°¨ì„  ì„¤ì •
        if ret_lane:
            lane_setup(first_frame_lane)
        else:
            messagebox.showwarning("ì˜¤ë¥˜", "ì°¨ì„  ì„¤ì •ì„ ìœ„í•œ ì²« í”„ë ˆì„ì„ ì½ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì°¨ì„  ì„¤ì • ì—†ì´ ì§„í–‰ë©ë‹ˆë‹¤.")

    global tracking_lines
    tracking_lines = []

    analysis_thread = threading.Thread(target=analysis_process, args=(
        input_video_path, model_path, output_folder_path, current_datetime,
        video_name_str, model_name_str, video_name_no_ext_str,
        "", # help_textëŠ” analysis_processì—ì„œ ì‚¬ìš© ì•ˆ í•¨
        rect_points_roi, width_m_val, height_m_val
    ))
    analysis_thread.start()
    messagebox.showinfo("ë¶„ì„ ì‹œì‘", "ì˜ìƒ ë¶„ì„ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì™„ë£Œ ì‹œ ì•Œë¦¼ì´ í‘œì‹œë©ë‹ˆë‹¤.")


def main():
    root = tk.Tk()
    root.title("Traffic Super Vision (CSV Output Ver.)") # ì œëª©ì— CSV ëª…ì‹œ
    root.geometry("700x320")  # ë†’ì´ ì¦ê°€

    video_path_var = tk.StringVar()
    model_path_var = tk.StringVar()
    analysis_mode_var = tk.StringVar(value="homography")  # ê¸°ë³¸ê°’: í˜¸ëª¨ê·¸ë˜í”¼

    help_text_content = (
        'ì´ í”„ë¡œê·¸ë¨ì€ YOLO ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ì—ì„œ ê°ì²´ë¥¼ ì¶”ì í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.\n'
        'ë“œë¡ (drone, uav, aircraft, helicopter) ê²€ì¶œ ì‹œ ë³´ë¼ìƒ‰ìœ¼ë¡œ ê°•ì¡° í‘œì‹œë©ë‹ˆë‹¤.\n\n'
        '--- ì„¤ì • ë‹¨ê³„ ---\n'
        '1. [ë¶„ì„í•  ì˜ìƒ íŒŒì¼ ì„ íƒ]: MP4, AVI ë“±ì˜ ì˜ìƒ íŒŒì¼ì„ ì„ íƒí•©ë‹ˆë‹¤.\n'
        '2. [ì‚¬ìš©í•  ëª¨ë¸ íŒŒì¼ ì„ íƒ]: YOLO .pt ëª¨ë¸ íŒŒì¼ì„ ì„ íƒí•©ë‹ˆë‹¤.\n'
        '3. [ë¶„ì„ ëª¨ë“œ ì„ íƒ]:\n'
        '   â˜… í˜¸ëª¨ê·¸ë˜í”¼ ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ (ì¼ë°˜ ì¹´ë©”ë¼, ê³ ì • ìœ„ì¹˜)\n'
        '     - ROI 4ê°œ ì  ì„¤ì • â†’ ì‹¤ì œ ê°€ë¡œ/ì„¸ë¡œ ê¸¸ì´ ì…ë ¥\n'
        '   â˜… í•­ê³µë·° ëª¨ë“œ: ë“œë¡  ì´¬ì˜ìš© (ìˆ˜ì§ ì´¬ì˜, ê· ì¼í•œ ìŠ¤ì¼€ì¼)\n'
        '     - ì‹¤í–‰ í›„ ê¸°ì¤€ì„  í•˜ë‚˜ë§Œ ê·¸ë¦¬ê³  ì‹¤ì œ ê±°ë¦¬ ì…ë ¥\n'
        '   â˜… ì£¼í–‰ëª¨ë“œ: ìš´ì „ ì¤‘ ì´¬ì˜ ì˜ìƒ (í‘œì§€íŒ ê²€ì§€ ë° ë¶„ì„)\n'
        '     - í‘œì§€íŒ ê²€ì§€ ì‹œ ìë™ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜\n'
        '     - ë°”ìš´ë”© ë°•ìŠ¤ í‘œì‹œ ë° CSV ë°ì´í„° ì €ì¥\n'
        '   â˜… í…ŒìŠ¤íŠ¸ ëª¨ë“œ: PT ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ì˜ìƒ ë¶ˆí•„ìš”)\n'
        '     - ì´ë¯¸ì§€ íŒŒì¼ë“¤ë¡œ ëª¨ë¸ ê²€ì§€ ì„±ëŠ¥ í™•ì¸\n'
        '     - ì‹¤ì‹œê°„ ê²°ê³¼ í‘œì‹œ ë° ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸\n'
        '4. í˜¸ëª¨ê·¸ë˜í”¼ ëª¨ë“œ ì„ íƒ ì‹œ:\n'
        '   - ROI ì„¤ì • ì°½ì—ì„œ ë„¤ ì ì„ ì°ì–´ ë¶„ì„í•  ë„ë¡œ ì˜ì—­ ì§€ì •\n'
        '   - ì‹¤ì œ ê°€ë¡œ/ì„¸ë¡œ ê¸¸ì´ë¥¼ ë¯¸í„° ë‹¨ìœ„ë¡œ ì…ë ¥\n'
        '   - ì°¨ì„  ì„¤ì • ì°½ì—ì„œ ì°¨ì„ ë³„ ì˜ì—­ ì„¤ì • (ì„ íƒì‚¬í•­)\n'
        '5. í•­ê³µë·° ëª¨ë“œ ì„ íƒ ì‹œ:\n'
        '   - ë°”ë¡œ ë¶„ì„ ì‹œì‘ë¨\n'
        '   - ì‹¤í–‰ ì¤‘ì— ë§ˆìš°ìŠ¤ë¡œ ê¸°ì¤€ì„  ê·¸ë¦¬ê³  ì‹¤ì œ ê±°ë¦¬ ì…ë ¥\n\n'
        '--- ë¶„ì„ ì¤‘ ìƒí˜¸ì‘ìš© (YOLO Detection ì°½) ---\n'
        'ê³µí†µ ê¸°ëŠ¥:\n'
        'â€¢ ë§ˆìš°ìŠ¤ ìš°í´ë¦­: ê²€ì§€ì„ (Tracking Line) ì„¤ì •\n'
        'â€¢ ë§ˆìš°ìŠ¤ íœ í´ë¦­: ì´ˆê¸° ê²€ì§€ì„ (Initial Line) ì„¤ì •\n'
        'â€¢ \'n\' í‚¤: ì‚¬ìš©ì ì •ì˜ ì˜ì—­ ê·¸ë¦¬ê¸°\n'
        'â€¢ \'p\' í‚¤: ì •ë³´ í‘œì‹œ í† ê¸€\n'
        'â€¢ \'t\' í‚¤: ì‹¤ì‹œê°„ ê°ì²´ ì´ë™ ê²½ë¡œ í‘œì‹œ í† ê¸€\n'
        'â€¢ \'i\' í‚¤: ëˆ„ì  trail ìŠ¤íƒ í‘œì‹œ í† ê¸€ (ì°¨ë¡œ ì‚¬ìš©ëŸ‰ ë¶„ì„)\n'
        'â€¢ \'q\' í‚¤: ë¶„ì„ ì¤‘ë‹¨\n'
        'â€¢ ì˜ìƒ ë¶„ì„ ì™„ë£Œ ì‹œ ìë™ìœ¼ë¡œ ëˆ„ì  trail ì´ë¯¸ì§€ ì €ì¥\n\n'
        'í•­ê³µë·° ëª¨ë“œ ì „ìš©:\n'
        'â€¢ ë§ˆìš°ìŠ¤ ì¢Œí´ë¦­: ê¸°ì¤€ì„  ê·¸ë¦¬ê¸° (ì‹œì‘ì â†’ëì )\n'
        'â€¢ ê±°ë¦¬ ì…ë ¥ ì°½: ê¸°ì¤€ì„ ì˜ ì‹¤ì œ ê±°ë¦¬(m) ì…ë ¥\n'
        'â€¢ \'r\' í‚¤: ìŠ¤ì¼€ì¼ ì´ˆê¸°í™”\n\n'
        'í˜¸ëª¨ê·¸ë˜í”¼ ëª¨ë“œ ì „ìš©:\n'
        'â€¢ \'a\' í‚¤: í•­ê³µë·° ëª¨ë“œë¡œ ì „í™˜ ê°€ëŠ¥\n\n'
        '--- ê²°ê³¼ë¬¼ ---\n'
        'ë¶„ì„ ì™„ë£Œ í›„, ì›ë³¸ ì˜ìƒì´ ìˆëŠ” í´ë” ë‚´ì— ê²°ê³¼ ì˜ìƒ(.avi)ê³¼ ë‹¤ìˆ˜ì˜ ë¶„ì„ ë°ì´í„° CSV íŒŒì¼(.csv)ì´ ì €ì¥ëœ ìƒˆ í´ë”ê°€ ìƒì„±ë©ë‹ˆë‹¤.\n'
        'CSV íŒŒì¼ë“¤ì€ ì°¨ëŸ‰ë³„ í†µí–‰ ê¸°ë¡, ì†ë„, ê°€ê°ì†ë„, ì°¨ê°„ ê±°ë¦¬, ì‹œê°„ëŒ€ë³„ í†µê³„ ë“± ë‹¤ì–‘í•œ ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n\n'
        '--- ê°œë°œì ì—°ë½ì²˜ ---\n'
        'ë…¸ìš°í˜„ / 010-6886-0368 / KaKaoTalk ID: NoHyung\n'
        'GitHub: https://github.com/imonkfcwifi (imonkfcwifi)'
    )

    def browse_video():
        file_path = filedialog.askopenfilename(title="ë¶„ì„í•  ì˜ìƒ íŒŒì¼ ì„ íƒ",
                                               filetypes=[("Video íŒŒì¼", "*.mp4 *.avi *.mov *.mkv"), ("ëª¨ë“  íŒŒì¼", "*.*")])
        if file_path: video_path_var.set(file_path)

    def browse_model():
        file_path = filedialog.askopenfilename(title="ì‚¬ìš©í•  ëª¨ë¸ íŒŒì¼ ì„ íƒ (.pt)",
                                               filetypes=[("PyTorch ëª¨ë¸", "*.pt"), ("ëª¨ë“  íŒŒì¼", "*.*")])
        if file_path: model_path_var.set(file_path)

    def show_help():
        help_window = tk.Toplevel(root)
        help_window.title("ë„ì›€ë§ (Help)")
        help_window.geometry("700x700")

        text_frame = tk.Frame(help_window)
        text_frame.pack(fill="both", expand=True, padx=10, pady=5)
        scrollbar = tk.Scrollbar(text_frame)
        scrollbar.pack(side="right", fill="y")
        help_text_widget = tk.Text(text_frame, wrap="word", yscrollcommand=scrollbar.set, padx=5, pady=5, font=("Malgun Gothic", 10))
        help_text_widget.insert("1.0", help_text_content)
        help_text_widget.config(state="disabled")
        help_text_widget.pack(side="left", fill="both", expand=True)
        scrollbar.config(command=help_text_widget.yview)

        github_label = tk.Label(help_window, text="ê°œë°œì GitHub ë°©ë¬¸ (í´ë¦­)", fg="blue", cursor="hand2", font=("Malgun Gothic", 10, "underline"))
        github_label.pack(pady=(5,0))
        github_label.bind("<Button-1>", lambda e: webbrowser.open("https://github.com/imonkfcwifi"))

        close_button = tk.Button(help_window, text="ë‹«ê¸°", command=help_window.destroy, width=10)
        close_button.pack(pady=10)


    padding_options = {'padx': 10, 'pady': 5}
    file_select_frame = tk.LabelFrame(root, text="íŒŒì¼ ì„ íƒ", padx=10, pady=10)
    file_select_frame.pack(fill='x', **padding_options)

    tk.Label(file_select_frame, text="ì˜ìƒ íŒŒì¼:").grid(row=0, column=0, sticky='w', pady=2)
    tk.Entry(file_select_frame, textvariable=video_path_var, width=70).grid(row=0, column=1, sticky='ew', padx=5, pady=2)
    tk.Button(file_select_frame, text="ì°¾ì•„ë³´ê¸°", command=browse_video, width=10).grid(row=0, column=2, padx=(0,5), pady=2)

    tk.Label(file_select_frame, text="ëª¨ë¸ íŒŒì¼:").grid(row=1, column=0, sticky='w', pady=2)
    tk.Entry(file_select_frame, textvariable=model_path_var, width=70).grid(row=1, column=1, sticky='ew', padx=5, pady=2)
    tk.Button(file_select_frame, text="ì°¾ì•„ë³´ê¸°", command=browse_model, width=10).grid(row=1, column=2, padx=(0,5), pady=2)

    file_select_frame.columnconfigure(1, weight=1)

    # ë¶„ì„ ëª¨ë“œ ì„ íƒ í”„ë ˆì„ ì¶”ê°€
    mode_select_frame = tk.LabelFrame(root, text="ë¶„ì„ ëª¨ë“œ ì„ íƒ", padx=10, pady=10)
    mode_select_frame.pack(fill='x', **padding_options)
    
    tk.Radiobutton(mode_select_frame, text="í˜¸ëª¨ê·¸ë˜í”¼ ëª¨ë“œ (ê¸°ì¡´ ë°©ì‹) - 4ê°œ ì ìœ¼ë¡œ ROI ì„¤ì • í›„ ì‹¤ì œ ê°€ë¡œ/ì„¸ë¡œ ê¸¸ì´ ì…ë ¥", 
                   variable=analysis_mode_var, value="homography", font=("Malgun Gothic", 10)).pack(anchor='w', pady=2)
    tk.Radiobutton(mode_select_frame, text="í•­ê³µë·° ëª¨ë“œ (ë“œë¡  ì´¬ì˜ìš©) - ê¸°ì¤€ì„  í•˜ë‚˜ë¡œ ìŠ¤ì¼€ì¼ ì„¤ì •", 
                   variable=analysis_mode_var, value="aerial", font=("Malgun Gothic", 10)).pack(anchor='w', pady=2)
    tk.Radiobutton(mode_select_frame, text="ì£¼í–‰ëª¨ë“œ (ìš´ì „ ì¤‘ ì´¬ì˜) - í‘œì§€íŒ ê²€ì§€ ë° ë¶„ì„", 
                   variable=analysis_mode_var, value="driving", font=("Malgun Gothic", 10)).pack(anchor='w', pady=2)
    tk.Radiobutton(mode_select_frame, text="í…ŒìŠ¤íŠ¸ ëª¨ë“œ - PT ëª¨ë¸ + ì´ë¯¸ì§€ë¡œ ê²€ì§€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸", 
                   variable=analysis_mode_var, value="test", font=("Malgun Gothic", 10)).pack(anchor='w', pady=2)

    button_frame = tk.Frame(root)
    button_frame.pack(fill='x', pady=15, padx=10)

    start_button = tk.Button(button_frame, text="ë¶„ì„ ì‹œì‘",
        command=lambda: start_analysis(video_path_var.get(), model_path_var.get(), help_text_content, analysis_mode_var.get()),
        font=("Helvetica", 14, "bold"), bg="#4CAF50", fg="white", relief="raised", width=12, height=1, padx=10, pady=5)
    start_button.pack(side='left', expand=True, padx=5)

    help_button = tk.Button(button_frame, text="ë„ì›€ë§", command=show_help,
        font=("Helvetica", 14), bg="#2196F3", fg="white", relief="raised", width=12, height=1, padx=10, pady=5)
    help_button.pack(side='right', expand=True, padx=5)

    root.mainloop()

if __name__ == "__main__":
    
    main()
